<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-08-31 Wed 19:16 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Gibbs sampling</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Gibbs sampling</h1>
<p>
Gibbs sampling is a family of <a href="20220306194751-markov_chain_monte_carlo_new.html#ID-5acc4f0f-417e-424f-95a5-1c95e7e822ff">Markov Chain Monte Carlo</a> algorithm where we conditionally sample from each parameter while holding the others fixed, successively. It is useful when the joint distribution is difficult to sample from (no closed-form expression, for instance) but where the conditional distribution of each variable is known.
</p>

<div id="outline-container-org2dcefe0" class="outline-2">
<h2 id="org2dcefe0">General principle</h2>
<div class="outline-text-2" id="text-org2dcefe0">
<p>
Assume that we want to generate \(N\) samples \(\left\{\tilde{\Theta}^{(n)}\right\}_{n=1 \dots N}\) from the joint distribution \(P\left(\theta_1, \dots, \theta_D\right)\). We start with an initial position \(\tilde{\Theta}^{(0)}\), and to generate \(\tilde{\Theta}}^{i+1}\) we successively draw from the following conditional distributions:
</p>

<ul class="org-ul">
<li>\(\tilde{\theta}^{(i+1)}_1 \sim P(\theta_1\; |\; \theta_2 = \tilde{\theta_2}^{(i)}, \dots, \theta_D = \tilde{\theta_D}^{(i)})\)</li>
<li>\(\tilde{\theta}^{(i+1)}_2 \sim P(\theta_2\; |\; \theta_1 = \tilde{\theta_1}^{(i+1)}, \theta_3 = \tilde{\theta_3}^{(i)} \dots, \theta_D = \tilde{\theta_D}^{(i)})\)</li>
<li>\(\tilde{\theta}^{(i+1)}_j \sim P(\theta_j\; |\; \theta_1 = \tilde{\theta_1}^{(i+1)}, \dots, \theta_{j-1} = \tilde{\theta}_{j-1}^{(i+1)}, \theta_{j+1} = \tilde{\theta}_{j+1}^{(i)}, \dots, \theta_D = \tilde{\theta_D}^{(i)})\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgdc6207d" class="outline-2">
<h2 id="orgdc6207d">Using auxiliary variables</h2>
<div class="outline-text-2" id="text-orgdc6207d">
<p>
It is not uncommon to introduce auxiliary variables to the model to make computations easier. Let us consider the <i>augmented likelihood</i> \(P(y, \omega | \theta)\), so that the original likelihood:
</p>

<p>
\[
P(Y, \omega|\theta) = P(Y|\omega, \theta) P(\omega)
\]
</p>

<p>
This works/is useful iff:
</p>

<ol class="org-ol">
<li>Marginalizing the augmented likelihood returns the original likleihood \(\int P(Y, \omega | \theta) \mathrm{d} \omega = \int P(Y|\omega, \theta) P(\omega) \mathrm{d}\omega = P(Y|\theta)\)</li>
<li>The prior \(P(\theta)\) is conjugate to \(P(Y|\omega, \theta)\) so \(P(\theta|Y) \propto P(Y | \theta) P(\theta) = \int P(Y|\omega, \theta) P(\omega) P(\theta) \mathrm{d}\omega\)</li>
</ol>

<p>
This trick is used to build Gibbs samplers for:
</p>
<ul class="org-ul">
<li>The Bernoulli logit regression (<a href="20220728151132-polya_gamma_augmentation.html#ID-16338bc2-222c-4acf-aa28-38b951dfcb89">Polya-Gamma augmentation</a>)</li>
<li>The Negative Binomial logit regression (Idem)</li>
<li>The Horsehoe prior (using the <a href="20220214161724-half_cauchy_distribution.html#ID-45ccc897-f07c-4adc-9142-9ae8870fbddc">inverse-gamma expansion of the Half-Cauchy distribution</a>)</li>
</ul>
</div>
</div>

<div id="outline-container-org6b489f9" class="outline-2">
<h2 id="org6b489f9">Links to this note</h2>
<div class="outline-text-2" id="text-org6b489f9">
<ul class="org-ul">
<li><a href="./inbox.html">Writing inbox</a></li>
</ul>
</div>
</div>
</div>
</body>
</html>
