:PROPERTIES:
:ID:       2e41e200-be7a-482b-8cfe-d0d67df26920
:END:
#+title: Random Variables in Aesara

- =RandomGenerator=
- =RandomState=
- =RandomStream=

* AeP (Aesara Enhancement Propoal)
** Guidelines

A good PRNG design satisfies the following conditions:
1. It is **expressive**: the behavior of the system is **predictable** by the caller, and allows them to expression any probabilistic program;
2. It makes it possible to build **reproducible** programs ("seeding");
3. It is **explicit in Aesara's IR**
4. It can be manipulated by **Aesara's rewrite system**;
5. It can be *easily transpiled* to current backends;
6. It enables **vectorization** with generalized universal functions;

** Limits of the current design

The current =RandomState= model threads the RNG state through all the functions that generate random values. This is how the =RandomState= API would look like with =default_output= set to =None=:

#+begin_src python
import aesara.tensor as at
import numpy as np

def sample(rng):

rng = np.random.default_rng(12)
rng_x, x_rv = at.random.normal(0, 1, rng=rng)
rng_y, y_rv = at.random.normal(0, 1, rng=rng_x)
z_at = x_rv + y_rv
#+end_src

This design is inconvenient to work with.

**** Error-prone and cumbersome

*For users* having to explictly thread the =rng= states is both error-prone and cumbersome. I believe this is why the RNG output is hidden by setting =default_output= to =1=, and one of the motivations behind the =RandomStream= interface.

**** Adds sequencing constraints between functions that have no data dependence

*For the Aesara IR* the threading of RNGs adds sequencing constraints between functions that have no data dependence otherwise. In theory, the two calls to =at.random.normal= are independent and could be made in parallel. In practice, one has to wait for the first call to complete to be able to make the second all.

Same with gufuncs: we do not want to introduce rng-dependence between the different function calls. We want to be able to give each dimension (give example) a rng and let it run with it. No collision:

**** Lack of a clear symbolic representation

*For the Aesara IR* we don't have distinct operators that updates the random state and generate a pseudo-random number from a random state.

And finally this is not enought for the one-to-many problem in rewrites.

Only the caller knows how the random state should be managed as called functions are unaware of the context. The solution is very simple, it is to give the caller the responsibility of advancing the rng state. So functions do not need to return RNG state.

/Note:/ Well, some function have to return the RNG state in JAX and it's the body function of while loops. Still goes to show how broken of an abstraction it is. There needs to pass RNG state because there /is/ a data dependence between the two.

#+begin_src python
def sequential(rng):
    rng_y, rng_z = at.random.split(rng, 2)
    return call(rng_y) + call(rng_z)

sequential_et = etuple(
    etuplize(at.add),
    etuple(
        call,
        etuple(
            nth,
            0,
            etuple(
                etuplize(at.random.split),
                rng,
                2
            )
        )
    ),
    etuple(
        call,
        etuple(
            nth,
            1,
            etuple(
                etuplize(at.random.split),
                rng,
                2
            )
        )
    )
)
#+end_src

*First* justify the =split=, =rand= symbolic interface (reference Haskell)


Counter based approach: [[http://www.thesalmons.org/john/random123/papers/random123sc11.pdf][Parallel Random Numbers: As easy as 1, 2, 3]]

** Proposal: A Haskell-like design

In the following we will first focus on /the symbolic representation of RNGs in Aesara's IR/. We leave discussions about compilation of this representation for the end.

To begin with we need to distinguish between two objects that are conflated in Aesara:
- The =RandomState= which is the current state of the random number generator;
- The =RandomVariable=, which takes a =RandomState= and returns a pseudo-random number =Bit=

If we represent the internal state of the PRNG by the type =RandState=, the current design of =RandomVariable=\s can be summarized by the following simplified signature:

#+begin_src haskell
RandomVariable :: RandState -> (RandState, TensorVariable)
#+end_src

In other words, =RandomVariable=\s are responsible for advancing the state of the PRNG, and produce a random value. This double responsibility is what creates graph dependencies between nodes that have otherwise no dependency i.e. that wouldn't be linked had they not needed the PRNG. We can see that on the following listing:

#+begin_src python :session :results output
import aesara
import aesara.tensor as at

rng = at.random.type.RandomStateType()('rng')

rng_x, x_rv = at.random.normal(0, 1, rng=rng, name='x').owner.outputs
rng_y, y_rv = at.random.normal(0, 1, rng=rng_x, name='y').owner.outputs
z_rv = at.random.normal(0, 1, rng=rng_y, name='z')
w_at = x_rv + y_rv + z_rv

aesara.dprint(w_at)
#+end_src

#+RESULTS:
#+begin_example
Elemwise{add,no_inplace} [id A]
 |Elemwise{add,no_inplace} [id B]
 | |normal_rv{0, (0, 0), floatX, False}.1 [id C] 'x'
 | | |rng [id D]
 | | |TensorConstant{[]} [id E]
 | | |TensorConstant{11} [id F]
 | | |TensorConstant{0} [id G]
 | | |TensorConstant{1} [id H]
 | |normal_rv{0, (0, 0), floatX, False}.1 [id I] 'y'
 |   |normal_rv{0, (0, 0), floatX, False}.0 [id C]
 |   |TensorConstant{[]} [id J]
 |   |TensorConstant{11} [id K]
 |   |TensorConstant{0} [id L]
 |   |TensorConstant{1} [id M]
 |normal_rv{0, (0, 0), floatX, False}.1 [id N] 'z'
   |normal_rv{0, (0, 0), floatX, False}.0 [id I]
   |TensorConstant{[]} [id O]
   |TensorConstant{11} [id P]
   |TensorConstant{0} [id Q]
   |TensorConstant{1} [id R]
#+end_example

*** New operators

A natural idea is to simplify the design of =RandomVariable=\s so that it is only responsible for one thing: create a random value from a PRNG state. The =Op= thus creates an =Apply= node that takes a =RandomState= as input and outputs a (random) =Variable=:

#+begin_src haskell
RandomVariable :: RandomState -> Variable
#+end_src

We can now make the =RandState= output explicit (1) by making =rng= an explicit input of the =RandomVariable='s =__call__= method. And we can verify in the following that the sequential dependency between =x_rv= and =y_rv= disappears:

#+begin_src python
import aesara.tensor as at

# rng_x and rng_y are created before that.
x_rv = at.random.normal(rng_x, 0, 1)
y_rv = at.random.normal(rng_y, 0, 1)
#+end_src

This interface presupposes the existence of an operator that creates an updated =RandomState= from a =RandomState=:

#+begin_src haskell
next :: RandomState -> RandomState
#+end_src

To be able to build reproducible programs (2), we also need an operator that creates a =RandomState= from a seed:

#+begin_src haskell
default_rng :: Seed -> RandomState
#+end_src

We can thus fill in the blanks in the previous code snippet:

#+begin_src python
import aesara.tensor as at

rng = at.random.default_rng(0)

rng_x = at.random.next(rng)
x_rv = at.random.normal(rng_x, 0, 1)

rng_y = at.random.next(rng_x)
y_rv = at.random.normal(rng_y, 0, 1)

z_at = x_rv + y_rv
#+end_src

#+begin_src python :session
import aesara.tensor as at
import numpy as np

rng = at.random.type.RandomGeneratorType()("rng")

rng_x, x_rv = at.random.normal(0,1, rng=rng).owner.outputs
rng_y, y_rv = at.random.normal(0,1, rng=rng_x).owner.outputs

def next(rng):
    bit_generator = rng.bit_generator.advance(1)
    return np.random.Generator(bit_generator)

#+end_src

#+RESULTS:

This code generates Aesara graphs that are very similar to the ones generated by =RandomStream=:

#+begin_src python :session
import aesara.tensor as at

srng = at.random.RandomStream()

x_rv = srng.normal(0, 1)
y_rv = srng.normal(0, 1)
z_at = x_rv + y_rv
#+end_src

#+RESULTS:

Roughly speaking, =RandomStream= moves this "next" mechanism to the /updates/ graph, while this symbolic representation of the state of the PRNG makes it explicit in the same graph. The graph between =RandomState=\s in the proposed scheme is a concretization of the updates graph for the =RandomStream=.

#+begin_src python :results output :session
import aesara
import aesara.tensor as at

srng = at.random.RandomStream(seed=123)
x_rv = srng.normal()
y_rv = srng.normal()
z_at = x_rv + y_rv

aesara.dprint(y_rv.owner.inputs[0].default_update)
#+end_src

#+RESULTS:
: normal_rv{0, (0, 0), floatX, False}.0 [id A]
:  |RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FE7411BCF20>) [id B]
:  |TensorConstant{[]} [id C]
:  |TensorConstant{11} [id D]
:  |TensorConstant{0.0} [id E]
:  |TensorConstant{1.0} [id F]

Let us now consider a more complex situation, when =call_x= and =call_y= are two functions that require a =RandomState=:

#+begin_src python
import aesara.tensor as at

rng = at.random.default_rng(0)

rng_x = at.random.next(rng)
x_rv = call_x(rng_x)

rng_y = at.random.next(rng_x)
y_rv = call_y(rng_y)

z_at = x_rv + y_rv
#+end_src

We can easily make the previous code generate a collision:

#+begin_src python
def call_x(rng_a):
    a_rv = at.random.normal(rng_a, 0, 1)
    rng_b = at.random.next(rng_a)
    b_rv = at.random.normal(rng_b, 0, 1)
    return a_rv * b_rv
#+end_src

The issue arises because our symbolic structure is /linear/: each =RandState= has one and only one ancestor. What we need instead is to allow =RandState=\s to have several ancestors; We thus define the =at.random.op.Split= operator:

#+begin_src haskell
split :: RandState -> (RandState, RandState)
#+end_src

This can be easily implemented: if =RandState= is a binary number <..> then the leftmost key is obtained by appending =0=, the rightmost =1=. Any succession of splitting operations builds a binary tree. If =Rand= is a deterministic function of this value, then the computations are fully reproducible. This binary tree structure is encoded directly in the graph IR:

#+begin_src python
import aesara.tensor as at

rng = at.random.default_rng(0)
rng_x, rng_y = at.random.split(rng)

etuplize(rng_x)
# etuple(nth, 0, etuple(at.random.split, etuple(at.random.default_rng, 0)))

etuplize(rng_y)
# etuple(nth, 1, etuple(at.random.split, etuple(at.random.default_rng, 0)))
#+end_src

Although it may be possible to keep =Next= within this representation, its interaction with the =Split= operator requires careful thought. We forget it for now, as =Split= is expressive enough. The original program becomes:

#+begin_src python
import aesara.tensor as at

rng = at.random.default_rng()
rng_x, rng_y = at.random.split(rng)

x_rv = at.random.normal(rng_x, 0, 1)
y_rv = at.random.normal(rng_y, 0, 1)
z_at = x_rv + y_rv
#+end_src

It is also natural to implement the =at.random.op.SplitN= operator represented by:

#+begin_src haskell
splitn :: RandState -> Int -> (RandState, ..., RandState)
#+end_src

So we can write the following code:

#+begin_src python
at.random.split = at.random.Split()

rng = at.random.default_rng()
rng_v, rng_w, rng_x, rng_y = at.random.splitn(rng, 4)

v_rv = at.random.normal(rng_y, 0, 1)
w_rv = at.random.normal(rng_x, 0, 1)
x_rv = at.random.normal(rng_x, 0, 1)
y_rv = at.random.normal(rng_y, 0, 1)
z_at = v_rv + w_rv + x_rv + z_rv
#+end_src

This will prove useful e.g. for use with generalized universal functions.

A nice consequence of this "splitting" design is that the graph that updates the =RandomState=\s is always neatly separated from the "main" graph that contains the =RandomVariable=\s. Does this help with rewrites?



Counter-based RNGs are now ubiquitous, in NumPy, JAX, XLA, PyTorch, etc so we can mostly rely on their design for =RandomState=

*** Implementation

Let us sketch an implementation in Aesara. First we need an implementation of =RandState=:

#+begin_src python :session :results silent
from typing import NamedTuple

class RandState(NamedTuple):
    key: int
    path: int
#+end_src


Here =key= is the seed given by the user. If not provided, it will be chosen at random at compilation. =path= is the path to this RNG state in the RNG graph, and is represented as bits. A naive implementation of the =split= operator is thus:

#+begin_src python :session :results output
def split(rng):
    left_rng = RandomState(rng.key, (rng.path << 1))
    right_rng = RandomState(rng.key, (rng.path << 1) + 1)
    return left_rng, right_rng

rng = RandomState(10, 0b011101)
print(rng)

left, right = split(rng)
print(left, right)
#+end_src

#+RESULTS:
: RandomState(key=10, path=29)
: RandomState(key=10, path=58) RandomState(key=10, path=59)

It is naive in the sense that the counter space $\mathcal{S}$ of real PRNGs does not usually extend indefinitely ($128$ for Philox in NumPy). In practice we will need to walk the graph and compress the state by incrementing the =key= as well, but this can be done at compile time once information about the =BitGenerator= is known. However, we might need to use Philox' hash function as the state progresses to be immediately compatible with NumPy in the =perform= function. Since we can still walk the =RandState= tree in our representation this should not affect it too much.

#+begin_src python :session :results output
left = RandomState(10, 0b01)
print(left)
for _ in range(100):
    left, _ = split(left)
print(left)
#+end_src

#+RESULTS:
: RandomState(key=10, path=1)
: RandomState(key=10, path=1267650600228229401496703205376)

This can easily be translated to an Aesara =Op=. The =RandomVariable= Ops are slightly changed, we need to modify their =__call__=, =make_node= and =perform= things:

#+begin_src python
from aesara.graph.op import Op


class RandomVariable(Op)
    def __call__(self, rng, *args, size=None, name=None, dtype=None, **kwargs):
        res = super().__call__(rng, size, dtype, *args, **kwargs)

        if name is not None:
            res.name = name

        return res

    def make_node(self, rng, size, dtype, *dist_params):
        """Create a random variable node.

        Parameters
        ----------
        rng: RandState
        size: int or Sequence
            NumPy-like size parameter.
        dtype: str
            The dtype of the sampled output.  If the value ``"floatX"`` is
            given, then `dtype` is set to ``aesara.config.floatX``.  This value is
            only used when ``self.dtype`` isn't set.
        dist_params: list
            Distribution parameters.

        Results
        -------
        out: Apply
            A node with inputs ``(rng, size, dtype) + dist_args`` and outputs
            ``(out_var)``.

        """
        size = normalize_size_param(size)

        dist_params = tuple(
            as_tensor_variable(p) if not isinstance(p, Variable) else p
            for p in dist_params
        )

        # `rng` needs to be explicitly provided.
        # if rng is None:
        #     rng = aesara.shared(np.random.default_rng())
        # elif not isinstance(rng.type, RandomType):
        #     raise TypeError(
        #         "The type of rng should be an instance of either RandomGeneratorType or RandomStateType"
        #     )

        shape = self._infer_shape(size, dist_params)
        _, bcast = infer_broadcastable(shape)
        dtype = self.dtype or dtype

        if dtype == "floatX":
            dtype = config.floatX
        elif dtype is None or (isinstance(dtype, str) and dtype not in all_dtypes):
            raise TypeError("dtype is unspecified")

        if isinstance(dtype, str):
            dtype_idx = constant(all_dtypes.index(dtype), dtype="int64")
        else:
            dtype_idx = constant(dtype, dtype="int64")
            dtype = all_dtypes[dtype_idx.data]

        outtype = TensorType(dtype=dtype, shape=bcast)
        out_var = outtype()
        inputs = (rng, size, dtype_idx) + dist_params
        output = (out_var,)

        return Apply(self, inputs, outputs)

    def perform(self, node, inputs, outputs):
        smpl_out = outputs
        rng, size, dtype, *args = inputs

        out_var = node.outputs[1]

        # If `size == []`, that means no size is enforced, and NumPy is trusted
        # to draw the appropriate number of samples, NumPy uses `size=None` to
        # represent that.  Otherwise, NumPy expects a tuple.
        if np.size(size) == 0:
            size = None
        else:
            size = tuple(size)

        ##----------------------##
        ##  THE IMPORTANT STUFF ##
        ##----------------------##
        rng = np.random.Generator(np.random.Philox(key=rng.key, counter=rng.counter))
        smpl_val = self.rng_fn(rng, *(args + [size]))
        ##----------------------##
        ##  THE IMPORTANT STUFF ##
        ##----------------------##

        if (
            not isinstance(smpl_val, np.ndarray)
            or str(smpl_val.dtype) != out_var.type.dtype
        ):
            smpl_val = _asarray(smpl_val, dtype=out_var.type.dtype)

        smpl_out[0] = smpl_val

    def rng_fn(self, rng, *args, **kwargs):
        """Sample a numeric random variate."""
        return getattr(rng, self.name)(*args, **kwargs)
#+end_src

We can keep the =RandomStream= API as follows by making it handle the splitting internally. It could even be an =OpFromGraph=?

#+begin_src python
def gen(self, op: "RandomVariable", *args, **kwargs) -> TensorVariable:
    r"""Generate a draw from `op` seeded from this `RandomStream`.

    Parameters
    ----------
    op
        A `RandomVariable` instance
    args
        Positional arguments passed to `op`.
    kwargs
        Keyword arguments passed to `op`.

    Returns
    -------
    The symbolic random draw performed by `op`.  This function stores
    the updated `RandomType`\s for use at compile time.

    """
    if "rng" in kwargs:
        raise ValueError(
            "The `rng` option cannot be used with a variate in a `RandomStream`"
        )

    # Update the random state and take a sample
    _, self.rng = at.random.split(self.rng)
    out = op(self.rng, *args, **kwargs)

    # This is the value that should be used to replace the old state
    # (i.e. `rng`) after `out` is sampled/evaluated.
    # The updates mechanism in `aesara.function` is supposed to perform
    # this replace action.
    new_rng = out.owner.outputs[0]

    self.state_updates.append((rng, new_rng))  # happens here

    rng.default_update = new_rng

    return out
#+end_src

*** Etuplization

These new operators simplify greatly the work with etuplized versions of graphs with random variables. Assuming we have a =Nth= operator in Aesara as well:

#+begin_src python
from etuples import ExpressionTuple, etuplize
from aesara.graph.rewriting.unify import OpExpressionTuple

rng = at.random.default_rng()
rng_x, rng_y = at.random.split(rng)
x_rv = at.random.normal(rng_x, 0, 1)
y_rv = at.random.normal(rng_y, 0, 1)
z_at = x_rv + y_rv

z_et = OpExpressionTuple(
    etuplize(at.add),
    OpExpressionTuple(
        etuplize(at.random.normal),
        ExpressionTuple(
           etuplize(aesara.graph.nth),
           1,
           OpExpressionTuple(
               etuplize(at.random.split)
               rng
           )
        ),
        0,
        1,
        size,
        dtype,
    ),
    OpExpressionTuple(
        etuplize(at.random.normal),
        ExpressionTuple(
           etuplize(aesara.graph.nth),
           0,
           OpExpressionTuple(
               etuplize(at.random.split)
               rng
           )
        ),
        0,
        1,
        size,
        dtype,
    ),
)
#+end_src

This solves the one-to-many problem we had for mixtures. If we have a relation between a single random variable and two we can now write:

#+begin_src python
z_et = etuple(etuplize(at.random.normal), rng_lv, size_lv, dtype_lv, at.as_tensor(1.), at.as_tensor(2.))

sum_et = etuple(
    etuplize(at.add),
    etuple(
        etuplize(at.random.normal),
        etuple(
            etuplize(at.graph.nth),
            0,
            etuple(
                etuplize(at.random.split),
                rng_lv,
            )
        )
        size_lv,
        dtype_lv,
        at.as_tensor(0.),
        at.as_tensor(1.),
    ),
    etuple(
        etuplize(at.random.normal),
        etuple(
            etuplize(at.graph.nth),
            1,
            etuple(
                etuplize(at.random.split),
                rng_lv,
            )
        )
        size_lv,
        dtype_lv,
        at.as_tensor(0.),
        at.as_tensor(1.),
    )
)
#+end_src

*** Behavior under transpilation

**** JAX

Transpilation to JAX would be straightforward, as JAX [[https://jax.readthedocs.io/en/latest/jep/263-prng.html][uses a splittable PRNG representation]]. We will simply need to perform the following substitutions:

#+begin_src python
rng = at.random.RandomState()
rng_key = jax.random.PRNGKey()

at.random.split(rng)
jax.random.split(rng_key)

at.random.splitn(rng, 10)
jax.random.split(rng_key, 10)
#+end_src

**** Numba

After [[https://github.com/aesara-devs/aesara/pull/1245][#1245]] Aesara will support NumPy's Generator API. NumPy has support for [[https://numpy.org/doc/stable/reference/random/bit_generators/philox.html][Philox as a BitGenerator]]. Philox is a [[http://www.thesalmons.org/john/random123/papers/random123sc11.pdf][counter-based PRNG]] which can easily accomodate [[https://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf][splittable PRNG representations]]. We can directly translate the =RandomVariable=\s to NumPy =Generator=\s and seed these generators at compile time:

#+begin_src python
at.random.normal(rng, 0, 1)

# This happens at compile time
counter, key = rng_index_to_key_counter(rng)

# Now we can seed the rng
rng = np.random.Generator(np.random.Philox(counter=counter, key=key))
rng.normal(0, 1)
#+end_src

Alternatively, we could interpret the structure of the RNG-Variables graph, initialize one =np.random.Generator= and use the =jumped= and =advance= methods of the Philox =BitGenerator=.

* RandomVariable Ops

We have a =default_rng= function, but the result does not behave as a generator in =numpy=.

#+begin_src python :session
from aesara.tensor.random import default_rng
rng = default_rng(32)
rng.type
#+end_src

#+RESULTS:
: RandomGeneratorType


#+begin_src python :session
from aesara.tensor.random.basic import NormalRV

norm = NormalRV()
norm_rv = norm(0, 1, size=(2,), rng=rng)

norm_rv.eval()
#+end_src

#+RESULTS:
| -0.0242532 | 0.72212055 |


=Aesara= also defines aliases for the =RandomVariable= Ops:

#+begin_src python :session
from aesara.tensor.random import normal

normal_rv = normal(0, 1, size=(2,), rng=rng)
normal_rv.eval()
#+end_src

#+RESULTS:
| 0.93330371 | -0.22801103 |

Let's look at the graphs that are produced:

#+begin_src python :results output
import aesara
from aesara.tensor.random import default_rng, normal

rng = default_rng(0)
a_rv = normal(0, 1, rng=rng)
b_rv = normal(0, 1, rng=rng)
c_tt = a_rv + b_rv

d_rv = normal(0, 1, rng=rng)

aesara.dprint(c_tt * d_rv)
#+end_src

#+RESULTS:
#+begin_example
Elemwise{mul,no_inplace} [id A]
 |Elemwise{add,no_inplace} [id B]
 | |normal_rv{0, (0, 0), floatX, False}.1 [id C]
 | | |DefaultGeneratorMakerOp [id D]
 | | | |TensorConstant{0} [id E]
 | | |TensorConstant{[]} [id F]
 | | |TensorConstant{11} [id G]
 | | |TensorConstant{0} [id H]
 | | |TensorConstant{1} [id I]
 | |normal_rv{0, (0, 0), floatX, False}.1 [id J]
 |   |DefaultGeneratorMakerOp [id D]
 |   |TensorConstant{[]} [id K]
 |   |TensorConstant{11} [id L]
 |   |TensorConstant{0} [id M]
 |   |TensorConstant{1} [id N]
 |normal_rv{0, (0, 0), floatX, False}.1 [id O]
   |DefaultGeneratorMakerOp [id D]
   |TensorConstant{[]} [id P]
   |TensorConstant{11} [id Q]
   |TensorConstant{0} [id R]
   |TensorConstant{1} [id S]
#+end_example


How does =RandomGeneratorType= work? It looks like it has internal state.

* Define custom random variables

It is fairly simple as =srng.gen(RV, *args)= will call =RV()(random_state, *args)=.

#+begin_src python
srng.gen(zero_truncated_betabinom, eta_at, kappa_rv, n_at),
#+end_src

where the =RandomVariable= is implemented as:

#+begin_src python
class ZeroTruncatedBetaBinomial(RandomVariable):
    r"""A zero-truncated beta-binomial distribution.

    This distribution is implemented in the :math:`\kappa`
    and :math:`\eta` parameterization, which is related to
    the standard :math:`\alpha` and :math:`\beta` parameterization
    of the beta-binomial through the following:

    .. math::
        \alpha = \eta / \kappa \\
        \beta = (1 - \eta) / \kappa

    Truncation aside, for a :math:`Y \sim \operatorname{BetaBinom}\left(N, \eta, \kappa\right)`,  # noqa: E501

    .. math::
        \operatorname{E}\left[ Y \right] = N \eta \\
        \operatorname{Var}\left[ Y \right] = N \eta (1 - \eta) (N \kappa + 1) / (\kappa + 1)


    Under this parameterization, :math:`\kappa` in the standard beta-binomial
    serves as an over-dispersion term with the following properties:

    .. math::
        \lim_{\kappa \to 0} \operatorname{Var}\left[ Y \right] = N \eta (1 - \eta) \\
        \lim_{\kappa \to \infty} \operatorname{Var}\left[ Y \right] = N^2 \eta (1 - \eta)

    In other words, :math:`\kappa` modulates between the standard binomial
    variance and :math:`N`-times that variance.

    The un-truncated probability mass function (PMF) is as follows:

    .. math::
        \frac{\operatorname{B}\left(\frac{\eta}{\kappa} + y, n - y + \frac{1 - \eta}{\kappa}\right) {\binom{n}{y}}}{\operatorname{B}\left(\frac{\eta}{\kappa}, \frac{1 - \eta}{\kappa}\right)}  # noqa: E501

    and the zero-truncated PMF is as follows:

    .. math::
        \frac{\operatorname{B}\left(\frac{\eta}{\kappa} + y, - \frac{\eta}{\kappa} + n - y + \frac{1}{\kappa}\right) {\binom{n}{y}}}{\operatorname{B}\left(\frac{\eta}{\kappa}, - \frac{\eta}{\kappa} + \frac{1}{\kappa}\right) - \operatorname{B}\left(\frac{\eta}{\kappa}, - \frac{\eta}{\kappa} + n + \frac{1}{\kappa}\right)}  # noqa: E501

    """
    name = "zero_truncated_betabinom"
    ndim_supp = 0
    ndims_params = [0, 0, 0]
    dtype = "int64"
    _print_name = ("ZeroTruncBetaBinom", "\\operatorname{BetaBinom}_{>0}")

    def __init__(self, rejection_threshold=200, **kwargs):
        """
        Parameters
        ----------
        rejection_threshold
            The number of rejection iterations to perform before raising an
            exception.
        """
        self.rejection_threshold = rejection_threshold
        super().__init__(**kwargs)

    def __call__(self, eta, kappa, n, size=None, **kwargs):
        """
        Parameters
        ----------
        eta
        kappa
        n
        """

        self.eta = at.as_tensor_variable(eta, dtype=aesara.config.floatX)
        self.kappa = at.as_tensor_variable(kappa, dtype=aesara.config.floatX)
        self.n = at.as_tensor_variable(n, dtype=np.int64)

        return super().__call__(eta, kappa, n, size=size, **kwargs)

    def rng_fn(self, rng, eta, kappa, n, size):
        """A naive hybrid rejection + inverse sampler."""

        n = np.asarray(n, dtype=np.int64)
        eta = np.asarray(eta, dtype=np.float64)
        kappa = np.asarray(kappa, dtype=np.float64)

        # Values below this will produce errors (plus, it means this is really
        # a binomial)
        alpha = np.clip(eta / kappa, near_zero, 1e100)
        beta = np.clip((1 - eta) / kappa, near_zero, 1e100)

        # def zt_bb_inv(n, alpha, beta, size=None):
        #     """A zero-truncated beta-binomial inverse sampler."""
        #     # bb_dist = scipy.stats.betabinom(n, alpha, beta)
        #     beta_smpls = np.clip(
        #         scipy.stats.beta(alpha, beta).rvs(size=size), 1e-10, np.inf
        #     )
        #     binom_dist = scipy.stats.binom(n, beta_smpls)
        #     u = np.random.uniform(size=size)
        #     F_0 = binom_dist.cdf(0)
        #     samples = binom_dist.ppf(F_0 + u * (1 - F_0))
        #     return samples

        samples = scipy.stats.betabinom(n, alpha, beta).rvs(size=size, random_state=rng)
        alpha = np.broadcast_to(alpha, samples.shape)
        beta = np.broadcast_to(beta, samples.shape)
        n = np.broadcast_to(n, samples.shape)
        rejects = samples <= 0

        thresh_count = 0
        while rejects.any():
            _n = n[rejects] if np.size(n) > 1 else n
            _alpha = alpha[rejects] if np.size(alpha) > 1 else alpha
            _beta = beta[rejects] if np.size(beta) > 1 else beta
            _size = rejects.sum()

            beta_smpls = np.clip(
                scipy.stats.beta(_alpha, _beta).rvs(size=_size, random_state=rng),
                near_zero,
                near_one,
            )
            samples[rejects] = scipy.stats.binom(_n, beta_smpls).rvs(
                size=_size, random_state=rng
            )
            # samples[rejects] = scipy.stats.betabinom(_n, _alpha, _beta).rvs(size=_size)  # noqa: E501

            new_rejects = samples <= 0
            if new_rejects.sum() == rejects.sum():
                if thresh_count > self.rejection_threshold:
                    # # Attempt rejection sampling until the rejection results
                    # # get stuck, then use the inverse-sampler
                    # samples[rejects] = zt_bb_inv(_n, _alpha, _beta, size=_size)
                    # break
                    # raise ValueError("The sampling rejection threshold was met")
                    warnings.warn(
                        "The sampling rejection threshold was met "
                        "and mean values were used as sample values"
                    )
                    sp_ref_dist = scipy.stats.betabinom(_n, _alpha, _beta)
                    trunc_mean = sp_ref_dist.mean() / (1 - sp_ref_dist.cdf(0))
                    assert np.all(trunc_mean >= 1)
                    samples[rejects] = trunc_mean
                    break
                else:
                    thresh_count += 1
            else:
                thresh_count = 0

            rejects = new_rejects

        return samples


zero_truncated_betabinom = ZeroTruncatedBetaBinomial()


def _logp(value, eta, kappa, n):
    return (
        # binomln(n, value)
        -at.log(n + 1)
        # - betaln(n - value + 1, value + 1)
        # + betaln(value + alpha, n - value + beta)
        # - betaln(alpha, beta)
        - at.gammaln(n - value + 1)
        - at.gammaln(value + 1)
        + at.gammaln(n + 2)
        + at.gammaln(value + eta / kappa)
        + at.gammaln(n - value + (1 - eta) / kappa)
        - at.gammaln(1 / kappa + n)
        - at.gammaln(eta / kappa)
        - at.gammaln((1 - eta) / kappa)
        + at.gammaln(1 / kappa)
    )


@_logprob.register(ZeroTruncatedBetaBinomial)
def zero_truncated_betabinom_logprob(op, values, *inputs, **kwargs):
    (values,) = values
    (eta, kappa, n) = inputs[3:]

    l0 = (
        # gammaln(alpha + beta)
        # + gammaln(n + beta)
        # - gammaln(beta)
        # - gammaln(alpha + beta + n)
        at.gammaln(1 / kappa)
        + at.gammaln(n + (1 - eta) / kappa)
        - at.gammaln((1 - eta) / kappa)
        - at.gammaln(1 / kappa + n)
    )

    log1mP0 = at.log1mexp(l0)
    # log1mP0 = 0

    res = CheckParameterValue("values <= n, eta > 0, kappa > 0")(
        at.switch(values > 0, _logp(values, eta, kappa, n) - log1mP0, -np.inf),
        at.all(values <= n),
        at.all(eta > 0),
        at.all(kappa > 0),
    )
    return res
#+end_src

Note that you can also define this random variables' logprob dispatching =_logprob= for the =ZeroTruncBetaBinom=.

* Sampling vs Logprobability =aeppl=
- How define the logprob of a custom distribution?

* Shapes

Shapes are always a mess when it comes to random variables. In =aesara= we note two distinct shapes:
- =ndim_supp= the number of dimensions of the RV's support.
- =ndim_params=
- =size= which is the sample size

Remember that shapes in Aesara can be determined at runtime! So if we assume that:

#+begin_src python
batch_shape = size
np.ndim(sample_shape) = ndim_supp
shape = sample_shape + batch_shape
#+end_src

And we should have a look at broadcasting rules because they are not all very obvious.

#+begin_src python :session :results output
import aesara.tensor as at
from aesara.tensor.random import RandomStream

srng = RandomStream(0)
a_rv = srng.normal(0, 1, size=(2,3))
print(a_rv.eval())
#+end_src

#+RESULTS:
: [[ 1.44369095 -0.89594598  0.73595567]
:  [ 0.00587704  0.85338179  0.16094803]]

#+begin_src python :session :results output
mu = at.as_tensor([1., 2., 3.])
a_rv = srng.normal(mu, 1, size=(2,3))
print(a_rv.eval())
#+end_src

#+RESULTS:
: [[0.05413093 1.105416   4.68806659]
:  [0.63396273 1.38008182 1.99801801]]

#+begin_src python :session :results output
mu = at.as_tensor([1., 2.])
a_rv = srng.normal(mu, 1, size=(2,3))
print(a_rv.eval())
#+end_src

#+RESULTS:

More complex is the case where the random variable is non-scalar, as multivariate normal. Here you can see that the "event shape" is equal to 2. The resulting shape, if we assume =event_shape= and =batch_shape= are tuples is given by:

#+begin_src python
shape = event_shape + batch_shape
#+end_src

#+begin_src python :session :results output
import numpy as np

mu = np.r_[1, 2]
sigma = np.array([[.5, .5], [.4, .6]])
a_rv = srng.multivariate_normal(mu, sigma, size=(2, 5))
print(a_rv.eval().shape)
#+end_src

#+RESULTS:
: (2, 5, 2)

See [[https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/][Eric Ma's blog post on the topic]].

* Problems with =RandomStream=

https://github.com/aesara-devs/aesara/pull/1211#discussion_r985057882

* Proposal

#+begin_src python
import aesara.tensor as at

rng = at.random.RandomState()

# RandomVariables divide the rng
a_rv, rng = at.random.normal(rng, 0, 1)
b_rv, _ = at.random.normal(rng, 0, 1)

# We have to update the rng manually
a_rv = at.random.normal(rng, 0, 1)
rng = at.random.update(rng)
b_rv = at.random.normal(rng, 0, 1)

rng_a, rng_b = at.random.split(rng)
a_rv = at.random.normal(rng_a, 0, 1)
b_rv = at.random.normal(rng_b, 0, 1)

rngs = at.random.split(rng, 10)
rvs = []
for rng in rngs:
    rvs.append(at.random.normal(rng, 0, 1))
#+end_src

How does that solve the previous issues?

1. Monkey patching to specialize the RV =Op=\s
2. RVs in S-expressions and rewrites

What does that complicate?

#+begin_src python

def standard_normal():

#+end_src
