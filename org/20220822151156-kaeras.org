:PROPERTIES:
:ID:       ee2b16f2-0d64-4172-90bb-fa3f6dab3eac
:END:
#+title: AeNN


AeNN would be a deep-learning API that sits on top of Aesara. It is inspired by [[https://keras.io][Keras]] (which used to be built on top of Theano), Flax (NN library built with JAX), and of course [[https://github.com/Lasagne/Lasagne][Lasagne]]. It would make full use of Aesara's graph and rewrite capabilities.

* Inspiration

A minimum viable library could be to be able to reproduce [[https://keras.io/][Keras' documentation]] examples. For instance the MNIST convnet:

#+begin_src python
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation="softmax"),
    ]
)
#+end_src

I find Keras' API to be sometimes too high-level. In particular, I am not a big fan of the activation being a property of layers, and passed as a keyword argument. Activation functions are just functions, and we should allow users to use custom operators. [[https://github.com/google/flax][Flax]]'s API is more on point with that respect. I also like the =flax.linen.Module= interface to define blocks a lot:

#+begin_src python
import flax.linen as nn


class CNN(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Conv(features=32, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = nn.Conv(features=64, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = x.reshape((x.shape[0], -1))  # flatten
    x = nn.Dense(features=256)(x)
    x = nn.relu(x)
    x = nn.Dense(features=10)(x)
    x = nn.log_softmax(x)
    return x

model = CNN()
batch = jnp.ones((32, 10))
variables = model.init(jax.random.PRNGKey(0), batch)
output = model.apply(variables, batch)
#+end_src

* Building a simple MLP model

Let's take and try to reproduce a simpler example, the MLP example taken from the Flax documentation:

#+begin_src python
from typing import Sequence
import flax.linen as nn

class MLP(nn.Module):
    features: Sequence[int]
    @nn.compact
    def __call__(self, x):
        for feature in features[:-1]:
            x = nn.Dense(feature)(x)
            x = nn.relu(x)
        x = nn.Dense(features[-1])(x)
        return x
#+end_src

** Activation Function

The first approach to implementing the activation function $\operatorname{Relu}$ is to define it as a function:

#+begin_src python
import aesara.tensor as at

def relu(x):
    """Rectified linear unit activation function.

    .. math::

        \operatorname{relu}(x) = \max(0, x)

    """
    return at.max(0, x)
#+end_src

However, we need the gradient at $0$ to be:

$$
\nabla \operatorname{Relu}(0) = 0
$$

So =relu= will need to be implemented as an =OpFromGraph=, with a custom gradient. The upside of using =OpFromGraph= in this case is that the =Op=\s it builds are directly identifiable in the graph.

** Dense layer

Layers are best implemented as =OpFromGraph=s for several reasons:

1. We can target layers with rewrites; This can be useful for AutoML, or optimization at the mathematical level.
2. We can retrieve the layer's parameters by walking the graph and looking at the inputs of =Apply= nodes whose =Op= is of =Layer= type. We can even add type parameters to indicate which are trainable, regularizable, etc.
3. =aesara.dprint= shows the graph structure directly.

#+begin_src python
class Layer(OpFromGraph):
    """Represents a Layer.

    The difference between layers and transformations is that the former
    hold (potentially trainable) parameter values.
    """


class DenseLayer(Layer):
    """`Op` that represents a Dense Layer"""


class Dense(Op):
    features: int
    b: Optional[TensorVariable] = None
    W: Optional[TensorVariable] = None

    def __call__(self, x):

        if self.W is None:
            self.W = at.random.uniform(size=(x.shape[0], features))

        output = at.dot(x, self.W)
        if self.b is not None:
            output = output + self.b

        dense = DenseLayer(output, [x, self.W, self.b])
        return dense(parameters, x)
#+end_src

#+RESULTS:

** Module

** Training

We need to be able to call the model with =model(batch, parameters)=, then compute the loss, then update the parameter values using an optimizer.

* Graph rewriting

We can perform rewriting at the layer semantic level. For instance, the [[https://github.com/uwplse/tensat][TENSAT]] library adds equality saturation to the [[https://github.com/jiazhihao/TASO][TASO]] library. There are a list of rewrites that operate on a so-called /layer algebra/. Here are a few examples:

#+begin_src ascii
matmul(matmul(input_1,input_4),input_5)==matmul(input_1,matmul(input_4,input_5))
conv2d(1,1,0,2,ewadd(input_12,ewadd(input_10,input_11)),ewadd(input_12,ewadd(input_10,input_11)))==conv2d(1,1,0,2,ewadd(input_11,ewadd(input_10,input_12)),ewadd(input_11,ewadd(input_10,input_12)))
poolavg(3,3,1,1,0,input_8)==conv2d(1,1,0,0,input_8,Cpool(3,3))
relu(input_8)==conv2d(1,1,0,2,input_8,Iconv(3,3))
#+end_src
