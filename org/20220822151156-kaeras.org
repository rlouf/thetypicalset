:PROPERTIES:
:ID:       ee2b16f2-0d64-4172-90bb-fa3f6dab3eac
:END:
#+title: AeNN


AeNN would be a deep-learning API that sits on top of Aesara. It is inspired by [[https://keras.io][Keras]] (which used to be built on top of Theano), Flax (NN library built with JAX), and of course [[https://github.com/Lasagne/Lasagne][Lasagne]]. It would make full use of Aesara's graph and rewrite capabilities.

* Inspiration

A minimum viable library could be to be able to reproduce [[https://keras.io/][Keras' documentation]] examples. For instance the MNIST convnet:

#+begin_src python
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation="softmax"),
    ]
)
#+end_src

I find Keras' API to be sometimes too high-level. In particular, I am not a big fan of the activation being a property of layers, and passed as a keyword argument. Activation functions are just functions, and we should allow users to use custom operators. [[https://github.com/google/flax][Flax]]'s API is more on point with that respect. I also like the =flax.linen.Module= interface to define blocks a lot:

#+begin_src python
import flax.linen as nn


class CNN(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Conv(features=32, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = nn.Conv(features=64, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = x.reshape((x.shape[0], -1))  # flatten
    x = nn.Dense(features=256)(x)
    x = nn.relu(x)
    x = nn.Dense(features=10)(x)
    x = nn.log_softmax(x)
    return x

model = CNN()
batch = jnp.ones((32, 10))
variables = model.init(jax.random.PRNGKey(0), batch)
output = model.apply(variables, batch)
#+end_src

* Building a simple MLP model

Let's take and try to reproduce a simpler example, the MLP example taken from the Flax documentation:

#+begin_src python
from typing import Sequence
import flax.linen as nn

class MLP(nn.Module):
    features: Sequence[int]
    @nn.compact
    def __call__(self, x):
        for feature in features[:-1]:
            x = nn.Dense(feature)(x)
            x = nn.relu(x)
        x = nn.Dense(features[-1])(x)
        return x
#+end_src

** Activation Function

The first approach to implementing the activation function $\operatorname{Relu}$ is to define it as a function:

#+begin_src python
import aesara.tensor as at

def relu(x):
    """Rectified linear unit activation function.

    .. math::

        \operatorname{relu}(x) = \max(0, x)

    """
    return at.max(0, x)
#+end_src

However, we need the gradient at $0$ to be:

$$
\nabla \operatorname{Relu}(0) = 0
$$

So =relu= will need to be implemented as an =OpFromGraph=, with a custom gradient. The upside of using =OpFromGraph= in this case is that the =Op=\s it builds are directly identifiable in the graph.

** Dense layer

Layers are best implemented as =OpFromGraph=s for several reasons:

1. We can target layers with rewrites; This can be useful for AutoML, or optimization at the mathematical level.
2. We can retrieve the layer's parameters by walking the graph and looking at the inputs of =Apply= nodes whose =Op= is of =Layer= type. We can even add type parameters to indicate which are trainable, regularizable, etc.
3. =aesara.dprint= shows the graph structure directly.

#+begin_src python :results output
from typing import Optional

import aesara
import aesara.tensor as at
from aesara.tensor.var import TensorVariable
from aesara.compile.builders import OpFromGraph

class Layer(OpFromGraph):
    """Represents a Layer.

    The difference between layers and transformations is that the former
    hold (potentially trainable) parameter values.
    """


class DenseLayer(Layer):
    """`Op` that represents a Dense Layer"""


class Dense():

    def __init__(self, features: int, W: Optional[TensorVariable], b: Optional[TensorVariable]):
        self.features = features
        self.W = W
        self.b = b

    def __call__(self, x):
        output = at.dot(x, self.W) + self.b
        dense = DenseLayer([x, self.W, self.b], [output])
        return dense(x, self.W, self.b)


x = at.matrix("X")
W = at.vector("W")
b = at.scalar("b")

out = Dense(x.shape[1], W, b)(x)

aesara.dprint(out)
# DenseLayer{inline=False} [id A]
#  |X [id B]
#  |W [id C]
#  |b [id D]
#
# DenseLayer{inline=False} [id A]
#  >Elemwise{add,no_inplace} [id E]
#  > |dot [id F]
#  > | |*0-<TensorType(float64, (None, None))> [id G]
#  > | |*1-<TensorType(float64, (None,))> [id H]
#  > |InplaceDimShuffle{x} [id I]
#  >   |*2-<TensorType(float64, ())> [id J]

assert isinstance(out.owner.op, Layer)
print(out.owner.inputs)
# [X, W, b]
#+end_src

#+RESULTS:
#+begin_example
DenseLayer{inline=False} [id A]
 |X [id B]
 |W [id C]
 |b [id D]

Inner graphs:

DenseLayer{inline=False} [id A]
 >Elemwise{add,no_inplace} [id E]
 > |dot [id F]
 > | |*0-<TensorType(float64, (None, None))> [id G]
 > | |*1-<TensorType(float64, (None,))> [id H]
 > |InplaceDimShuffle{x} [id I]
 >   |*2-<TensorType(float64, ())> [id J]
[X, W, b]
#+end_example

Representing layers as =Ops= has several advantages:
1. More readable =aesara.dprint= outputs;
2. Parameters can be directly recovered by walking the graphs;
3. Layers can be targetted by rewrites, which opens possibilities for optimizations and also AutoML (we can replace layers);
4. We can have layer-specific rules for transpilation. XLA has convolution-specific Ops.

** TODO Module

How do we define a module in a similar way we defined =MLP= above with Flax? Is there anything special about modules compared to normal layers? Should we attribute a specific =Module= type to them, as opposed to =Layer=? If we consider they're merely a way to define a new layer they should be implemented as =OpFromGraph= as well. In this case we should have a general way to define layers so the code looks like:

** Training

We need to be able to call the model with =model(batch, parameters)=, then compute the loss, then update the parameter values using an optimizer.

#+begin_src python
class MLP(nn.Module):
    features: Sequence[int]

    @nn.module.from_graph
    def __call__(self, x):
        """This returns a layer as an `Op` named `MLP`.
        """
        for feature in features[:-1]:
            x = nn.Dense(feature)(x)
            x = nn.relu(x)
        x = nn.Dense(features[-1])(x)
        return x
#+end_src

* Graph rewriting

We can perform rewriting at the layer semantic level, for two reasons:
1. **AutoML:** We can swap activation functions, layer sizes, etc.
2. **Performance:** For instance, the [[https://github.com/uwplse/tensat][TENSAT]] library adds equality saturation to the [[https://github.com/jiazhihao/TASO][TASO]] library. There are a list of rewrites that operate on a so-called /layer algebra/. Here are a few examples:

#+begin_src ascii
matmul(matmul(input_1,input_4),input_5)==matmul(input_1,matmul(input_4,input_5))
conv2d(1,1,0,2,ewadd(input_12,ewadd(input_10,input_11)),ewadd(input_12,ewadd(input_10,input_11)))==conv2d(1,1,0,2,ewadd(input_11,ewadd(input_10,input_12)),ewadd(input_11,ewadd(input_10,input_12)))
poolavg(3,3,1,1,0,input_8)==conv2d(1,1,0,0,input_8,Cpool(3,3))
relu(input_8)==conv2d(1,1,0,2,input_8,Iconv(3,3))
#+end_src
