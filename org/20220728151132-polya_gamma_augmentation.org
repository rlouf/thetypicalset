:PROPERTIES:
:ID:       16338bc2-222c-4acf-aa28-38b951dfcb89
:END:
#+title: Polya-Gamma augmentation

#+begin_abstract
In this note we will go over the Polya-Gamma augmentation for logistic regression models.
#+end_abstract

Let us consider the binary target $\boldsymbol{y} = \left[ y_1, \dots, y_N\right]$ and the dependant variables $X = \left[\boldsymbol{x}_1, \dots, \boldsymbol{x}_N\right]$ such that

\begin{equation*}
P\left(\boldsymbol{y} | X, \boldsymbol{\beta} \right) = \prod_{i=1}^N P\left(y_i | \boldsymbol{x}_i, \boldsymbol{\beta} \right)
\end{equation*}

We further assume that

\begin{align*}
y_i | \boldsymbol{x_i}, \boldsymbol{\beta} &\sim \operatorname{Bernoulli}\left(p_i\right)\\
p_i & = \sigma(\boldsymbol{x_i}^T\,\boldsymbol{\beta})\\
\sigma(x) &= \left(1 + \exp(-x)\right)^{-1}
\end{align*}

It can be shown that

\begin{equation*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) = \frac{\exp\left(y_i\: \boldsymbol{x}_i^T \boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_i^T \boldsymbol{\beta}\right)}
\end{equation*}

while assuming a normal prior on each coefficient $\beta_j$

\begin{equation*}
\beta_j \sim \operatorname{N}\left(\mu, \sigma^2\right)
\end{equation*}

So the posterior distribution:

$$
P(\beta|y, X) \propto P(Y| X, \beta) P(\beta)
$$

does not have a closed-form solution.

But behold! Polson et al. remind us of this neat result:


\begin{align*}
\frac{\left(e^u\right)^a}{\left(1 + e^u\right)^b} &= \frac{1}{2^b}\, e^{\kappa u}\,\int_0^\infty e^{-\frac{u^2}{2} \omega}\; p(\omega)\, \mathrm{d}\omega\\
\kappa &= a - \frac{b}{2}\\
p(\omega) &= \mathrm{PG}\left(\omega|b, 0\right)
\end{align*}


So if we set $b=1$, $u = \boldsymbol{x}_i^T \boldsymbol{\beta}$ and $a = y_i$ the following holds:

\begin{align*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) &= \frac{1}{2} \int_0^\infty \exp\left( y_i \boldsymbol{x_i}^T\,\boldsymbol{\beta} - \frac{\left(\boldsymbol{x_i}^T\,\boldsymbol{\beta}\right)^2}{2} \omega\right)\;P(\omega) \mathrm{d}\omega\\
&= \frac{1}{2} \int_0^\infty \exp\left( -\frac{\omega}{2} \left( \frac{y_i}{\omega} - \boldsymbol{x_i}^T\,\boldsymbol{\beta}\right)^2\right)\;P(\omega) \mathrm{d}\omega\\
\end{align*}


I guess Gibbs sampling works because we're able to successively

* Bernoulli likelihood

Let us derive the likelihood $p(\boldsymbol{y} | x, \boldsymbol{\beta} )$ assuming that the observations are i.i.d. from the same distribution:

\begin{align*}
P\left(\boldsymbol{y} | x, \boldsymbol{\beta}\right) &= \prod_{i=1}^N P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right)\\
&=\prod_{i=1}^n\; p_i^{\,1-y_i}\,\left(1 - p_i\right)^{y_i}\\
&=\prod_{i=1}^n\; \left[ \frac{\exp(-x_i^T \boldsymbol{\beta})}{1 + \exp(-x_i^T \boldsymbol{\beta})}\right]^{\,1-y_i}\,\left[1 - \frac{\exp(-x_i^T \boldsymbol{\beta})}{1 + \exp(-x_i^T \boldsymbol{\beta})}\right]^{y_i}\\
&=\prod_{i=1}^n\; \left[ \frac{\exp(-x_i^T \boldsymbol{\beta})}{1 + \exp(-x_i^T \boldsymbol{\beta})}\right]^{\,1-y_i}\,\left[\frac{1}{1 + \exp(-x_i^T \boldsymbol{\beta})}\right]^{y_i}\\
&=\prod_{i=1}^n\; \frac{\left[ \exp(-x_i^T \boldsymbol{\beta}) \right]^{\,1-y_i}}{1 + \exp(-x_i^T \boldsymbol{\beta})}\\
&=\prod_{i=1}^n\; \frac{\left( \exp(x_i^T \boldsymbol{\beta}) \right)^{\,y_i}}{1 + \exp(x_i^T \boldsymbol{\beta})}\\
\end{align*}


So that:
\begin{equation*}
\mathcal{L}_i(\boldsymbol{\beta}) = P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) = \frac{\left(\exp\left(\boldsymbol{x}_i^T \boldsymbol{\beta}\right)\right)^{y_i}}{1 + \exp\left(\boldsymbol{x}_i^T \boldsymbol{\beta}\right)}
\end{equation*}

* References

- Dunson, etc. Bayesian inference for logistic models using Polya-Gamma latent variables. (2013)
- Slides: [[http://www.gatsby.ucl.ac.uk/tea/tea_archive/attached_files/polya-gamma-slides.pdf]]
- https://tiao.io/post/polya-gamma-bayesian-logistic-regression/#i
- https://gregorygundersen.com/blog/2019/09/20/polya-gamma/
