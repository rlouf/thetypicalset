#+title: Random Walk Rosenbluth-Metropolis-Hastings in Aesara


Right before I started working on [[file:~/projects/thetypicalset/org/blog/introducing-mcx.org][MCX]] I wrote a simple benchmarks for PyTorch, Tensorflow and JAX on a very simple problem: using the random walk Rosenbluth-Metropolis-Hastings algorithm to sample from a mixture distribution. MCX was discontinued a bit more than a year ago, when I started working with a PPL based on [[https://github.com/aesara-devs/aesara][Aesara]]. So let me revisit this simple example using Aeasara!

The full code was added to the [[https://github.com/rlouf/blog-benchmark-rwmetropolis/blob/master/aesara_sampler.py][repository]].

- [x] Implement a mixture model with =Aesara=
- [x] We can implement the Rosenbluth-Metropolis-Hastings algorithm in =Aesara=
- [ ] Run on several chains
  It is not clear how we can do this using =aeppl='s =joint_logprob=.
  Of course I can just use the normal
- [ ] Performance comparison with JAX on a single chain.
- [ ] Compile to JAX
- [ ] Compile to Numba
- [ ] What's missing & how we're going to get there

* Mixture model

[[file:~/projects/thetypicalset/org/blog/jax-parallel-mcmc.org][In the original blog post]] I set to sample from a mixture distribution with 4 components. I had to write the corresponding log-probability density function by hand, i.e. without using a PPL. Implementing a mixture model in =Aesara= is straightforward. No need for a =Mixture= distribution (like in e.g. PyMC), you just write it like it is:

#+begin_src python :session :results silent :exports code
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

loc = np.array([-2, 0, 3.2, 2.5])
scale = np.array([1.2, 1, 5, 2.8])
weights = np.array([0.2, 0.3, 0.1, 0.4])

N_rv = srng.normal(loc, scale, name="N")
I_rv = srng.categorical(weights, name="I")
Y_rv = N_rv[I_rv]
#+end_src

We can generate forward samples from this model by compiling the model graph choosing =Y_rv= as an output:

#+begin_src python :session :results output
import aesara

sample_fn = aesara.function((), Y_rv)
samples = [sample_fn() for _ in range(10000)]

print(samples[:100])
#+end_src

#+RESULTS:
: [array(2.51645571), array(0.16094803), array(4.16173818), array(-0.75365736), array(0.91897138), array(-1.96086176), array(2.60226408), array(2.28198192), array(-1.05260784), array(1.38469404), array(5.98739744), array(7.37274208), array(-2.15961733), array(8.43786623), array(-1.528235), array(-2.56085612), array(-1.49792648), array(-0.54058883), array(-1.73614481), array(2.68343646), array(-0.74588224), array(5.81172156), array(2.63622568), array(-1.52262747), array(-3.17441886), array(-0.83029964), array(2.57077231), array(-0.68490254), array(1.49112131), array(4.51531186), array(1.26036493), array(2.8718194), array(0.42948448), array(6.96430393), array(-1.41820233), array(-0.48159976), array(6.46244479), array(7.17495802), array(4.62729473), array(3.75983318), array(1.35390152), array(0.72638649), array(7.83843779), array(-1.93741605), array(-0.97753687), array(-2.6341784), array(-0.80476746), array(2.65904953), array(0.89057595), array(4.85358388), array(-0.30544774), array(-1.71254491), array(1.53838811), array(2.4976651), array(-4.49094492), array(-3.10530498), array(1.14952679), array(-2.0286298), array(-1.54603067), array(-2.27386863), array(-0.51537537), array(0.72265698), array(2.98239204), array(3.03497124), array(2.88541305), array(-0.79794257), array(-2.91166205), array(-1.14063909), array(-1.46864571), array(10.10318879), array(1.37147168), array(1.33971956), array(1.10113552), array(-1.23877521), array(0.35713318), array(1.36438476), array(-2.02780756), array(3.95659427), array(0.00671702), array(0.7383008), array(0.11008148), array(1.4652128), array(7.25069105), array(0.93955977), array(-0.07202498), array(-3.55905252), array(1.63349073), array(0.57912606), array(0.33508746), array(6.45579571), array(-0.14630237), array(-3.94949758), array(-1.6464691), array(0.50124777), array(-0.58339942), array(0.64205155), array(-4.46255652), array(1.17787744), array(-0.74443437), array(3.70892789)]

If you are not familiar with Theano/Aesara, the =aesara.function= may surprise you. What does it do exactly? When you manipulate Aesara tensors, you are not manipulating numbers, but rather you are /describing the computation to perform on the inputs/. As a result, the result of an Aesara operation is a graph:

#+begin_src python :session :results output
aesara.dprint(Y_rv)
#+end_src

#+RESULTS:
#+begin_example
Subtensor{int64} [id A]
 |normal_rv{0, (0, 0), floatX, False}.1 [id B] 'N'
 | |RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7F83CD6F7220>) [id C]
 | |TensorConstant{[]} [id D]
 | |TensorConstant{11} [id E]
 | |TensorConstant{[-2.   0. .. 3.2  2.5]} [id F]
 | |TensorConstant{[1.2 1.  5.  2.8]} [id G]
 |ScalarFromTensor [id H]
   |categorical_rv{0, (1,), int64, False}.1 [id I] 'I'
     |RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7F83CD7444A0>) [id J]
     |TensorConstant{[]} [id K]
     |TensorConstant{4} [id L]
     |TensorConstant{[0.2 0.3 0.1 0.4]} [id M]
#+end_example

=aesara.function= is therefore used to /compile/ the graph into a function that can be executed. For that, we need to specify the inputs and outputs of the function. In this case there are no outputs, and the value of =Y_rv= is the output.

To compute the log-probability density function we can use [[https://github.com/aesara-devs/aeppl][AePPL]]'s =joint_logprob= function. AePPL transforms the Aesara model graph to get the graph that computes the model's joint logprob (see, working with computation graphs is nice!). We pass a dictionary that tells which value to associate with the random variables =Y_rv= and =I_rv=:

#+begin_src python :session :results output :exports both
from aeppl import joint_logprob

y_vv = Y_rv.clone()
i_vv = I_rv.clone()
logprob = joint_logprob({Y_rv: y_vv, I_rv: i_vv})

print(logprob.eval({y_vv: 10., i_vv: 3}))
#+end_src

#+RESULTS:
: -6.452221131239579

Here we do not really care about the values that =I_rv= takes, so we marginalize the log-probability density function:

#+begin_src python :session :results output
logprob = []
for i in range(4):
    i_vv = at.as_tensor(i, dtype="int64")
    logprob.append(joint_logprob({Y_rv: y_vv, I_rv: i_vv}))
logprob = at.stack(logprob, axis=0)
total_logprob = at.logsumexp(at.log(weights) + logprob)

print(total_logprob.eval({y_vv: 10.}))
#+end_src

#+RESULTS:
: -6.961941398089025

* Implement the algorithm

The random walk Rosenbluth-Metropolis-Hasting algorithm is also straightforward to implement:

#+begin_src python
def rw_metropolis_kernel(srng, logprob_fn):
    """Build the random walk Rosenbluth-Metropolis-Hastings (RNH) kernel."""

    def one_step(position, logprob):
        """Generate one sample using the random walk RMH algorithm.

        Attributes
        ----------
        position:
            The initial position.
        logprob:
            The initial value of the logprobability.

        Returns
        ------
        The next positions and values of the logprobability.

        """
        move_proposal = 0.1 * srng.normal(0, 1)
        proposal = position + move_proposal
        proposal_logprob = logprob_fn(proposal)

        log_uniform = at.log(srng.uniform())
        do_accept = log_uniform < proposal_logprob - logprob

        position = at.where(do_accept, proposal, position)
        logprob = at.where(do_accept, proposal_logprob, logprob)

        return position, logprob

    return one_step
#+end_src

Syntactically, =aesara.tensor= looks like a drop-in replacement to =numpy=. Remember, however, that these functions do not act on numbers but add an operation to an existing graph of computation. In particular, =logprob_fn= is a function that takes a graph (possibly a single variable), and returns the graph that computes the value of the log-probability density function.


* So, does it work?

* Perspectives

** TODO Automatic marginalization of the =Categorical= RV

Here we manually marginalized over the values of =Y_rv=, but it would be nice if we could automatically marginalize over discrete variables.

[Link to the issue]

** TODO Multiple backend

Having multiple backends means that you can interact with different ecosystems / hardware with the same model expression in Python. You can also decide to leverage the strengths of XLA (JAX) or LLVM (Numba) depending on your application.

*** TODO Compile to Numba

When nothing is specified =Aesara= compiles functions to C. We have however introduced a new Numba backend to =Aesara=.

[Example]
[Link to the documentation]

*** TODO Compile to JAX

We can also compile functions to JAX so they can play nice with JAX ecosystems.

[Example]
[Link to the documentation]


** TODO Vectorize the execution (several chains)

We cannot vectorize the execution the same we can in =JAX= with =vmap=, but we should be able to. How?

[Link to the relevant issues]
