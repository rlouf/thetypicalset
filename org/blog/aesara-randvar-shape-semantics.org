#+title: Shape semantics for random variables in Aesara
#+date: <2022-08-17 Wed>
#+PROPERTY: header-args :results output :eval never-export :exports both

**Although this contains useful information, this post is a draft and will probably be integrated to Aesara's documentation**

* Random variables in Aesara

Random variables are represented in [[id:5a5e87b1-558c-43db-ad38-32a073b10351][Aesara]] with the =RandomVariable= operator, which corresponds to the following mathematical function:

$$
\operatorname{RandomVariable}: \Omega \times \Theta \to E
$$

Were $\Omega$ is the set of available RNG seeds, $\Theta$ the parameter space. $E$ is the state space, which corresponds to the support of the corresponding distribution. Given a random seed and parameter values, the operator returns a *realization* of the random variable it represents, i.e. element of $E$.

The recommended way to use random variables in Aesara is via the =RandomStream= interface, which automatically seeds the random functions. Here, =srng.normal= defines a single, normally-distributed, random variable:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
x_rv = srng.normal(0, 1)
print(x_rv.eval())
#+end_src

#+RESULTS:
: 1.4436909546981256

The shape of the output of the random functions depends on three things:
- The /support shape/ of the random variable;
- Broadcasting rules between the parameters of the random variable's distribution;
- The =size= parameter passed to the random function.

In this document we will make explaine the semantics of shape in Aesara, and what they represent.

* The support shape

The dimensionality of the parameter space and the sample space differs depending on the distribution. For instance, the normal distribution is parametrized by $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$ and the realizations of the corresponding random variables are scalars $\in \mathbb{R}$.

#+begin_src python :results output
import aesara.tensor as at

srng = at.random.RandomStream(0)

mu = 0
sigma = 1
x_rv = srng.normal(mu, sigma)
sample = x_rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: 1.4436909546981256
: support shape: ()

We say that the support shape of =normal= is =()=. The Dirichlet distribution is slightly more complicated: it is parametrized by a vector $\boldsymbol{\alpha} \in \mathbb{R}^k$ and its realizations are vectors in the k-unit simplex $\operatorname{\Delta}^k$:

#+begin_src python :results output
import aesara.tensor as at

srng = at.random.RandomStream(0)

alpha = [1., 3., 4.]
x_rv = srng.dirichlet(alpha)
sample = x_rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [0.39086221 0.17265609 0.43648169]
: support shape: (3,)

The support shape of =dirichlet= is =(k,)=, with =k= the length of its parameter $\alpha$. The multinomial is another interesting example because the dimensionality of its parameters; it is parametrized by a probability vector $\boldsymbol{p} \in \Delta^k$, a number of trials $n \in \mathbb{N}$ and returns a vector in $\mathbb{N}^k$:

#+begin_src python :results output
import aesara.tensor as at

srng = at.random.RandomStream(0)

n = 10
p = [.1, .3, .6]
x_rv = srng.multinomial(n, p)
sample = x_rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [3 2 5]
: support shape: (3,)

The support shape of =multinomial= is =(k,)= with =k= the length of the probability vector $p$. Here we have only considered random variables with a 0- or 1-dimensional sample space, but it can obviously be more complicated. The random variable with a [[https://en.wikipedia.org/wiki/Wishart_distribution][Wishart density]] is a function that maps to $\mathbb{R}^{n \times m}$, and the corresponding support shape is thus =(n,m)=.

The support shape and how it relates to the shape of the parameters is explicited in [[https://aesara.readthedocs.io/en/latest/library/tensor/random/basic.html][the documentation]], where we attach a =signature= string to each random variable (these are [[https://numpy.org/doc/stable/reference/c-api/generalized-ufuncs.html][gufunc]]-like signatures). For instance, for the previous examples we have:

- Normal: =(), () -> ()=
- Dirichlet: =(n) -> (n)=
- Multinomial: =(), (n) -> (n)=

* The batch shape

We just saw how to define a single random variables with different distributions, and how the choice of distribution determined the /support shape/. In many realistic settings, however, we would like to define several independently distributed random variables at once. Aesara provides two mechanisms to do so: broadcasting of the parameters, and the =size= parameter. The shape induced by this mechanism is called the /batch shape/.

** Batching by broadcasting

Say we want a sample from three independent, normally distributed,  random variables with a mean of $0$, $3$ and $5$ respectively. One (cumbersome) way to achieve this is:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream()
rv_0 = srng.normal(0, 1)
rv_3 = srng.normal(3, 1)
rv_5 = srng.normal(5, 1)
rv = at.stack([rv_0, rv_3, rv_5])

sample = rv.eval()
print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [1.65040785 1.76749492 5.86773357]
: sample shape: (3,)

To simplify this common operation, we can pass arrays as parameters to Aesara's =RandomVariable=, and the =Op= will use NumPy broadcasting rules to return an array of independent random variables:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
rv = srng.normal(mean, 1)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [1.44369095 2.10405402 5.73595567]
: sample shape: (3,)

 In this case the /batch shape/ is also  =(3,)=; it is the shape of the tensor that contains random variables that are independently distributed and whose distribution belong to the same family.

 In this case, =srng.normal(mean, 1)= implicitly represents 3 independent random variables; if it helps one can imagine it is a shortcut for the first code block of this section.

 We can also use arrays for the standard deviation in this case. Standard broadcasting rules apply to determine the batch shape. For instance, the following fails with a shape mismatch error:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2])
rv = srng.normal(mean, sigma)

try:
    rv.eval()
except ValueError as err:
    print(err)
#+end_src

#+RESULTS:
#+begin_example
shape mismatch: objects cannot be broadcast to a single shape
Apply node that caused the error: normal_rv{0, (0, 0), floatX, True}(RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FDB97DFD200>), TensorConstant{[]}, TensorConstant{11}, TensorConstant{[0 3 5]}, TensorConstant{[1 2]})
Toposort index: 0
Inputs types: [RandomGeneratorType, TensorType(int64, (0,)), TensorType(int64, ()), TensorType(int64, (3,)), TensorType(int64, (2,))]
Inputs shapes: ['No shapes', (0,), (), (3,), (2,)]
Inputs strides: ['No strides', (8,), (), (8,), (8,)]
Inputs values: [Generator(PCG64) at 0x7FDB97DFD200, array([], dtype=int64), array(11), array([0, 3, 5]), array([1, 2])]
Outputs clients: [['output'], ['output']]

HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
#+end_example

Indeed =mean= and =sigma= cannot be broadcast together:

#+begin_src python
import numpy as np

mean = np.array([0, 3, 5])
sigma = np.array([1, 2])
try:
    np.broadcast(mean, sigma)  # error
except ValueError as err:
    print(err)
#+end_src

#+RESULTS:
: shape mismatch: objects cannot be broadcast to a single shape

=np.broadcast(mean, sigma)= gives us the batch shape:

#+begin_src python
import numpy as np

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 7])
print(np.broadcast(mean, sigma).shape)
#+end_src

#+RESULTS:
: (3,)

Indeed:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 3])
rv = srng.normal(mean, sigma)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"batch shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [1.44369095 1.20810805 7.20786701]
: batch shape: (3,)

Since the =RandomVariable= represents a batch of random variables, we will call the resulting shape the *batch shape*.

The normal distribution is fairly simple since its parameters and realization are 1-dimensional. Let take our dirichlet example:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

alpha = np.array([[1., 2., 4.], [3., 5., 7.]])
rv = srng.dirichlet(alpha)
sample = rv.eval()

print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[0.42615878 0.09794332 0.4758979 ]
:  [0.15408529 0.34781447 0.49810024]]
: sample shape: (2, 3)

Which is equivalent to:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

rv1 = srng.dirichlet([1., 2., 4.])
rv2 = srng.dirichlet([3., 5., 7.])
rv = at.stack([rv1, rv2])
sample = rv.eval()

print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[0.42615878 0.09794332 0.4758979 ]
:  [0.27582652 0.02985376 0.69431972]]
: sample shape: (2, 3)

So we have the simple formula; if =support_shape= and =batch_shape= are tuples, then:

#+begin_quote
sample_shape = batch_shape + support_shape
#+end_quote

** Expanding to create identically distributed random variables

We also frequently need to define iiid random variables. We can define 3 normally-distributed random variables with mean 0 and variance 1 with:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.zeros(3)
rv = srng.normal(mean, 1)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [ 1.44369095 -0.89594598  0.73595567]
: sample shape: (3,)

But there is a shortcut: the =size= parameter of the distribution. In the following code, =size= allows us to define the same 3 random variables as above in a more concise way:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

rv = srng.normal(0, 1, size=3)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [ 1.44369095 -0.89594598  0.73595567]
: sample shape: (3,)

We can of course do the same thing with the dirichlet distribution:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

rv = srng.dirichlet([1, 3, 5], size=3)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[0.34934376 0.15431609 0.49634016]
:  [0.16080299 0.37886972 0.4603273 ]
:  [0.21030357 0.42525361 0.36444282]]
: sample shape: (3, 3)

Since we are still talking about independent random variables, =batch= refers indistinctly to identifically distributed or differently distributed random variables.

** Broadcasting and expanding

*Size corresponds to the batch shape*

/(show an example where broadcasting and using =size= breaks)/

It is possible to vectorize and batch at the same time. Note that =size= and that vectorized shape must be broadcastable

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 3])
rv = srng.normal(mean, sigma, size=(2, 2, 3))

sample = rv.eval()
print(f"sample values: {sample}")
print(f"batch shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[[1.44369095e+00 1.20810805e+00 7.20786701e+00]
:   [5.87704041e-03 4.70676358e+00 5.48284410e+00]]
:
:  [[8.19314690e-01 4.61131137e+00 5.65270195e+00]
:   [9.70078743e-01 1.52177388e+00 6.78043377e+00]]]
: batch shape: (2, 2, 3)

where =np.broadcast(mean, sigma).shape= must correspond to the last dimensions of =size=. Or in other words, the sample shape is =np.broadcast_shapes(np.broadcast(mean, sigma).shape, size)= if this does not raise an error.

It IS really simple:

=sample_shape = np.broadcast_shapes(np.broadcast(*args), size)=


* Summary

The shape of random tensors in [[id:5a5e87b1-558c-43db-ad38-32a073b10351][Aesara]] is partitioned in semantically different pieces, that refer to the shape of $E$, i.e. the shape of draws we get:
- The *support shape* corresponds to the shape of one element in $E$, the support of the distribution;
- The *batch shape* is the number $N$ of independent random variables $X_i: \Omega \to E$ where $i \in \left\{ 1 \dots N\right\}$; These can be identically or differently distributed.
- =RandomVariable= creates /differently distributed/ random variables by passing different values of parameters to the operators. Broadcasting rules apply, and the *batch shape* (i.e. number of differently distributed random variables) is inferred from these broadcasting rules.
- =RandomVariable= creates /identically distributed/ random variables via the =size= keyword argument. There is a level of indirection here; the shape specified by =size= must broadcast with the shape of the broadcasted arguments. So if the latter is =(a, b)=, to define =c= identically distributed RVs one must set =size= to =(c, a, b)=.
- The shape of the array of random variables is given by =sample_shape = batch_shape + support_shape=
