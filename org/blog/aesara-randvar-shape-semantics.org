#+title: Shape semantics for random variables in Aesara
#+date: <2022-08-17 Wed>
#+PROPERTY: header-args :results output :eval never-export :exports both

**DRAFT**

We can relate the =RandomVariable= objects in Aesara to their mathematical counterpart by tying the set of possible RNG states to the sample space $\Omega$. If the random variable $X$ takes its values in the measurable space $E$, then $X$ can be understod as a function $X: \Omega \to E$. It is important to undestand that the =RandomVariable= operator does not represent random variables, but /creates/ random variables (the apply nodes that take random draws). We represent $E$ by its elements; although the returned arrays are symbolic representations of elements of $E$, they do not represent the values themselves, which are obtained by compiling an running the function. We represent $X$ by its output (we conveniently forget about $\omega$)

The operators return *realizations* of the random variables.

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
x_rv = srng.normal(0, 1)

# srng.normal(0, 1) : \Omega \to E
# x \in E
# Other consideration are relevant for AePPL, here we just need to know they're elements of $E$.

#+end_src

The shape of random tensors in [[id:5a5e87b1-558c-43db-ad38-32a073b10351][Aesara]] is partitioned in semantically different pieces, that refer to the shape of $E$, i.e. the shape of draws we get:
- The *support shape* corresponds to the shape of one element in $E$, the support of the distribution;
- The *batch shape* is the number $N$ of independent random variables $X_i: \Omega \to E$ where $i \in \left\{ 1 \dots N\right\}$; These can be identically or differently distributed.
- =RandomVariable= creates /differently distributed/ random variables by passing different values of parameters to the operators. Broadcasting rules apply, and the *batch shape* (i.e. number of differently distributed random variables) is inferred from these broadcasting rules.
- =RandomVariable= creates /identically distributed/ random variables via the =size= keyword argument. There is a level of indirection here; the shape specified by =size= must broadcast with the shape of the broadcasted arguments. So if the latter is =(a, b)=, to define =c= identically distributed RVs one must set =size= to =(c, a, b)=.
- The shape of the array of random variables is given by =sample_shape = batch_shape + support_shape=

  #+begin_quote
What's confusing about all this is that we are passing a single RNG state to the distribution even when we are looking a realization of several different random variables. Underlying implementation must use different rng states, though, and we should probably talk about this.
  #+end_quote


* Support shape

The first distinction we must make is based on the /support/ $E$ of the distributions. Simply put, the support of a distribution is the domain over which the density of the probability measure is defined, i.e.

The length of the /support shape/ is the equal to the dimensionality of the distribution's support.
- 0 for distributions defined in 1 dimension (normal distribution, etc.)
- Vectors (1 dimension)
- Matrices (2 dimensions)

*The support shape is the shape of one draw.*

** Distributions over scalar values

The support of the normal distribution, for instance, is $\mathbb{R}$: every sample drawn from that distribution is an element of $\mathbb{R}$. The support of the bernoulli distribution, on the other hand, is the set of integers $\left\{0, 1\right\}$. These distributions are /scalar distributions/, in the sense that their support has 0 dimension (in /array/ terms). Their support shape is /always/ =(,)=.

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
rv = srng.normal(0, 1)
sample = rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: 1.4436909546981256
: support shape: ()

** Distributions over vectors

Other distributions are defined on more complex spaces. The multivariate normal distribution is defined on $\mathbb{R}^k$, so a sample from this distribution will be a vector of length $k$. In this case the support shape is thus equal to =(k,)=.

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

k = 4
mu = np.random.rand(k)
sigma = np.eye(k)
rv = srng.multivariate_normal(mu, sigma)
sample = rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [ 2.39376392 -0.59212003  1.48160085  0.18195058]
: support shape: (4,)

We can define a multivariate normal distribution with one component, and for consistency its support shape is equal to =(1,)=.

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mu = np.array([1])
sigma = np.eye(1)
rv = srng.multivariate_normal(mu, sigma)
sample = rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [2.44369095]
: support shape: (1,)

Example of distributions with 1-dimensional support: =multivariate_normal=, =categorical=, =multinomial=, =dirichlet=.

** Distributions over matrices, etc.

Finally, distributions can be defined on a space with an arbitrary number of dimensions, for instance over $\mathbb{R}^{n \times m}$. The [[https://en.wikipedia.org/wiki/Wishart_distribution][Wishart distribution]] for instance is 2-dimensional: a sample from this distribution is a matrix.


* Batch shape

Let us only consider distributions over scalar values for now.

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
rv = srng.normal(0, 1)
sample = rv.eval()

print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: 1.4436909546981256
: sample shape: ()

The =batch shape= encompasses one reality: the number of independent random variables we want to define using a probability distribution. There are two ways to arrive at the same batch shape:
- Setting the shape explicitly;
- Broadcasting of parameter arrays.

** Batching

If we want to take independent and indentically distributed samples from a distribution we can use the =size= keyword argument. Here we are requesting $3$ independent and identically distributed random variables:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
rv = srng.normal(0, 1, size=(3,))  # size=3 gives the same result
sample = rv.eval()

print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [ 1.44369095 -0.89594598  0.73595567]
: sample shape: (3,)

We can obviously require that these independent and identically distributed samples be returned in a tensor of any shape:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
batch_shape = (2, 2, 2)
rv = srng.normal(0, 1, size=batch_shape)
sample = rv.eval()

print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [[[ 1.44369095 -0.89594598]
:   [ 0.73595567  0.00587704]]
:
:  [[ 0.85338179  0.16094803]
:   [ 0.81931469  0.80565568]]]
: sample shape: (2, 2, 2)

=Size= explicits the /batch shape/, which is the shape of the tensor of idependent random variables produced by the op. In this example the random variables are identically distributed, but this need not be the case.

** Vectorizing

#+begin_quote
*tldr;* Random Variables are `Blockwise` operators
#+end_quote

Say we want a sample from three normal distributions with a $0$, $3$ and $5$ mean value respectively. One (cumbersome) way to achieve this is:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream()
rv_0 = srng.normal(0, 1)
rv_3 = srng.normal(3, 1)
rv_5 = srng.normal(5, 1)
rv = at.stack([rv_0, rv_3, rv_5])

sample = rv.eval()
print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [-0.41520246  1.92093324  6.74827434]
: sample shape: (3,)

To simplify this common operation we can pass arrays as parameters to the =RandomVariable=, and the =Op= will use NumPy broadcasting rules to return an array of independent random variables:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
rv = srng.normal(mean, 1)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"batch shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [1.44369095 2.10405402 5.73595567]
: sample shape: (3,)

 In this case the /batch shape/ is also  =(3,)=; it is the shape of the tensor that contains random variables that are independently distributed and whose distribution belong to the same family.

 We can also use arrays for the standard deviation in this case. Standard broadcasting rules apply to determine the batch shape. For instance, the following fails with a shape mismatch error:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2])
rv = srng.normal(mean, sigma)

try:
    rv.eval()
except ValueError as err:
    print(err)
#+end_src

#+RESULTS:
#+begin_example
shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (3,) and arg 1 with shape (2,).
Apply node that caused the error: normal_rv{0, (0, 0), floatX, True}(RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7F8DCB881A80>), TensorConstant{[]}, TensorConstant{11}, TensorConstant{[0 3 5]}, TensorConstant{[1 2]})
Toposort index: 0
Inputs types: [RandomGeneratorType, TensorType(int64, (0,)), TensorType(int64, ()), TensorType(int64, (3,)), TensorType(int64, (2,))]
Inputs shapes: ['No shapes', (0,), (), (3,), (2,)]
Inputs strides: ['No strides', (0,), (), (8,), (8,)]
Inputs values: [Generator(PCG64) at 0x7F8DCB881A80, array([], dtype=int64), array(11), array([0, 3, 5]), array([1, 2])]
Outputs clients: [['output'], ['output']]

HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
#+end_example

Indeed =mean= and =sigma= cannot be broadcast together:

#+begin_src python
import numpy as np

mean = np.array([0, 3, 5])
sigma = np.array([1, 2])
try:
    np.broadcast(mean, sigma)  # error
except ValueError as err:
    print(err)
#+end_src

#+RESULTS:
: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (3,) and arg 1 with shape (2,).

=np.broadcast(mean, sigma)= gives us the batch shape:

#+begin_src python
import numpy as np

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 7])
print(np.broadcast(mean, sigma).shape)
#+end_src

#+RESULTS:
: (3,)

Indeed:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 3])
rv = srng.normal(mean, sigma)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"batch shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [1.44369095 1.20810805 7.20786701]
: batch shape: (3,)

#+begin_quote
The functions that draw samples from $E$ could be implemented as generalized universal functions using the operator =Blockwise=.

#+end_quote


** Vectorizing + Batching

It is possible to vectorize and batch at the same time. Note that =size= and that vectorized shape must be broadcastable

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 3])
rv = srng.normal(mean, sigma, size=(2, 2, 3))

sample = rv.eval()
print(f"sample values: {sample}")
print(f"batch shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[[1.44369095e+00 1.20810805e+00 7.20786701e+00]
:   [5.87704041e-03 4.70676358e+00 5.48284410e+00]]
:
:  [[8.19314690e-01 4.61131137e+00 5.65270195e+00]
:   [9.70078743e-01 1.52177388e+00 6.78043377e+00]]]
: batch shape: (2, 2, 3)

where =np.broadcast(mean, sigma).shape= must correspond to the last dimensions of =size=. Or in other words, the sample shape is =np.broadcast_shapes(np.broadcast(mean, sigma).shape, size)= if this does not raise an error.

It IS really simple:

=sample_shape = np.broadcast_shapes(np.broadcast(*args), size)=


* All together

Same thing, =size= defines the batch shape, and =aesara= will raise an exception if this is not correctly set.

#+begin_src python
import aesara.tensor as at
import numpy as np

n = np.array([10, 9])
p = np.array([[.8, .1, .1], [.4, .1, .5]])

srng = at.random.RandomStream(0)
rv = srng.multinomial(n, p, size=(3, 2))

sample = rv.eval()
print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
print(f"support shape: {sample.shape[-1:]}")
print(f"batch shape: {sample.shape[:-1]}")
#+end_src

#+RESULTS:
#+begin_example
sample value: [[[6 2 2]
  [4 0 5]]

 [[8 1 1]
  [1 2 6]]

 [[9 0 1]
  [5 0 4]]]
sample shape: (3, 2, 3)
support shape: (3,)
batch shape: (3, 2)
#+end_example

** Broadcasting

When we broadcast the parameters of a non-scalar distribution, two things need to be defined:
1. The support shape
2. The batch shape

In the case of the multinomial distribution, the size of the last dimension of $p$ determines the support shape. Then the formula applies:

=sample_shape = batch_shape + support_shape=

#+begin_src python
import aesara.tensor as at
import numpy as np

n = np.array([10, 9])
p = np.array([[.8, .1, .1], [.4, .1, .5]])

srng = at.random.RandomStream(0)
rv = srng.multinomial(n, p)

sample = rv.eval()
print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
print(f"support shape: {sample.shape[-1:]}")
print(f"batch shape: {sample.shape[0]}")
#+end_src

#+RESULTS:
: sample value: [[6 2 2]
:  [4 0 5]]
: sample shape: (2, 3)
: support shape: (3,)
: batch shape: 2
