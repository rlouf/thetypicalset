#+title: Shape semantics for random variables in Aesara
#+date: <2022-08-17 Wed>
#+PROPERTY: header-args :results output :eval never-export :exports both

**Although this contains useful information, this post is a draft and will probably be integrated to Aesara's documentation**

* A primer on random variables in Aesara

The =RandomVariable= operator in [[id:5a5e87b1-558c-43db-ad38-32a073b10351][Aesara]] corresponds to the following mathematical function:

$$
\operatorname{RandomVariable}: \Omega \times \boldsymbol{\Theta} \to E
$$

Were $\Omega$ is the set of available RNG seeds, $\Theta$ the (potentially multi-dimensional) space in which the parameters of the distribution take their values. $E$ is the state space, also called the support of the corresponding distribution. Given a random seed and parameter values, the operator returns a *realization* of the random variable it represents, and element of $E$.

* The support shape

The dimensionality of the parameter space and the sample space differs depending on the distribution. For instance, the normal distribution takes scalar values for $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$ and returns a scalar $\in \mathbb{R}$:

#+begin_src python :results output
import aesara.tensor as at

srng = at.random.RandomStream(0)

mu = 0
sigma = 1
x_rv = srng.normal(mu, sigma)
sample = x_rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: 1.4436909546981256
: support shape: ()

The Dirichlet distribution is parametrized by a vector $\boldsymbol{\alpha} \in \mathbb{R}^k$ and returns a vector in the k-unit simplex $\operatorname{\Delta}^k$:

#+begin_src python :results output
import aesara.tensor as at

srng = at.random.RandomStream(0)

alpha = [1., 3., 4.]
x_rv = srng.dirichlet(alpha)
sample = x_rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [0.39086221 0.17265609 0.43648169]
: support shape: (3,)

The multinomial is another interesting example; it is parametrized by a probability vector $\boldsymbol{p} \in \Delta^k$, a number of trials $n \in \mathbb{N}$ and returns a vector in $\mathbb{N}^k$:

#+begin_src python :results output
import aesara.tensor as at

srng = at.random.RandomStream(0)

n = 10
p = [.1, .3, .6]
x_rv = srng.multinomial(n, p)
sample = x_rv.eval()

print(f"sample value: {sample}")
print(f"support shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [3 2 5]
: support shape: (3,)

We call the shape of the realizations of the random variables the *support shape*; for the normal distribution it is equal to =()=, the Dirichlet and multinomial distribution it is equal to =(k,)= where =k= is the size of the parameter $\alpha$ and $p$ respectively. Distributions can have arbitrarily complex support shapes: the random variable with a [[https://en.wikipedia.org/wiki/Wishart_distribution][Wishart density]] is a function in $\mathbb{R}^{n \times m}$, and the support shape is =(n,m)=.

* The batch shape

** Batching by broadcasting

Say we want a sample from three independent random variables that are normally distributed with a mean of $0$, $3$ and $5$ respectively. One (cumbersome) way to achieve this is:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream()
rv_0 = srng.normal(0, 1)
rv_3 = srng.normal(3, 1)
rv_5 = srng.normal(5, 1)
rv = at.stack([rv_0, rv_3, rv_5])

sample = rv.eval()
print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [1.65040785 1.76749492 5.86773357]
: sample shape: (3,)

To simplify this common operation, we can pass arrays as parameters to Aesara's =RandomVariable=, and the =Op= will use NumPy broadcasting rules to return an array of independent random variables:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
rv = srng.normal(mean, 1)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [1.44369095 2.10405402 5.73595567]
: sample shape: (3,)

 In this case the /batch shape/ is also  =(3,)=; it is the shape of the tensor that contains random variables that are independently distributed and whose distribution belong to the same family.

 In this case, =srng.normal(mean, 1)= implicitly represents 3 independent random variables; if it helps one can imagine it is a shortcut for the first code block of this section.

 We can also use arrays for the standard deviation in this case. Standard broadcasting rules apply to determine the batch shape. For instance, the following fails with a shape mismatch error:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2])
rv = srng.normal(mean, sigma)

try:
    rv.eval()
except ValueError as err:
    print(err)
#+end_src

#+RESULTS:
#+begin_example
shape mismatch: objects cannot be broadcast to a single shape
Apply node that caused the error: normal_rv{0, (0, 0), floatX, True}(RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FDB97DFD200>), TensorConstant{[]}, TensorConstant{11}, TensorConstant{[0 3 5]}, TensorConstant{[1 2]})
Toposort index: 0
Inputs types: [RandomGeneratorType, TensorType(int64, (0,)), TensorType(int64, ()), TensorType(int64, (3,)), TensorType(int64, (2,))]
Inputs shapes: ['No shapes', (0,), (), (3,), (2,)]
Inputs strides: ['No strides', (8,), (), (8,), (8,)]
Inputs values: [Generator(PCG64) at 0x7FDB97DFD200, array([], dtype=int64), array(11), array([0, 3, 5]), array([1, 2])]
Outputs clients: [['output'], ['output']]

HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
#+end_example

Indeed =mean= and =sigma= cannot be broadcast together:

#+begin_src python
import numpy as np

mean = np.array([0, 3, 5])
sigma = np.array([1, 2])
try:
    np.broadcast(mean, sigma)  # error
except ValueError as err:
    print(err)
#+end_src

#+RESULTS:
: shape mismatch: objects cannot be broadcast to a single shape

=np.broadcast(mean, sigma)= gives us the batch shape:

#+begin_src python
import numpy as np

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 7])
print(np.broadcast(mean, sigma).shape)
#+end_src

#+RESULTS:
: (3,)

Indeed:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 3])
rv = srng.normal(mean, sigma)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"batch shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [1.44369095 1.20810805 7.20786701]
: batch shape: (3,)

Since the =RandomVariable= represents a batch of random variables, we will call the resulting shape the *batch shape*.

The normal distribution is fairly simple since its parameters and realization are 1-dimensional. Let take our dirichlet example:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

alpha = np.array([[1., 2., 4.], [3., 5., 7.]])
rv = srng.dirichlet(alpha)
sample = rv.eval()

print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[0.42615878 0.09794332 0.4758979 ]
:  [0.15408529 0.34781447 0.49810024]]
: sample shape: (2, 3)

Which is equivalent to:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

rv1 = srng.dirichlet([1., 2., 4.])
rv2 = srng.dirichlet([3., 5., 7.])
rv = at.stack([rv1, rv2])
sample = rv.eval()

print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[0.42615878 0.09794332 0.4758979 ]
:  [0.27582652 0.02985376 0.69431972]]
: sample shape: (2, 3)

So we have the simple formula; if =support_shape= and =batch_shape= are tuples, then:

#+begin_quote
sample_shape = batch_shape + support_shape
#+end_quote

** Batching identically distributed random variables

We also frequently need to define iiid random variables. We can define 3 normally-distributed random variables with mean 0 and variance 1 with:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.zeros(3)
rv = srng.normal(mean, 1)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [ 1.44369095 -0.89594598  0.73595567]
: sample shape: (3,)

But there is a shortcut: the =size= parameter of the distribution. In the following code, =size= allows us to define the same 3 random variables as above in a more concise way:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

rv = srng.normal(0, 1, size=3)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [ 1.44369095 -0.89594598  0.73595567]
: sample shape: (3,)

We can of course do the same thing with the dirichlet distribution:

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

rv = srng.dirichlet([1, 3, 5], size=3)

sample = rv.eval()
print(f"sample values: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[0.34934376 0.15431609 0.49634016]
:  [0.16080299 0.37886972 0.4603273 ]
:  [0.21030357 0.42525361 0.36444282]]
: sample shape: (3, 3)

Since we are still talking about independent random variables, =batch= refers indistinctly to identifically distributed or differently distributed random variables.

** Broadcasting and expanding

* Matching

It gets complicated when broadcasting rules apply /and/ we set the =size= parameter.

* Rest

The shape of random tensors in [[id:5a5e87b1-558c-43db-ad38-32a073b10351][Aesara]] is partitioned in semantically different pieces, that refer to the shape of $E$, i.e. the shape of draws we get:
- The *support shape* corresponds to the shape of one element in $E$, the support of the distribution;
- The *batch shape* is the number $N$ of independent random variables $X_i: \Omega \to E$ where $i \in \left\{ 1 \dots N\right\}$; These can be identically or differently distributed.
- =RandomVariable= creates /differently distributed/ random variables by passing different values of parameters to the operators. Broadcasting rules apply, and the *batch shape* (i.e. number of differently distributed random variables) is inferred from these broadcasting rules.
- =RandomVariable= creates /identically distributed/ random variables via the =size= keyword argument. There is a level of indirection here; the shape specified by =size= must broadcast with the shape of the broadcasted arguments. So if the latter is =(a, b)=, to define =c= identically distributed RVs one must set =size= to =(c, a, b)=.
- The shape of the array of random variables is given by =sample_shape = batch_shape + support_shape=

  #+begin_quote
What's confusing about all this is that we are passing a single RNG state to the distribution even when we are looking a realization of several different random variables. Underlying implementation must use different rng states, though, and we should probably talk about this.
  #+end_quote


* Batch shape

Let us only consider distributions over scalar values for now.

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
rv = srng.normal(0, 1)
sample = rv.eval()

print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: 1.4436909546981256
: sample shape: ()

The =batch shape= encompasses one reality: the number of independent random variables we want to define using a probability distribution. There are two ways to arrive at the same batch shape:
- Setting the shape explicitly;
- Broadcasting of parameter arrays.

** Batching

If we want to take independent and indentically distributed samples from a distribution we can use the =size= keyword argument. Here we are requesting $3$ independent and identically distributed random variables:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
rv = srng.normal(0, 1, size=(3,))  # size=3 gives the same result
sample = rv.eval()

print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [ 1.44369095 -0.89594598  0.73595567]
: sample shape: (3,)

We can obviously require that these independent and identically distributed samples be returned in a tensor of any shape:

#+begin_src python
import aesara.tensor as at

srng = at.random.RandomStream(0)
batch_shape = (2, 2, 2)
rv = srng.normal(0, 1, size=batch_shape)
sample = rv.eval()

print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample value: [[[ 1.44369095 -0.89594598]
:   [ 0.73595567  0.00587704]]
:
:  [[ 0.85338179  0.16094803]
:   [ 0.81931469  0.80565568]]]
: sample shape: (2, 2, 2)

=Size= explicits the /batch shape/, which is the shape of the tensor of idependent random variables produced by the op. In this example the random variables are identically distributed, but this need not be the case.

** Vectorizing

#+begin_quote
*tldr;* Random Variables are `Blockwise` operators
#+end_quote



** Vectorizing + Batching

It is possible to vectorize and batch at the same time. Note that =size= and that vectorized shape must be broadcastable

#+begin_src python
import aesara.tensor as at
import numpy as np

srng = at.random.RandomStream(0)

mean = np.array([0, 3, 5])
sigma = np.array([1, 2, 3])
rv = srng.normal(mean, sigma, size=(2, 2, 3))

sample = rv.eval()
print(f"sample values: {sample}")
print(f"batch shape: {sample.shape}")
#+end_src

#+RESULTS:
: sample values: [[[1.44369095e+00 1.20810805e+00 7.20786701e+00]
:   [5.87704041e-03 4.70676358e+00 5.48284410e+00]]
:
:  [[8.19314690e-01 4.61131137e+00 5.65270195e+00]
:   [9.70078743e-01 1.52177388e+00 6.78043377e+00]]]
: batch shape: (2, 2, 3)

where =np.broadcast(mean, sigma).shape= must correspond to the last dimensions of =size=. Or in other words, the sample shape is =np.broadcast_shapes(np.broadcast(mean, sigma).shape, size)= if this does not raise an error.

It IS really simple:

=sample_shape = np.broadcast_shapes(np.broadcast(*args), size)=


* All together

Same thing, =size= defines the batch shape, and =aesara= will raise an exception if this is not correctly set.

#+begin_src python
import aesara.tensor as at
import numpy as np

n = np.array([10, 9])
p = np.array([[.8, .1, .1], [.4, .1, .5]])

srng = at.random.RandomStream(0)
rv = srng.multinomial(n, p, size=(3, 2))

sample = rv.eval()
print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
print(f"support shape: {sample.shape[-1:]}")
print(f"batch shape: {sample.shape[:-1]}")
#+end_src

#+RESULTS:
#+begin_example
sample value: [[[6 2 2]
  [4 0 5]]

 [[8 1 1]
  [1 2 6]]

 [[9 0 1]
  [5 0 4]]]
sample shape: (3, 2, 3)
support shape: (3,)
batch shape: (3, 2)
#+end_example

** Broadcasting

When we broadcast the parameters of a non-scalar distribution, two things need to be defined:
1. The support shape
2. The batch shape

In the case of the multinomial distribution, the size of the last dimension of $p$ determines the support shape. Then the formula applies:

=sample_shape = batch_shape + support_shape=

#+begin_src python
import aesara.tensor as at
import numpy as np

n = np.array([10, 9])
p = np.array([[.8, .1, .1], [.4, .1, .5]])

srng = at.random.RandomStream(0)
rv = srng.multinomial(n, p)

sample = rv.eval()
print(f"sample value: {sample}")
print(f"sample shape: {sample.shape}")
print(f"support shape: {sample.shape[-1:]}")
print(f"batch shape: {sample.shape[0]}")
#+end_src

#+RESULTS:
: sample value: [[6 2 2]
:  [4 0 5]]
: sample shape: (2, 3)
: support shape: (3,)
: batch shape: 2
