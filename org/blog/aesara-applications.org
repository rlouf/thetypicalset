#+TITLE: Interesting applications of Aesara
#+DATE: <2022-12-09 Fri>

Working on [[id:5a5e87b1-558c-43db-ad38-32a073b10351][Aesara]] and its related projects [[id:e18d689a-392a-407a-941a-f0ad2d2dc43e][AePPL]] and [[id:7d019ab6-c3f5-4f63-b689-ece3b88afcc2][AeMCMC]] I have come to realize the versatility and flexibiliy of the ecosystem that we are building. Aesara was built to be /modular/, which means that pretty much everything in the library can be hacked to fit one's particular use case:

- You can create new =Op=\s. These =Op=\s don't need to be functional and can be used just as an intermediate representation. AePPL, for instance, takes an Aesara model with =RandomVariable=\s and transforms it into a graph between measures. These measures are not used to perform computations, but instead to compute the density of the model. And soon by AeMCMC to automatically build samplers from models.
- You can implement new rewrites/optimizations. You found an optimization that makes your model run faster? You can implement it and register it in Aesara's rewrite database easily.
- You can transform parts of your graph into an =Op= that can be target by rewrites.
- You can add a new backend. A new framework came up and you'd like your Aesara models to run with it? You can easily add a new backend, even implement it in a different repository.

When you work on a project like this many ideas of applications pop up, but time is limited. So I am laying everything out here, hoping someone will take one of these ideas and run with it.

* Neural network library

Theano was completely oriented towards neural networks, but Aesara kind of departed from this to focus on the core functionalities of a tensor library. I see a couple reasons why it would be great to have a neural network library in Aesara.

First, deep learning frameworks come and go, and it is not uncommon to have to rewrite one's model several times in different frameworks. Once you have written your model in Aesara, however, you can compile to different backends. Working with the latest compiler is only a matter of adding it as a backend in Aesara, which would benefit the /entire Aesara ecosystem/.

But the biggest benefits come from the ability to give layers a /type/ by creating =Op=\s from their Aesara implementation. Having layer types means you can target them with rewrites, and I see two exciting possibilities coming from this.

** AutoML

*AutoML.* Tired of always making small changes to your models to get an increase in performance? Aesara can automate that! Implement what you manually do as rewrites (increasing the layer size, adding dropout, etc.) and Aesara will output different versions of your model. This means you will only ever have to touch the model implementation once, but also now you can re-use your rewrites for your next models. You are making cumulative progress instead of having to do the same thing over and over. In addition, Aesara's rewrite system will soon include a form of [[https://arxiv.org/abs/1012.1802][equality saturation]], which means that it will be able to return all the possible models that are defined by your rewrite rules. Now that's /true automation/.

** Higher-level optimizations

Compilers used in deep learning are generally good at optimizing the low-level optimizations (constant folding, loop fusions, etc.), but they cannot reason at the mathematical level. There are however [[https://github.com/uwplse/tensat/blob/master/single_rules.txt][plenty of optimizations]] that can be performed by reasoning at the layer level, and an Aesara-based DL library could target layers with rewrites to transform the model into a different but mathematically equivalent representation. This kind of rewrites [[https://github.com/jiazhihao/TASO][has been shown]] to outperform existing compilers by up to a factor of 3.

* Gaussian processes

A Gaussian process library could add kernels as types, and implement rewrites that allow to build models automatically by manipulating kernels. This is essentially what David Duvenaud did in [[https://www.cs.toronto.edu/~duvenaud/thesis.pdf][his PhD thesis]].

* Probabilistic circuits
