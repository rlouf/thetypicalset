:PROPERTIES:
:ID:       d25362a3-6f0c-4b00-98fa-13307b2cf217
:END:
#+title: Linear regression is a data smoother
#+filetags: :public:

- tags :: [[id:45c083ce-32ab-4758-aa1c-32b0496594b1][Machine Learning]]

[[https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch01.pdf][Notes of Cosma Shalizi on regression]]

Let us consider the simpler linear regression $r(x) = \alpha + \beta\, x$. The ordinary least square solution gives

 #+begin_src latex :results raw :exports results
\begin{equation}
  \hat{\beta} =\frac{\sum_{i} y_{i}\,x_{i}}{\sum_{i}x_{i}}
\end{equation}
 #+end_src

 #+RESULTS:
 \begin{equation}
   \hat{\beta} =\frac{\sum_{i} y_{i}\,x_{i}}{\sum_{i}x_{i}}
 \end{equation}

 The estimated regression function is thus:

 #+begin_src latex :results raw :exports results
\begin{equation}
  \hat{r}(x) = \sum_{i} y_{i} \frac{x_{i}}{n\, s_{X}^{2}} x
\end{equation}
 #+end_src

 #+RESULTS:
 \begin{equation}
   \hat{r}(x) = \sum_{i} y_{i} \frac{x_{i}}{n\, s_{X}^{2}} x
 \end{equation}

 where $s_{X}^2$ is the sample variance of X. The prediction is a weighted average of the observed values of the dependent variable
 where the weights are proportional to how far the $x_i$ is from the center, relative to the variance, and proportional to $x$.

 It is a special case of the *linear smoothers* which are estimates of the regression function as:

 #+begin_src latex :results raw :export results
\begin{equation}
  \hat{r}(x) = \sum_{i} y_{i} \hat{\omega}(x_{i, x})
\end{equation}
 #+end_src

 #+RESULTS:
 \begin{equation}
   \hat{r}(x) = \sum_{i} y_{i} \hat{\omega}(x_{i, x})
 \end{equation}
