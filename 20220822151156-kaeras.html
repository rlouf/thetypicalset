<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-01-01 Mon 16:32 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AeNN</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AeNN</h1>
<p>
AeNN would be a deep-learning API that sits on top of Aesara. It is inspired by <a target='_blank' rel='noopener noreferrer' class='external' href="https://keras.io">Keras</a> (which used to be built on top of Theano), Flax (NN library built with JAX), and of course <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/Lasagne/Lasagne">Lasagne</a>. It would make full use of Aesara's graph and rewrite capabilities.
</p>

<div id="outline-container-org6673942" class="outline-2">
<h2 id="org6673942">Inspiration</h2>
<div class="outline-text-2" id="text-org6673942">
<p>
A minimum viable library could be to be able to reproduce <a target='_blank' rel='noopener noreferrer' class='external' href="https://keras.io/">Keras' documentation</a> examples. For instance the MNIST convnet:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #DFAF8F;">model</span> = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=<span style="color: #CC9393;">"relu"</span>),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=<span style="color: #CC9393;">"relu"</span>),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=<span style="color: #CC9393;">"softmax"</span>),
    ]
)
</pre>
</div>

<p>
I find Keras' API to be sometimes too high-level. In particular, I am not a big fan of the activation being a property of layers, and passed as a keyword argument. Activation functions are just functions, and we should allow users to use custom operators. <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/google/flax">Flax</a>'s API is more on point with that respect. I also like the <code>flax.linen.Module</code> interface to define blocks a lot:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> flax.linen <span style="color: #F0DFAF; font-weight: bold;">as</span> nn


<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">CNN</span>(nn.Module):
  <span style="color: #7CB8BB;">@nn.compact</span>
  <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
    <span style="color: #DFAF8F;">x</span> = nn.Conv(features=32, kernel_size=(3, 3))(x)
    <span style="color: #DFAF8F;">x</span> = nn.relu(x)
    <span style="color: #DFAF8F;">x</span> = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    <span style="color: #DFAF8F;">x</span> = nn.Conv(features=64, kernel_size=(3, 3))(x)
    <span style="color: #DFAF8F;">x</span> = nn.relu(x)
    <span style="color: #DFAF8F;">x</span> = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    <span style="color: #DFAF8F;">x</span> = x.reshape((x.shape[0], -1))  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">flatten</span>
    <span style="color: #DFAF8F;">x</span> = nn.Dense(features=256)(x)
    <span style="color: #DFAF8F;">x</span> = nn.relu(x)
    <span style="color: #DFAF8F;">x</span> = nn.Dense(features=10)(x)
    <span style="color: #DFAF8F;">x</span> = nn.log_softmax(x)
    <span style="color: #F0DFAF; font-weight: bold;">return</span> x

<span style="color: #DFAF8F;">model</span> = CNN()
<span style="color: #DFAF8F;">batch</span> = jnp.ones((32, 10))
<span style="color: #DFAF8F;">variables</span> = model.init(jax.random.PRNGKey(0), batch)
<span style="color: #DFAF8F;">output</span> = model.<span style="color: #DCDCCC; font-weight: bold;">apply</span>(variables, batch)
</pre>
</div>
</div>
</div>

<div id="outline-container-org783a428" class="outline-2">
<h2 id="org783a428">Building a simple MLP model</h2>
<div class="outline-text-2" id="text-org783a428">
<p>
Let's take and try to reproduce a simpler example, the MLP example taken from the Flax documentation:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">from</span> typing <span style="color: #F0DFAF; font-weight: bold;">import</span> Sequence
<span style="color: #F0DFAF; font-weight: bold;">import</span> flax.linen <span style="color: #F0DFAF; font-weight: bold;">as</span> nn

<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">MLP</span>(nn.Module):
    features: Sequence[<span style="color: #DCDCCC; font-weight: bold;">int</span>]
    <span style="color: #7CB8BB;">@nn.compact</span>
    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
        <span style="color: #F0DFAF; font-weight: bold;">for</span> feature <span style="color: #F0DFAF; font-weight: bold;">in</span> features[:-1]:
            <span style="color: #DFAF8F;">x</span> = nn.Dense(feature)(x)
            <span style="color: #DFAF8F;">x</span> = nn.relu(x)
        <span style="color: #DFAF8F;">x</span> = nn.Dense(features[-1])(x)
        <span style="color: #F0DFAF; font-weight: bold;">return</span> x
</pre>
</div>
</div>

<div id="outline-container-org4ea3343" class="outline-3">
<h3 id="org4ea3343">Activation Function</h3>
<div class="outline-text-3" id="text-org4ea3343">
<p>
The first approach to implementing the activation function \(\operatorname{Relu}\) is to define it as a function:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at

<span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">relu</span>(x):
    <span style="color: #9FC59F;">"""Rectified linear unit activation function.</span>

<span style="color: #9FC59F;">    .. math::</span>

<span style="color: #9FC59F;">        \operatorname{relu}(x) = \max(0, x)</span>

<span style="color: #9FC59F;">    """</span>
    <span style="color: #F0DFAF; font-weight: bold;">return</span> at.<span style="color: #DCDCCC; font-weight: bold;">max</span>(0, x)
</pre>
</div>

<p>
However, we need the gradient at \(0\) to be:
</p>

<p>
\[
\nabla \operatorname{Relu}(0) = 0
\]
</p>

<p>
So <code>relu</code> will need to be implemented as an <code>OpFromGraph</code>, with a custom gradient. The upside of using <code>OpFromGraph</code> in this case is that the <code>Op</code>\s it builds are directly identifiable in the graph.
</p>
</div>
</div>

<div id="outline-container-orgd614e00" class="outline-3">
<h3 id="orgd614e00">Dense layer</h3>
<div class="outline-text-3" id="text-orgd614e00">
<p>
Layers are best implemented as =OpFromGraph=s for several reasons:
</p>

<ol class="org-ol">
<li>We can target layers with rewrites; This can be useful for AutoML, or optimization at the mathematical level.</li>
<li>We can retrieve the layer's parameters by walking the graph and looking at the inputs of <code>Apply</code> nodes whose <code>Op</code> is of <code>Layer</code> type. We can even add type parameters to indicate which are trainable, regularizable, etc.</li>
<li><code>aesara.dprint</code> shows the graph structure directly.</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">from</span> typing <span style="color: #F0DFAF; font-weight: bold;">import</span> Optional

<span style="color: #F0DFAF; font-weight: bold;">import</span> aesara
<span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at
<span style="color: #F0DFAF; font-weight: bold;">from</span> aesara.tensor.var <span style="color: #F0DFAF; font-weight: bold;">import</span> TensorVariable
<span style="color: #F0DFAF; font-weight: bold;">from</span> aesara.<span style="color: #DCDCCC; font-weight: bold;">compile</span>.builders <span style="color: #F0DFAF; font-weight: bold;">import</span> OpFromGraph

<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">Layer</span>(OpFromGraph):
    <span style="color: #9FC59F;">"""Represents a Layer.</span>

<span style="color: #9FC59F;">    The difference between layers and transformations is that the former</span>
<span style="color: #9FC59F;">    hold (potentially trainable) parameter values.</span>
<span style="color: #9FC59F;">    """</span>


<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">DenseLayer</span>(Layer):
    <span style="color: #9FC59F;">"""`Op` that represents a Dense Layer"""</span>


<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">Dense</span>():

    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__init__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, features: <span style="color: #DCDCCC; font-weight: bold;">int</span>, W: Optional[TensorVariable], b: Optional[TensorVariable]):
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.features = features
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.W = W
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.b = b

    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
        <span style="color: #DFAF8F;">output</span> = at.dot(x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W) + <span style="color: #F0DFAF; font-weight: bold;">self</span>.b
        <span style="color: #DFAF8F;">dense</span> = DenseLayer([x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W, <span style="color: #F0DFAF; font-weight: bold;">self</span>.b], [output])
        <span style="color: #F0DFAF; font-weight: bold;">return</span> dense(x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W, <span style="color: #F0DFAF; font-weight: bold;">self</span>.b)


<span style="color: #DFAF8F;">x</span> = at.matrix(<span style="color: #CC9393;">"X"</span>)
<span style="color: #DFAF8F;">W</span> = at.vector(<span style="color: #CC9393;">"W"</span>)
<span style="color: #DFAF8F;">b</span> = at.scalar(<span style="color: #CC9393;">"b"</span>)

<span style="color: #DFAF8F;">out</span> = Dense(x.shape[1], W, b)(x)

aesara.dprint(out)
<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">DenseLayer{inline=False} [id A]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">|X [id B]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">|W [id C]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">|b [id D]</span>
<span style="color: #5F7F5F;">#</span>
<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">DenseLayer{inline=False} [id A]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">&gt;Elemwise{add,no_inplace} [id E]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">&gt; |dot [id F]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">&gt; | |*0-&lt;TensorType(float64, (None, None))&gt; [id G]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">&gt; | |*1-&lt;TensorType(float64, (None,))&gt; [id H]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">&gt; |InplaceDimShuffle{x} [id I]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">&gt;   |*2-&lt;TensorType(float64, ())&gt; [id J]</span>

<span style="color: #F0DFAF; font-weight: bold;">assert</span> <span style="color: #DCDCCC; font-weight: bold;">isinstance</span>(out.owner.op, Layer)
<span style="color: #F0DFAF; font-weight: bold;">print</span>(out.owner.inputs)
<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">[X, W, b]</span>
</pre>
</div>

<p>
Representing layers as <code>Ops</code> has several advantages:
</p>
<ol class="org-ol">
<li>More readable <code>aesara.dprint</code> outputs;</li>
<li>Parameters can be directly recovered by walking the graphs;</li>
<li>Layers can be targetted by rewrites, which opens possibilities for optimizations and also AutoML (we can replace layers);</li>
<li>We can have layer-specific rules for transpilation. XLA has convolution-specific Ops.</li>
</ol>
</div>
</div>

<div id="outline-container-org647ad71" class="outline-3">
<h3 id="org647ad71">Management of parameters</h3>
<div class="outline-text-3" id="text-org647ad71">
<p>
Neural network libraries typically initialize the parameters for the users, and they provide them with a way to access their values.
</p>
</div>

<div id="outline-container-orgf9245a5" class="outline-4">
<h4 id="orgf9245a5">Lasagne</h4>
<div class="outline-text-4" id="text-orgf9245a5">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at
<span style="color: #F0DFAF; font-weight: bold;">from</span> lasagne.layers <span style="color: #F0DFAF; font-weight: bold;">import</span> InputLayer, DenseLayer, get_all_params, get_output
<span style="color: #F0DFAF; font-weight: bold;">from</span> lasagne.updates <span style="color: #F0DFAF; font-weight: bold;">import</span> nesterov_momentum
<span style="color: #F0DFAF; font-weight: bold;">from</span> lasagme.objectives <span style="color: #F0DFAF; font-weight: bold;">import</span> categorical_crossentropy

<span style="color: #DFAF8F;">x</span> = at.matrix(<span style="color: #CC9393;">'x'</span>)
<span style="color: #DFAF8F;">y</span> = at.matrix(<span style="color: #CC9393;">'y'</span>)

<span style="color: #DFAF8F;">l_in</span> = InputLayer((100, 20), x)
<span style="color: #DFAF8F;">l1</span> = DenseLayer(l_in, num_units=50)
<span style="color: #DFAF8F;">l2</span> = DenseLayer(l1, num_units=30)

<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">compue loss</span>
<span style="color: #DFAF8F;">prediction</span> = get_output(l2)
<span style="color: #DFAF8F;">loss</span> = categorical_crossentropy(prediction, y)

<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">get parameter updates</span>
<span style="color: #DFAF8F;">all_params</span> = get_all_params(l2)
<span style="color: #F0DFAF; font-weight: bold;">assert</span> all_params ==[l1.W, l1.b, l2.W, l2.b]

<span style="color: #DFAF8F;">updates</span> = nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)
</pre>
</div>
</div>
</div>

<div id="outline-container-org1748622" class="outline-4">
<h4 id="org1748622">Flax</h4>
<div class="outline-text-4" id="text-org1748622">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">from</span> typing <span style="color: #F0DFAF; font-weight: bold;">import</span> Sequence

<span style="color: #F0DFAF; font-weight: bold;">import</span> numpy <span style="color: #F0DFAF; font-weight: bold;">as</span> np
<span style="color: #F0DFAF; font-weight: bold;">import</span> jax
<span style="color: #F0DFAF; font-weight: bold;">import</span> jax.numpy <span style="color: #F0DFAF; font-weight: bold;">as</span> jnp
<span style="color: #F0DFAF; font-weight: bold;">import</span> flax.linen <span style="color: #F0DFAF; font-weight: bold;">as</span> nn

<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">MLP</span>(nn.Module):
  features: Sequence[<span style="color: #DCDCCC; font-weight: bold;">int</span>]

  <span style="color: #7CB8BB;">@nn.compact</span>
  <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
    <span style="color: #F0DFAF; font-weight: bold;">for</span> feat <span style="color: #F0DFAF; font-weight: bold;">in</span> <span style="color: #F0DFAF; font-weight: bold;">self</span>.features[:-1]:
      <span style="color: #DFAF8F;">x</span> = nn.relu(nn.Dense(feat)(x))
    <span style="color: #DFAF8F;">x</span> = nn.Dense(<span style="color: #F0DFAF; font-weight: bold;">self</span>.features[-1])(x)
    <span style="color: #F0DFAF; font-weight: bold;">return</span> x

<span style="color: #DFAF8F;">model</span> = MLP([12, 8, 4])
<span style="color: #DFAF8F;">batch</span> = jnp.ones((32, 10))

<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">Get the parameter values</span>
<span style="color: #DFAF8F;">variables</span> = model.init(jax.random.PRNGKey(0), batch)

<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">Get model prediction</span>
<span style="color: #DFAF8F;">output</span> = model.<span style="color: #DCDCCC; font-weight: bold;">apply</span>(variables, batch)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org95c635c" class="outline-3">
<h3 id="org95c635c">Equinox</h3>
<div class="outline-text-3" id="text-org95c635c">
<p>
Equinox uses the <code>model(x)</code>  and <code>equinox.update(model, updates)</code> pattern (where <code>updates</code> are gradient updates) to avoid having to deal with parameters explicitly
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #DFAF8F;">linear</span> = equinox.nn.Linear(in_features, out_features)
</pre>
</div>
</div>
</div>

<div id="outline-container-org56e38f0" class="outline-3">
<h3 id="org56e38f0">AeNN</h3>
<div class="outline-text-3" id="text-org56e38f0">
<p>
A few notes:
</p>
<ul class="org-ul">
<li>Only the number of units need be specified</li>
<li>In some situations (BNN) we'd like to specify the units themselves</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aenn <span style="color: #F0DFAF; font-weight: bold;">as</span> nn

<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">multiple dispatch</span>
<span style="color: #DFAF8F;">x</span> = nn.Dense(50)(x)
<span style="color: #DFAF8F;">x</span> = nn.Dense(W, b)(x)

<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">Lasagne currently does this</span>
<span style="color: #DFAF8F;">x</span> = nn.Dense(50, W, b)(x)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> flax.linen <span style="color: #F0DFAF; font-weight: bold;">as</span> nn


<span style="color: #DFAF8F;">x</span> = nn.Dense(50, bias=<span style="color: #BFEBBF;">False</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">Dense</span>():

    <span style="color: #DFAF8F;">__init__</span> = MethodDispatcher(<span style="color: #CC9393;">'f'</span>)

    <span style="color: #7CB8BB;">@__init__.register</span>(<span style="color: #DCDCCC; font-weight: bold;">int</span>)
    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">_init_units</span>(units: <span style="color: #DCDCCC; font-weight: bold;">int</span>):
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.units = units
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.W = <span style="color: #BFEBBF;">None</span>
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.b = <span style="color: #BFEBBF;">None</span>

    <span style="color: #7CB8BB;">@__init__.register</span>(TensorVariable)
    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">_init_W</span>(W = <span style="color: #BFEBBF;">None</span>, b=<span style="color: #BFEBBF;">None</span>):
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.W = W
        <span style="color: #F0DFAF; font-weight: bold;">self</span>.b = b

    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
        <span style="color: #DFAF8F;">num_inputs</span> = x.shape[0]

        <span style="color: #DFAF8F;">output</span> = at.dot(x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W) + <span style="color: #F0DFAF; font-weight: bold;">self</span>.b
        <span style="color: #DFAF8F;">dense</span> = DenseLayer([x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W, <span style="color: #F0DFAF; font-weight: bold;">self</span>.b], [output])
        <span style="color: #F0DFAF; font-weight: bold;">return</span> dense(x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W, <span style="color: #F0DFAF; font-weight: bold;">self</span>.b)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf0678f4" class="outline-3">
<h3 id="orgf0678f4"><span class="todo TODO">TODO</span> Module</h3>
<div class="outline-text-3" id="text-orgf0678f4">
<p>
How do we define a module in a similar way we defined <code>MLP</code> above with Flax? Is there anything special about modules compared to normal layers? Should we attribute a specific <code>Module</code> type to them, as opposed to <code>Layer</code>? If we consider they're merely a way to define a new layer they should be implemented as <code>OpFromGraph</code> as well. In this case we should have a general way to define layers so the code looks like:
</p>
</div>
</div>

<div id="outline-container-org572a037" class="outline-3">
<h3 id="org572a037"><span class="todo TODO">TODO</span> Bayesian Neural Network</h3>
<div class="outline-text-3" id="text-org572a037">
<p>
PPLs typically require you to define <i>probabilistic layers</i>, but this approach is not general as you cannot define an arbitrary prior structure on the weights. With Aesara/AeNN/AePPL it is fairly simple; you just initialize the weights as random variables:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #DFAF8F;">srng</span> = at.random.RandomStream(0)

<span style="color: #DFAF8F;">num_units</span> = at.scalar(<span style="color: #CC9393;">"N"</span>)
<span style="color: #DFAF8F;">X</span> = at.matrix(<span style="color: #CC9393;">"X"</span>)
<span style="color: #DFAF8F;">W</span> = srng.normal(0, 1, size=(X.shape[1], num_units))
<span style="color: #DFAF8F;">b</span> = at.scalar(<span style="color: #CC9393;">"b"</span>)

<span style="color: #DFAF8F;">out</span> = Dense(x.shape[1], W, b)(x)

aesara.dprint(out)
<span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">DenseLayer{inline=False} [id A]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">|X [id B]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">|normal_rv{0, (0, 0), floatX, False}.1 [id C]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| |RandomGeneratorSharedVariable(&lt;Generator(PCG64) at 0x7FE904AED660&gt;) [id D]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| |SpecifyShape [id E]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |Elemwise{Cast{int64}} [id F]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | | |MakeVector{dtype='float64'} [id G]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |   |Elemwise{Cast{float64}} [id H]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |   | |Subtensor{int64} [id I]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |   |   |Shape [id J]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |   |   | |X [id K]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |   |   |ScalarConstant{1} [id L]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |   |N [id M]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| | |TensorConstant{2} [id N]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| |TensorConstant{11} [id O]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| |TensorConstant{0} [id P]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">| |TensorConstant{1} [id Q]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">|b [id R]</span>
<span style="color: #5F7F5F;">#  </span><span style="color: #7F9F7F;">|RandomGeneratorSharedVariable(&lt;Generator(PCG64) at 0x7FE904AED660&gt;) [id D]</span>
</pre>
</div>

<p>
One interesting thing to note is that since the operations performed inside layers are deterministic, AePPL won't need to see "inside" the layer Ops to compute the models' log-density.
</p>

<p>
<i>Q: What if I want to initialize some weights randomly but not consider them as random variables?</i>
A: Don't pass them to AePPL's <code>joint_logprob</code>!
</p>
</div>
</div>

<div id="outline-container-orgb6de4bd" class="outline-3">
<h3 id="orgb6de4bd">Training</h3>
<div class="outline-text-3" id="text-orgb6de4bd">
<p>
We need to be able to call the model with <code>model(batch, parameters)</code>, then compute the loss, then update the parameter values using an optimizer.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">MLP</span>(nn.Module):
    features: Sequence[<span style="color: #DCDCCC; font-weight: bold;">int</span>]

    <span style="color: #7CB8BB;">@nn.module.from_graph</span>
    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
        <span style="color: #9FC59F;">"""This returns a layer as an `Op` named `MLP`.</span>
<span style="color: #9FC59F;">        """</span>
        <span style="color: #F0DFAF; font-weight: bold;">for</span> feature <span style="color: #F0DFAF; font-weight: bold;">in</span> features[:-1]:
            <span style="color: #DFAF8F;">x</span> = nn.Dense(feature)(x)
            <span style="color: #DFAF8F;">x</span> = nn.relu(x)
        <span style="color: #DFAF8F;">x</span> = nn.Dense(features[-1])(x)
        <span style="color: #F0DFAF; font-weight: bold;">return</span> x
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orge06714e" class="outline-2">
<h2 id="orge06714e">Graph rewriting</h2>
<div class="outline-text-2" id="text-orge06714e">
<p>
We can perform rewriting at the layer semantic level, for two reasons:
</p>
<ol class="org-ol">
<li><b><b>AutoML:</b></b> We can swap activation functions, layer sizes, etc.</li>
<li><b><b>Performance:</b></b> For instance, the <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/uwplse/tensat">TENSAT</a> library adds equality saturation to the <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/jiazhihao/TASO">TASO</a> library. There are a list of rewrites that operate on a so-called <i>layer algebra</i>. Here are a few examples:</li>
</ol>

<div class="org-src-container">
<pre class="src src-ascii">matmul(matmul(input_1,input_4),input_5)==matmul(input_1,matmul(input_4,input_5))
conv2d(1,1,0,2,ewadd(input_12,ewadd(input_10,input_11)),ewadd(input_12,ewadd(input_10,input_11)))==conv2d(1,1,0,2,ewadd(input_11,ewadd(input_10,input_12)),ewadd(input_11,ewadd(input_10,input_12)))
poolavg(3,3,1,1,0,input_8)==conv2d(1,1,0,0,input_8,Cpool(3,3))
relu(input_8)==conv2d(1,1,0,2,input_8,Iconv(3,3))
</pre>
</div>
</div>
</div>

<div id="outline-container-orgac0645f" class="outline-2">
<h2 id="orgac0645f">Links to this note</h2>
<div class="outline-text-2" id="text-orgac0645f">
<ul class="org-ul">
<li><a href="./20220823090532-ops.html">Ops</a></li>
<li><a href="./20220729163627-aesara.html">Aesara</a></li>
<li><a href="./20220826141720-automatic_model_selection_with_aesara_rewrites.html">Automatic model selection with Aesara rewrites</a></li>
</ul>
</div>
</div>
</div>
</body>
</html>
