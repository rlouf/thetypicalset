<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-20 Tue 10:30 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AeNN</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AeNN</h1>
<p>
AeNN would be a deep-learning API that sits on top of Aesara. It is inspired by <a target='_blank' rel='noopener noreferrer' class='external' href="https://keras.io">Keras</a> (which used to be built on top of Theano), Flax (NN library built with JAX), and of course <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/Lasagne/Lasagne">Lasagne</a>. It would make full use of Aesara's graph and rewrite capabilities.
</p>

<div id="outline-container-org59d3c57" class="outline-2">
<h2 id="org59d3c57">Inspiration</h2>
<div class="outline-text-2" id="text-org59d3c57">
<p>
A minimum viable library could be to be able to reproduce <a target='_blank' rel='noopener noreferrer' class='external' href="https://keras.io/">Keras' documentation</a> examples. For instance the MNIST convnet:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #DFAF8F;">model</span> = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=<span style="color: #CC9393;">"relu"</span>),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=<span style="color: #CC9393;">"relu"</span>),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=<span style="color: #CC9393;">"softmax"</span>),
    ]
)
</pre>
</div>

<p>
I find Keras' API to be sometimes too high-level. In particular, I am not a big fan of the activation being a property of layers, and passed as a keyword argument. Activation functions are just functions, and we should allow users to use custom operators. <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/google/flax">Flax</a>'s API is more on point with that respect. I also like the <code>flax.linen.Module</code> interface to define blocks a lot:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> flax.linen <span style="color: #F0DFAF; font-weight: bold;">as</span> nn


<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">CNN</span>(nn.Module):
  <span style="color: #7CB8BB;">@nn.compact</span>
  <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
    <span style="color: #DFAF8F;">x</span> = nn.Conv(features=32, kernel_size=(3, 3))(x)
    <span style="color: #DFAF8F;">x</span> = nn.relu(x)
    <span style="color: #DFAF8F;">x</span> = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    <span style="color: #DFAF8F;">x</span> = nn.Conv(features=64, kernel_size=(3, 3))(x)
    <span style="color: #DFAF8F;">x</span> = nn.relu(x)
    <span style="color: #DFAF8F;">x</span> = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    <span style="color: #DFAF8F;">x</span> = x.reshape((x.shape[0], -1))  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">flatten</span>
    <span style="color: #DFAF8F;">x</span> = nn.Dense(features=256)(x)
    <span style="color: #DFAF8F;">x</span> = nn.relu(x)
    <span style="color: #DFAF8F;">x</span> = nn.Dense(features=10)(x)
    <span style="color: #DFAF8F;">x</span> = nn.log_softmax(x)
    <span style="color: #F0DFAF; font-weight: bold;">return</span> x

<span style="color: #DFAF8F;">model</span> = CNN()
<span style="color: #DFAF8F;">batch</span> = jnp.ones((32, 10))
<span style="color: #DFAF8F;">variables</span> = model.init(jax.random.PRNGKey(0), batch)
<span style="color: #DFAF8F;">output</span> = model.<span style="color: #DCDCCC; font-weight: bold;">apply</span>(variables, batch)
</pre>
</div>
</div>
</div>

<div id="outline-container-org50b7f05" class="outline-2">
<h2 id="org50b7f05">Building a simple MLP model</h2>
<div class="outline-text-2" id="text-org50b7f05">
<p>
Let's take and try to reproduce a simpler example, the MLP example taken from the Flax documentation:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">from</span> typing <span style="color: #F0DFAF; font-weight: bold;">import</span> Sequence
<span style="color: #F0DFAF; font-weight: bold;">import</span> flax.linen <span style="color: #F0DFAF; font-weight: bold;">as</span> nn

<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">MLP</span>(nn.Module):
    features: Sequence[<span style="color: #DCDCCC; font-weight: bold;">int</span>]
    <span style="color: #7CB8BB;">@nn.compact</span>
    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):
        <span style="color: #F0DFAF; font-weight: bold;">for</span> feature <span style="color: #F0DFAF; font-weight: bold;">in</span> features[:-1]:
            <span style="color: #DFAF8F;">x</span> = nn.Dense(feature)(x)
            <span style="color: #DFAF8F;">x</span> = nn.relu(x)
        <span style="color: #DFAF8F;">x</span> = nn.Dense(features[-1])(x)
        <span style="color: #F0DFAF; font-weight: bold;">return</span> x
</pre>
</div>
</div>

<div id="outline-container-orgb1597a3" class="outline-3">
<h3 id="orgb1597a3">Activation Function</h3>
<div class="outline-text-3" id="text-orgb1597a3">
<p>
The first approach to implementing the activation function \(\operatorname{Relu}\) is to define it as a function:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at

<span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">relu</span>(x):
    <span style="color: #9FC59F;">"""Rectified linear unit activation function.</span>

<span style="color: #9FC59F;">    .. math::</span>

<span style="color: #9FC59F;">        \operatorname{relu}(x) = \max(0, x)</span>

<span style="color: #9FC59F;">    """</span>
    <span style="color: #F0DFAF; font-weight: bold;">return</span> at.<span style="color: #DCDCCC; font-weight: bold;">max</span>(0, x)
</pre>
</div>

<p>
However, we need the gradient at \(0\) to be:
</p>

<p>
\[
\nabla \operatorname{Relu}(0) = 0
\]
</p>

<p>
So <code>relu</code> will need to be implemented as an <code>OpFromGraph</code>, with a custom gradient. The upside of using <code>OpFromGraph</code> in this case is that the <code>Op</code>\s it builds are directly identifiable in the graph.
</p>
</div>
</div>

<div id="outline-container-org563eba0" class="outline-3">
<h3 id="org563eba0">Dense layer</h3>
<div class="outline-text-3" id="text-org563eba0">
<p>
Layers are best implemented as =OpFromGraph=s for several reasons:
</p>

<ol class="org-ol">
<li>We can target layers with rewrites; This can be useful for AutoML, or optimization at the mathematical level.</li>
<li>We can retrieve the layer's parameters by walking the graph and looking at the inputs of <code>Apply</code> nodes whose <code>Op</code> is of <code>Layer</code> type. We can even add type parameters to indicate which are trainable, regularizable, etc.</li>
<li><code>aesara.dprint</code> shows the graph structure directly.</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">Layer</span>(OpFromGraph):
    <span style="color: #9FC59F;">"""Represents a Layer.</span>

<span style="color: #9FC59F;">    The difference between layers and transformations is that the former</span>
<span style="color: #9FC59F;">    hold (potentially trainable) parameter values.</span>
<span style="color: #9FC59F;">    """</span>


<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">DenseLayer</span>(Layer):
    <span style="color: #9FC59F;">"""`Op` that represents a Dense Layer"""</span>


<span style="color: #F0DFAF; font-weight: bold;">class</span> <span style="color: #7CB8BB;">Dense</span>(Op):
    features: <span style="color: #DCDCCC; font-weight: bold;">int</span>
    b: <span style="color: #DFAF8F;">Optional</span>[TensorVariable] = <span style="color: #BFEBBF;">None</span>
    W: <span style="color: #DFAF8F;">Optional</span>[TensorVariable] = <span style="color: #BFEBBF;">None</span>

    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">__call__</span>(<span style="color: #F0DFAF; font-weight: bold;">self</span>, x):

        <span style="color: #F0DFAF; font-weight: bold;">if</span> <span style="color: #F0DFAF; font-weight: bold;">self</span>.W <span style="color: #F0DFAF; font-weight: bold;">is</span> <span style="color: #BFEBBF;">None</span>:
            <span style="color: #F0DFAF; font-weight: bold;">self</span>.W = at.random.uniform(size=(x.shape[0], features))

        <span style="color: #DFAF8F;">output</span> = at.dot(x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W)
        <span style="color: #F0DFAF; font-weight: bold;">if</span> <span style="color: #F0DFAF; font-weight: bold;">self</span>.b <span style="color: #F0DFAF; font-weight: bold;">is</span> <span style="color: #F0DFAF; font-weight: bold;">not</span> <span style="color: #BFEBBF;">None</span>:
            <span style="color: #DFAF8F;">output</span> = output + <span style="color: #F0DFAF; font-weight: bold;">self</span>.b

        <span style="color: #DFAF8F;">dense</span> = DenseLayer(output, [x, <span style="color: #F0DFAF; font-weight: bold;">self</span>.W, <span style="color: #F0DFAF; font-weight: bold;">self</span>.b])
        <span style="color: #F0DFAF; font-weight: bold;">return</span> dense(parameters, x)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgdce84f3" class="outline-3">
<h3 id="orgdce84f3">Module</h3>
</div>

<div id="outline-container-org74eee1b" class="outline-3">
<h3 id="org74eee1b">Training</h3>
<div class="outline-text-3" id="text-org74eee1b">
<p>
We need to be able to call the model with <code>model(batch, parameters)</code>, then compute the loss, then update the parameter values using an optimizer.
</p>
</div>
</div>
</div>

<div id="outline-container-org83e6967" class="outline-2">
<h2 id="org83e6967">Graph rewriting</h2>
<div class="outline-text-2" id="text-org83e6967">
<p>
We can perform rewriting at the layer semantic level. For instance, the <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/uwplse/tensat">TENSAT</a> library adds equality saturation to the <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/jiazhihao/TASO">TASO</a> library. There are a list of rewrites that operate on a so-called <i>layer algebra</i>. Here are a few examples:
</p>

<div class="org-src-container">
<pre class="src src-ascii">matmul(matmul(input_1,input_4),input_5)==matmul(input_1,matmul(input_4,input_5))
conv2d(1,1,0,2,ewadd(input_12,ewadd(input_10,input_11)),ewadd(input_12,ewadd(input_10,input_11)))==conv2d(1,1,0,2,ewadd(input_11,ewadd(input_10,input_12)),ewadd(input_11,ewadd(input_10,input_12)))
poolavg(3,3,1,1,0,input_8)==conv2d(1,1,0,0,input_8,Cpool(3,3))
relu(input_8)==conv2d(1,1,0,2,input_8,Iconv(3,3))
</pre>
</div>
</div>
</div>

<div id="outline-container-org384a64b" class="outline-2">
<h2 id="org384a64b">Links to this note</h2>
<div class="outline-text-2" id="text-org384a64b">
<ul class="org-ul">
<li><a href="./20220823090532-ops.html">Ops</a></li>
<li><a href="./20220729163627-aesara.html">Aesara</a></li>
<li><a href="./20220826141720-automatic_model_selection_with_aesara_rewrites.html">Automatic model selection with Aesara rewrites</a></li>
</ul>
</div>
</div>
</div>
</body>
</html>