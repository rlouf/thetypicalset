<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-01 Thu 14:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Prior choice</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Prior choice</h1>


<div id="outline-container-org3a99c44" class="outline-2">
<h2 id="org3a99c44">Negative Binomial</h2>
<div class="outline-text-2" id="text-org3a99c44">
<p>
Generic priors often fail dramatically when used for the dispersion parameter of the <a href="20210420200526-negative_binomial_distribution.html#ID-273bfd3a-7e6e-4971-b422-048f930ae5b0">Negative Binomial Distribution</a>.
</p>

<p>
In PyMC3 the Negative binomial distribution can be parametrized in terms of the mean \(\mu\) and the dispersion parameter \(\alpha\) so that
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #d73a49;">\begin</span>{<span style="color: #6f42c1;">equation</span>}
  <span style="color: #d73a49;">\sigma</span>^{2} = <span style="color: #d73a49;">\mu</span> + <span style="color: #d73a49;">\alpha</span> <span style="color: #d73a49;">\mu</span>^{2}
<span style="color: #d73a49;">\end</span>{<span style="color: #6f42c1;">equation</span>}
</pre>
</div>

<p>
We typically want \(\alpha\) to be small-ish, which can be problematic when \(\mu\) is large. A typically prior choice would be to define \(\gamma = 1 / \alpha\) and
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #d73a49;">\begin</span>{<span style="color: #6f42c1;">equation</span>}
  <span style="color: #d73a49;">\gamma</span> <span style="color: #d73a49;">\sim</span> <span style="color: #d73a49;">\HalfCauchy</span>(10e-2)
<span style="color: #d73a49;">\end</span>{<span style="color: #6f42c1;">equation</span>}
</pre>
</div>

<p>
So \(\gamma\) can take arbitrarily large values. We can see it by rewriting the Negative binomial as:
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #d73a49;">\begin</span>{<span style="color: #6f42c1;">align</span>}
  X | z &amp;<span style="color: #d73a49;">\sim</span> <span style="color: #d73a49;">\operatorname</span>{Poisson}(<span style="color: #d73a49;">\mu</span> <span style="color: #d73a49;">\,</span> z)<span style="color: #24292e;">\\</span>
  z &amp;<span style="color: #d73a49;">\sim</span> <span style="color: #d73a49;">\operatorname</span>{Gamma}(<span style="color: #d73a49;">\gamma</span>^{-1}, <span style="color: #d73a49;">\gamma</span>^{-1})
<span style="color: #d73a49;">\end</span>{<span style="color: #6f42c1;">align</span>}
</pre>
</div>

<p>
and the standard deviation of \(z\) is \(\sqrt{\gamma}\). This Gamma distribution has a mean of \(1\) and a variance of \(\gamma\). cite:simpson2018 suggests that the prior should be put on \(\sqrt{\gamma}\), which can be seen as a deviation \(d(\gamma)\) so that if \(d(\gamma\))$ is increased by one unit the square root of the information lost by replacing this model by the base model (Poisson, which occurs when \(\Gamma\) =0) is increases by one. See cite:simpson2015 and cite:simpson2017 for more information.
</p>
</div>

<div id="outline-container-org3667b31" class="outline-3">
<h3 id="org3667b31">Numerically</h3>
<div class="outline-text-3" id="text-org3667b31">
<p>
Are there general numerical transformations on the computation graph that we can learn from this example?
</p>
</div>

<div id="outline-container-org4deac45" class="outline-4">
<h4 id="org4deac45">Model implementation in PyMC3</h4>
<div class="outline-text-4" id="text-org4deac45">
<p>
The original model is written:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #24292e;">alpha</span> = pm.HalfNormal(<span style="color: #032f62;">"alpha"</span>, 10e-2)
<span style="color: #24292e;">Y</span> = pm.NegativeBinomial(<span style="color: #032f62;">"Y"</span>, mu, alpha)
</pre>
</div>

<p>
While the reparametrized model is written:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #24292e;">alpha</span> = pm.HalfNormal(<span style="color: #032f62;">"alpha"</span>, 10e-2)
<span style="color: #24292e;">phi</span> = pm.Gamma(<span style="color: #032f62;">"phi"</span>, tt.sqrt(alpha), tt.sqrt(alpha))
<span style="color: #24292e;">Y</span> = pm.Poisson(<span style="color: #032f62;">"Y"</span>, mu * phi)
</pre>
</div>

<p>
Note that we introduced a new degree of liberty by expanding the Negative Binomial in its canonical form.
</p>
</div>
</div>

<div id="outline-container-org59fd519" class="outline-4">
<h4 id="org59fd519">The logpdf</h4>
<div class="outline-text-4" id="text-org59fd519">
<p>
The logpdf of the negative binomial of parameters \(\mu\) and \(\alpha\) is implemented as
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #d73a49;">def</span> <span style="color: #6f42c1;">logpdf_negativebinomial</span>(x, mu, alpha):
    <span style="color: #d73a49;">return</span> (binomln(x + alpha -1, x)
            + logpow(mu, x)
            - logpow(mu+alpha, x)
            + logpow(alpha, alpha)
            - logpow(mu+alpha, alpha)
    )
</pre>
</div>

<p>
While the Poisson is implemented as:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #d73a49;">def</span> <span style="color: #6f42c1;">logpdf_poisson</span>(x, mu, phi):
    <span style="color: #d73a49;">return</span> aet.log(aet.gammainc(X + 1, mu * phi))
</pre>
</div>

<p>
And the Gamma distribution as:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #d73a49;">def</span> <span style="color: #6f42c1;">logpdf_gamma</span>(phi, alpha):
    <span style="color: #d73a49;">return</span> (-gmmaln(alpha)
            + logpow(alpha, alpha)
            - alpha * phi
            + logpow(phi, alpha - 1)
    )
</pre>
</div>

<p>
So the logpdf of the re-parametrized negative binomial is given by:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #d73a49;">def</span> <span style="color: #6f42c1;">logpdf_reparam</span>(x, mu, alpha, phi):
    <span style="color: #d73a49;">return</span> (
        aet.log(aet.gammainc(X + 1, mu * phi))
        -gmmaln(alpha)
        + logpow(alpha, alpha)
        - alpha * phi
        + logpow(phi, alpha - 1)
    )
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orge6a5e8e" class="outline-2">
<h2 id="orge6a5e8e">Links to this note</h2>
</div>
</div>
</body>
</html>