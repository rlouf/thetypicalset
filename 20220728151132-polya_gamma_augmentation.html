<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-09-05 Mon 18:22 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Polya-Gamma augmentation</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Polya-Gamma augmentation</h1>
<div class="abstract" id="orgf37a328">
<p>
In this note we will go over the Polya-Gamma augmentation for logistic regression models.
</p>

</div>

<p>
Let us consider the binary target \(\boldsymbol{y} = \left[ y_1, \dots, y_N\right]\) and the dependant variables \(X = \left[\boldsymbol{x}_1, \dots, \boldsymbol{x}_N\right]\) such that
</p>

\begin{equation*}
P\left(\boldsymbol{y} | X, \boldsymbol{\beta} \right) = \prod_{i=1}^N P\left(y_i | \boldsymbol{x}_i, \boldsymbol{\beta} \right)
\end{equation*}

<p>
We further assume that
</p>

\begin{align*}
y_i | \boldsymbol{x_i}, \boldsymbol{\beta} &\sim \operatorname{Bernoulli}\left(p_i\right)\\
p_i & = \sigma(\boldsymbol{x_i}^T\,\boldsymbol{\beta})\\
\sigma(x) &= \left(1 + \exp(-x)\right)^{-1}
\end{align*}

<p>
It can be shown that
</p>

\begin{equation*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) = \frac{\exp\left(y_i\: \boldsymbol{x}_i^T \boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_i^T \boldsymbol{\beta}\right)}
\end{equation*}

<p>
while assuming a normal prior on each coefficient \(\beta_j\)
</p>

\begin{equation*}
\beta_j \sim \operatorname{N}\left(\mu, \sigma^2\right)
\end{equation*}

<p>
So the posterior distribution:
</p>

<p>
\[
P(\beta|y, X) \propto P(Y| X, \beta) P(\beta)
\]
</p>

<p>
does not have a closed-form solution.
</p>

<p>
But behold! Polson et al. remind us of this neat result:
</p>


\begin{align*}
\frac{\left(e^u\right)^a}{\left(1 + e^u\right)^b} &= \frac{1}{2^b}\, e^{\kappa u}\,\int_0^\infty e^{-\frac{u^2}{2} \omega}\; p(\omega)\, \mathrm{d}\omega\\
\kappa &= a - \frac{b}{2}\\
p(\omega) &= \mathrm{PG}\left(\omega|b, 0\right)
\end{align*}


<p>
So if we set \(b=1\), \(u = \boldsymbol{x}_i^T \boldsymbol{\beta}\) and \(a = y_i\) the following holds:
</p>

\begin{align*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) &= \frac{1}{2} \int_0^\infty \exp\left( y_i \boldsymbol{x_i}^T\,\boldsymbol{\beta} - \frac{\left(\boldsymbol{x_i}^T\,\boldsymbol{\beta}\right)^2}{2} \omega\right)\;P(\omega)\; \mathrm{d}\omega\\
&= \frac{1}{2} \int_0^\infty \exp\left( -\frac{\omega}{2} \left( \frac{y_i}{\omega} - \boldsymbol{x_i}^T\,\boldsymbol{\beta}\right)^2\right)\;P(\omega)\; \mathrm{d}\omega\\
\end{align*}


<p>
So if we condition on \(\omega_n\) we find that \(P(y_i | x_i, \beta, \omega_n)\), and hence \(P(\beta|Y, X, \omega_n)\) to be gaaussian (if \(\omega_n\) is draws from \(\operatorname{PG}(1, 0)\))
</p>

<div id="outline-container-org3b2147f" class="outline-2">
<h2 id="org3b2147f"><span class="todo TODO">TODO</span> Simple example with a bimodal distribution</h2>
<div class="outline-text-2" id="text-org3b2147f">
<p>
And \(Y_i\) indicates which mode the sample belongs to. We can use the <code>polyagamma</code> library.
</p>
</div>
</div>

<div id="outline-container-org3991f76" class="outline-2">
<h2 id="org3991f76">Appendix</h2>
<div class="outline-text-2" id="text-org3991f76">
</div>
<div id="outline-container-orgc066f6c" class="outline-3">
<h3 id="orgc066f6c">Likelihood</h3>
<div class="outline-text-3" id="text-orgc066f6c">
<p>
Let us derive the likelihood \(p(\boldsymbol{y} | x, \boldsymbol{\beta} )\) for the <a href="20220729104422-bernoulli_distribution.html#ID-82cc8d0e-682d-4082-90ac-36cf7fadcb72">Bernoulli</a> and <a href="20210420200526-negative_binomial_distribution.html#ID-273bfd3a-7e6e-4971-b422-048f930ae5b0">Negative Binomial</a> observation distribution. Assuming that the observations are i.i.d. from the same distribution we can write:
</p>

\begin{equation*}
P\left(\boldsymbol{y} | x, \boldsymbol{\beta}\right) = \prod_{i=1}^N P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right)
\end{equation*}
</div>

<div id="outline-container-orgea32db3" class="outline-4">
<h4 id="orgea32db3">Bernoulli observation distribution</h4>
<div class="outline-text-4" id="text-orgea32db3">
<p>
In this case:
</p>

\begin{align*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) &= p_i^{\,1-y_i}\,\left(1 - p_i\right)^{y_i}\\
p_i &= \frac{\exp(f_i)}{1 + \exp(f_i)}\\
f_i &= - x_i^T\,\boldsymbol{\beta}
\end{align*}


\begin{align*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) &= p_i^{\,1-y_i}\,\left(1 - p_i\right)^{y_i}\\
&= \left[ \frac{\exp(f_i)}{1 + \exp(f_i)}\right]^{\,1-y_i}\,\left[1 - \frac{\exp(f_i)}{1 + \exp(f_i)}\right]^{y_i}\\
&= \left[ \frac{\exp(f_i)}{1 + \exp(f_i)}\right]^{\,1-y_i}\,\left[\frac{1}{1 + \exp(f_i)}\right]^{y_i}\\
&= \frac{\left( \exp(f_i) \right)^{\,1-y_i}}{1 + \exp(f_i)}\\
&= \frac{\left( \exp(-f_i) \right)^{\,y_i}}{1 + \exp(-f_i)}\\
\end{align*}


<p>
So that:
</p>
\begin{equation*}
\mathcal{L}(\boldsymbol{\beta}) = \prod_{i=1}^N P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) = \prod_{i=1}^N \frac{\left(\exp\left(\boldsymbol{x}_i^T \boldsymbol{\beta}\right)\right)^{y_i}}{1 + \exp\left(\boldsymbol{x}_i^T \boldsymbol{\beta}\right)}
\end{equation*}

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> aesara.tensor <span style="font-weight: bold;">as</span> at

<span style="font-weight: bold; font-style: italic;">srng</span> = at.random.RandomStream(0)
<span style="font-weight: bold; font-style: italic;">X</span> = at.matrix(<span style="font-style: italic;">"X"</span>)
<span style="font-weight: bold; font-style: italic;">beta</span> = at.vector(<span style="font-style: italic;">"beta"</span>)

<span style="font-weight: bold; font-style: italic;">p</span> = at.sigmoid(at.dot(X.T, beta))
<span style="font-weight: bold; font-style: italic;">Y_rv</span> = srng.bernoulli(p, size=X.shape[0])
<span style="font-weight: bold; font-style: italic;">y_vv</span> = Y_rv.clone()

<span style="font-weight: bold; font-style: italic;">loglikelihood</span> = y.logprob(y_vv)
</pre>
</div>

<p>
Is equivalent to:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">X</span> = at.matrix(<span style="font-style: italic;">"X"</span>)
<span style="font-weight: bold; font-style: italic;">beta</span> = at.vector(<span style="font-style: italic;">"beta"</span>)
<span style="font-weight: bold; font-style: italic;">f</span> = at.dot(X.T, beta)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">loglikelihood = at.prod(at.pow(at.exp(f), y) / (1 + at.dot(exp(f)))...)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org2468d5d" class="outline-4">
<h4 id="org2468d5d">Negative binomial observation distribution</h4>
<div class="outline-text-4" id="text-org2468d5d">
<p>
If we note \(r\) the <a href="20210420200526-negative_binomial_distribution.html#ID-273bfd3a-7e6e-4971-b422-048f930ae5b0">negative binomial</a>'s dispersion parameter, and \(y_i\) the number of observations in experiment \(i\), then:
</p>

\begin{align*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) &= p_i^{\,r}\,\left(1 - p_i\right)^{y_i}\\
p_i &= \frac{\exp(f_i)}{1 + \exp(f_i)}\\
f_i &= - x_i^T\,\boldsymbol{\beta}
\end{align*}

<p>
The previous calculation trivially applies to this as well:
</p>

\begin{align*}
P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) &\propto p_i^{\,r}\,\left(1 - p_i\right)^{y_i}\\
&= \left[ \frac{\exp(f_i)}{1 + \exp(f_i)}\right]^{\,r}\,\left[1 - \frac{\exp(f_i)}{1 + \exp(f_i)}\right]^{y_i}\\
&= \left[ \frac{\exp(f_i)}{1 + \exp(f_i)}\right]^{\,r}\,\left[\frac{1}{1 + \exp(f_i)}\right]^{y_i}\\
&= \frac{\left( \exp(f_i) \right)^{\,r}}{\left(1 + \exp(f_i)\right)^{y_i+r}}\\
\end{align*}

<p>
So that
</p>

\begin{equation*}
\mathcal{L}_i\left(\boldsymbol{\beta}\right) = \prod_{i=1}^N P\left(y_i | \boldsymbol{x_i}, \boldsymbol{\beta}\right) = \prod_{i=1}^N \frac{\left(\exp\left( - \boldsymbol{x}_i^T \boldsymbol{\beta}\right)\right)^{r}}{\left(1 + \exp\left(-\boldsymbol{x}_i^T \boldsymbol{\beta}\right)\right)^{y_i+r}}
\end{equation*}
</div>
</div>
</div>
</div>


<div id="outline-container-org30e2c0f" class="outline-2">
<h2 id="org30e2c0f">References</h2>
<div class="outline-text-2" id="text-org30e2c0f">
<ul class="org-ul">
<li>Dunson, etc. Bayesian inference for logistic models using Polya-Gamma latent variables. (2013)</li>
<li>Slides: <a target='_blank' rel='noopener noreferrer' class='external' href="http://www.gatsby.ucl.ac.uk/tea/tea_archive/attached_files/polya-gamma-slides.pdf">http://www.gatsby.ucl.ac.uk/tea/tea_archive/attached_files/polya-gamma-slides.pdf</a></li>
<li><a target='_blank' rel='noopener noreferrer' class='external' href="https://tiao.io/post/polya-gamma-bayesian-logistic-regression/#i">https://tiao.io/post/polya-gamma-bayesian-logistic-regression/#i</a></li>
<li><a target='_blank' rel='noopener noreferrer' class='external' href="https://gregorygundersen.com/blog/2019/09/20/polya-gamma/">https://gregorygundersen.com/blog/2019/09/20/polya-gamma/</a></li>
<li><a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aemcmc">Implementation</a> in <code>AeMCMC</code></li>
</ul>
</div>
</div>

<div id="outline-container-org36cb2b4" class="outline-2">
<h2 id="org36cb2b4">Links to this note</h2>
<div class="outline-text-2" id="text-org36cb2b4">
<ul class="org-ul">
<li><a href="./20220823093551-rewrites_implement_facts.html">Rewrites implement facts</a></li>
<li><a href="./20220823094247-reference_other_people_s_notes.html">Reference other people's ideas</a></li>
<li><a href="./inbox.html">Writing inbox</a></li>
<li><a href="./20220728171643-gibbs_sampling.html">Gibbs sampling</a></li>
</ul>
</div>
</div>
</div>
</body>
</html>
