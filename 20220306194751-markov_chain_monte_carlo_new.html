<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-01 Thu 10:29 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Markov Chain Monte Carlo</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Markov Chain Monte Carlo</h1>
<p>
We are interested in new sampling algorithms, and more interestingly at methods to calibrate the samplers.
</p>

<div id="outline-container-org6d26899" class="outline-2">
<h2 id="org6d26899">The motivation behind MCMC algorithms</h2>
<div class="outline-text-2" id="text-org6d26899">
<p>
Let us briefly explain where it comes from and the motivations behind it. Let us consider a response variable \(\bb{Y}\), feature matrix \(X\) and a model with parameter \(\boldsymbol{\beta}\) so the likelihood of the data given the model and parameter values is given by
</p>

<p>
\[
P\left(Y | X, \beta\right)
\]
</p>

<p>
Given a test point \(x_*\) we are usually interested in the distribution over predictions \(Y_*\) (a random variable):
</p>

<p>
\[
P\left(Y_* | Y, X, x_*\right)
\]
</p>

<p>
Let us a assume we have a function (assumed to be deterministic here) that returns a sample \(y_*\) of \(Y_*\)'s distribution given a value of the set of parameters \(\tilde{\beta}\):
</p>

<p>
\[
y_* = f(\tilde{\beta}, x_*)
\]
</p>

<p>
Then we get this by marginalizing:
</p>

<p>
\[
P\left(Y_* | Y, X, x_*\right) = \int P(f\left(x_*, \beta\right)| \beta, x_*) P(\beta|Y, X) \mathrm{d} \beta
\]
</p>

<p>
where \(P(\beta|Y, X)\) is the <i>posterior distribution</i> of the model's parameters. This is the integral that we would like to evaluate for all practical purposes. Given \(\left\{\tilde{\beta}_1, \dots, \tilde{\beta}_N \right\}\) \(N\) samples from the posterior distribution.
</p>

<p>
(Adrien) - Should look at <a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/abs/1801.09065">Ensemble MCMC</a>.
</p>

<p>
(Junpeng) - <a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/pdf/2110.00610.pdf">Delayed rejection sampling</a>, <a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/pdf/2111.02434.pdf">Hamiltonian dynamics with non-newtonian momentum for rapid sampling</a>
</p>
</div>
</div>

<div id="outline-container-org75fa766" class="outline-2">
<h2 id="org75fa766">References</h2>
<div class="outline-text-2" id="text-org75fa766">
<ul class="org-ul">
<li><a target='_blank' rel='noopener noreferrer' class='external' href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo integration (Wikipedia)</a></li>
<li>Andrieu et al <a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/abs/2012.14881">A general perspective on the Metropolis-Hastings kernel</a> (2020)</li>
</ul>
</div>
</div>

<div id="outline-container-orgc9c5e39" class="outline-2">
<h2 id="orgc9c5e39">Links to this note</h2>
<div class="outline-text-2" id="text-orgc9c5e39">
<ul class="org-ul">
<li><a href="./20211203143110-approximate_bayesian_computation.html">Simulation-based inference</a></li>
<li><a href="./20211227220104-waste_free_sequential_monte_carlo.html">Waste-free Sequential Monte Carlo</a></li>
<li><a href="./20220307090701-no_u_turn_sampler.html">No-U-Turn Sampler</a></li>
<li><a href="./20220426183233-pathfinder.html">Pathfinder</a></li>
<li><a href="./20211202104804-what_i_am_working_on.html">ðŸ’» Working on now</a></li>
<li><a href="./20220112174537-orbital_mcmc.html">Orbital MCMC</a></li>
<li><a href="./20210604105221-perpetual_hmc.html">Perpetual HMC</a></li>
<li><a href="./20211202110014-reversible_jump_mcmc.html">Reversible-jump MCMC</a></li>
<li><a href="./20220307090810-hamiltonian_monte_carlo.html">Hamiltonian Monte Carlo</a></li>
<li><a href="./20220728171643-gibbs_sampling.html">Gibbs sampling</a></li>
<li><a href="./20211202111457-involutive_mcmc_a_unifying_framework.html">Involutive MCMC</a></li>
</ul>
</div>
</div>
</div>
</body>
</html>