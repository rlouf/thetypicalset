<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-01-20 Fri 17:40 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Aesara's Intermediate representation</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Aesara's Intermediate representation</h1>

<div id="outline-container-org22f1bcd" class="outline-2">
<h2 id="org22f1bcd">Requirements</h2>
<div class="outline-text-2" id="text-org22f1bcd">
</div>
<div id="outline-container-orgb3347e1" class="outline-3">
<h3 id="orgb3347e1">Graph introspection</h3>
<div class="outline-text-3" id="text-orgb3347e1">
</div>
<div id="outline-container-org6a88a1e" class="outline-4">
<h4 id="org6a88a1e">At different levels of abstraction</h4>
</div>
</div>
<div id="outline-container-orge16b805" class="outline-3">
<h3 id="orge16b805">Graph rewrites</h3>
<div class="outline-text-3" id="text-orge16b805">
</div>
<div id="outline-container-org95180ce" class="outline-4">
<h4 id="org95180ce">At different levels of abstraction</h4>
</div>
</div>
<div id="outline-container-orgaeec00d" class="outline-3">
<h3 id="orgaeec00d">Automatic differentiation through control flow</h3>
<div class="outline-text-3" id="text-orgaeec00d">
<p>
Which means breaking from the quasi-static assumption.
</p>
</div>
</div>
<div id="outline-container-orge20bf81" class="outline-3">
<h3 id="orge20bf81">Transpilation to different backends</h3>
</div>
</div>
<div id="outline-container-org885b3af" class="outline-2">
<h2 id="org885b3af">Elements of representation</h2>
<div class="outline-text-2" id="text-org885b3af">
</div>
<div id="outline-container-orgbac772b" class="outline-3">
<h3 id="orgbac772b"><code>Scalar</code></h3>
</div>
<div id="outline-container-org588cc88" class="outline-3">
<h3 id="org588cc88"><code>TensorVariable</code></h3>
</div>
<div id="outline-container-org7451c58" class="outline-3">
<h3 id="org7451c58"><code>Op</code>\s</h3>
</div>
<div id="outline-container-orgca0dbfb" class="outline-3">
<h3 id="orgca0dbfb"><code>OpFromGraph</code>\s</h3>
</div>
<div id="outline-container-orgac0db9d" class="outline-3">
<h3 id="orgac0db9d"><code>FunctionGraph</code></h3>
</div>
<div id="outline-container-org3b4611f" class="outline-3">
<h3 id="org3b4611f">Loops</h3>
<div class="outline-text-3" id="text-org3b4611f">
<p>
The difficulty of differentiating through loops and the necessity to model them in a better IR is motivated by?
</p>
</div>
</div>

<div id="outline-container-org3e0c7f1" class="outline-3">
<h3 id="org3e0c7f1">Conditionals</h3>
<div class="outline-text-3" id="text-org3e0c7f1">
<p>
A motivation for implementation a new control flow IR in Aesara comes from the desire to differentiate through expressions like:
</p>

\begin{equation*}
    f(x) =
        \begin{cases}
            0 & \text{if}\; x \geq 0\\
            x^2 & \text{otherwise}\\
        \end{cases}
\end{equation*}

<p>
JAX can already take the gradient of control flow:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> jax

<span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">f</span>(x):
    <span style="color: #F0DFAF; font-weight: bold;">if</span> x &gt;= 0:
        <span style="color: #F0DFAF; font-weight: bold;">return</span> 0.
    <span style="color: #F0DFAF; font-weight: bold;">else</span>:
        <span style="color: #F0DFAF; font-weight: bold;">return</span> x**2

<span style="color: #F0DFAF; font-weight: bold;">print</span>(jax.grad(f)(-10.))
</pre>
</div>

<p>
And so can Aesara:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at
<span style="color: #F0DFAF; font-weight: bold;">from</span> aesara.ifelse <span style="color: #F0DFAF; font-weight: bold;">import</span> ifelse

<span style="color: #DFAF8F;">x</span> = at.scalar(<span style="color: #CC9393;">'x'</span>)
<span style="color: #DFAF8F;">y</span> = ifelse(at.gt(x,0), at.as_tensor(0., dtype=<span style="color: #CC9393;">'float64'</span>), x**2)
<span style="color: #DFAF8F;">y_grad</span> = at.grad(y, wrt=x)

<span style="color: #F0DFAF; font-weight: bold;">print</span>(y_grad.<span style="color: #DCDCCC; font-weight: bold;">eval</span>({x: -10.}))
</pre>
</div>

<p>
What is the fundamental different between <code>ifelse</code> and a <code>phi</code> node?
</p>
</div>
</div>
</div>


<div id="outline-container-org048a1b8" class="outline-2">
<h2 id="org048a1b8">The pitfalls</h2>
<div class="outline-text-2" id="text-org048a1b8">
<p>
We need to balance relational programming and differentiation. Note that the papers below complain about "black-box operators", and point at JAX's `while<sub>loop</sub>`. Aesara operators are different though, as they allow us to translate concepts people are used to to a flexible IR. We could achieve the same thing that Julia does by parsing the AST, but providing `switch`, `cond`, `while`, `fori` etc operators is as efficient, if not more in python (parsing the AST is possible but tedious)
</p>

<ul class="org-ul">
<li>`Scan` requires a fixed number of steps. It can be a variable, but `while` loops should not require such input;</li>
<li>We need to be able to do relational programming + use e-graphs through control flow;</li>
<li>We need to be able to differentiate through stuff like ODE solvers.</li>
</ul>
</div>
</div>

<div id="outline-container-orge60ea09" class="outline-2">
<h2 id="orge60ea09">References</h2>
<div class="outline-text-2" id="text-orge60ea09">
<ul class="org-ul">
<li><p>
"Don't unroll adjoint: differentiating SSA-form programs" (<a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/abs/1810.07951">paper</a>)
</p>

<p>
Use a SSA representation to be able to differentiate through control flow.
</p></li>

<li><p>
"Equality saturation: A new Approach to optimization" (<a target='_blank' rel='noopener noreferrer' class='external' href="https://web.archive.org/web/20110614052534id_/http://cseweb.ucsd.edu/~lerner/papers/popl09.pdf">paper</a>)
</p>

<p>
Introduces PEG, a SSA intermediate representation that allows to do equality saturation on graphs with control flow. Introduces \(\theta\) and \(\Phi\) nodes as well as a few other, less fundamental, primitives to represent loops.
</p></li>

<li><p>
"Useful algorithms that are not optimized by JAX or PyTorch" (<a target='_blank' rel='noopener noreferrer' class='external' href="http://www.stochasticlifestyle.com/useful-algorithms-that-are-not-optimized-by-jax-pytorch-or-tensorflow/">blog post</a>)
</p>

<blockquote>
<p>
If you pull up your standard methods like convolutional neural networks, that's a fixed function kernel call with a good derivative defined, or a recurrent neural network, that's a fixed size for loop. If you want to break this assumption, you have to go to a space that is fundamentally about an algorithm where you cannot know "the amount of computation" until you know the specific values in the problem, and equation solvers are something of this form.
</p>

<p>
How many steps does it take for Newton's method to converge? How many steps does an adaptive ODE solver take? This is not questions that can be answered a priori: they are fundamentally questions which require knowing:
</p>

<p>
What equation are we solving?
What is the initial condition?
Over what time span?
With what solver tolerance?
</p>
</blockquote>

<p>
Machine learning framework in Python make quasi-static assumptions about the program, the fact that the size of the loops is fixed. <b>But</b> there are non quasi-static problems that are really useful for Machine Learning, like the neural ODE solver in the paper listed below.
</p></li>
</ul>



<ul class="org-ul">
<li>"Opening the Blackbox: Accelerating Neural Differential Equations By Regularizing Internal Solver Heuristics" (<a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/abs/2105.03918">ArXiV</a>)</li>
<li><p>
"Machine Learning Systems are stuck in a rut" (<a target='_blank' rel='noopener noreferrer' class='external' href="https://dl.acm.org/doi/abs/10.1145/3317550.3321441">paper</a>)
</p>

<p>
The problem is that current frameworks "work".
</p>

<blockquote>
<p>
On the other hand, there is little incentive to build high quality back ends that support other features, because all the front ends currently work in terms of monolithic
</p>
</blockquote></li>
</ul>
</div>
</div>

<div id="outline-container-orgdd0ee95" class="outline-2">
<h2 id="orgdd0ee95">Links to this note</h2>
</div>
</div>
</body>
</html>