<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-01 Thu 14:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Stochastic gradient Langevin Dynamics (SgLD)</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Stochastic gradient Langevin Dynamics (SgLD)</h1>

<div id="outline-container-org1bf625b" class="outline-2">
<h2 id="org1bf625b">Principle and motivation</h2>
<div class="outline-text-2" id="text-org1bf625b">
<p>
We use the same solver for the overdamped Langevin dynamics as for the <a href="20220329092624-metropolis_adjusted_langevin_algorithm.html#ID-2b7665c9-e950-4432-b260-9e30b593b375">Metropolis-Adjusted Langevin Algorithm</a>, but we work with an <i>estimator</i> \(\hat{\nabla} \log \pi\) for the gradient of the log-probability density function instead of the gradient itself. There are two reasons why one would want to do that:
</p>

<ul class="org-ul">
<li>When the amount of data is huge the full gradient can be very expensive to compute;</li>
<li>When there is a risk to be stuck in a region of the parameter space that is not representative of the typical set; the algorithm has annealing properties.</li>
</ul>

<p>
The exposition follows [Welling &amp; Teh]. At time \(t\) a subset \(X_t = \left\{x_{1t}, \dots, x_{nt}\right\}\) of \(n\) data items is sampled, a step size \(\epsilon_t\) is chosen. The gradient is approximated as:
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #d73a49;">\begin</span>{<span style="color: #6f42c1;">equation*</span>}
  <span style="color: #d73a49;">\hat</span>{<span style="color: #d73a49;">\nabla</span>} <span style="color: #d73a49;">\log</span> <span style="color: #d73a49;">\pi</span>(<span style="color: #d73a49;">\theta</span>_{t}) = <span style="color: #d73a49;">\nabla</span> <span style="color: #d73a49;">\log</span> p(<span style="color: #d73a49;">\theta</span>_{t}) + <span style="color: #d73a49;">\frac</span>{N}{n} <span style="color: #d73a49;">\sum</span>_{i} <span style="color: #d73a49;">\nabla</span> <span style="color: #d73a49;">\log</span> p(x_{it}|<span style="color: #d73a49;">\theta</span>_{t})
<span style="color: #d73a49;">\end</span>{<span style="color: #6f42c1;">equation*</span>}
</pre>
</div>
<p>
So the proposed update using the Langevin dynamics is:
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #d73a49;">\begin</span>{<span style="color: #6f42c1;">align*</span>}
  <span style="color: #d73a49;">\theta</span>_{t+1} &amp;= <span style="color: #d73a49;">\theta</span>_{t} + <span style="color: #d73a49;">\frac</span>{<span style="color: #d73a49;">\epsilon</span>_{t}}{2}<span style="color: #d73a49;">\;\left</span>(<span style="color: #d73a49;">\nabla</span> <span style="color: #d73a49;">\log</span> p(<span style="color: #d73a49;">\theta</span>_{t}) + <span style="color: #d73a49;">\frac</span>{N}{n} <span style="color: #d73a49;">\sum</span>_{i} <span style="color: #d73a49;">\nabla</span> <span style="color: #d73a49;">\log</span> p(x_{it}|<span style="color: #d73a49;">\theta</span>_{t})<span style="color: #d73a49;">\right</span>) + <span style="color: #d73a49;">\xi</span>_{t}<span style="color: #24292e;">\\</span>
  <span style="color: #d73a49;">\xi</span>_t &amp;<span style="color: #d73a49;">\sim</span> <span style="color: #d73a49;">\operatorname</span>{Normal}<span style="color: #d73a49;">\left</span>(0, <span style="color: #d73a49;">\epsilon</span>_t<span style="color: #d73a49;">\right</span>)
<span style="color: #d73a49;">\end</span>{<span style="color: #6f42c1;">align*</span>}
</pre>
</div>


<p>
<i>We do not use RMH acceptance/rejection step</i> (which would require evaluation over the whole dataset).
</p>
</div>
</div>

<div id="outline-container-org1dc8bfb" class="outline-2">
<h2 id="org1dc8bfb">Convergence</h2>
</div>

<div id="outline-container-org3f8a05e" class="outline-2">
<h2 id="org3f8a05e">References</h2>
<div class="outline-text-2" id="text-org3f8a05e">
<ul class="org-ul">
<li>Welling &amp; Teh (2011) <a target='_blank' rel='noopener noreferrer' class='external' href="https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf">Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></li>
</ul>
</div>
</div>

<div id="outline-container-org00c6c10" class="outline-2">
<h2 id="org00c6c10">Links to this note</h2>
</div>
</div>
</body>
</html>