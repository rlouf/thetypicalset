<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-01-19 Thu 17:07 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Write a generative models, aesara and aeppl will take care of the rest</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Write a generative models, aesara and aeppl will take care of the rest</h1>
<p>
I am currently developping yet another probabilistic programming library in
Python, based on source code generation. This is not a new idea, and it is
basically how Stan works: models are written in a DSL and interpreted by the C++
library. Most other libraries rely on a non-standard intepretation of the
language and while they manage to bend python well-enough to build complete
probabilistic programming libraries, there is a sense of complication that
probably should not exist:
</p>

<ul class="org-ul">
<li>A probabilistic language is akin to a regular language, with an additional
construct that is the idea of a random variable. This changes the behaviour
of the language at runtime, behaviour which can be encoded in the intepreter
or compiler.</li>
<li>Generative models are parametrizable functions: from data \(\left\{X\right\}\) and
parameters \(\left(\alpha_i\right\)\) they can generate values.</li>
<li>They are not your regular kind of functions; as in standard Machine Learning,
they can be learned from data.</li>
</ul>

<p>
One of the ideas of `mcx` (pronounce "mix") is to modify the python syntax only
the slightlest to be able to express a wide range of probabilistic models. A
`mcx` program should <b>feel</b> like regular python, with and additional construct.
</p>

<p>
But to get things straights, here are a couple of questions:
</p>

<ul class="org-ul">
<li>Are generative models distributions?</li>
<li>If not, what are they?</li>
</ul>

<p>
Let us take the simplified example of the normal distribution.
</p>

<p>
```python
class Normal(Distribution):
  parameters = {'mu': constraints.real, 'sigma': constraints.positive}
  domain = constraints.real
</p>

<p>
def _<sub>init</sub>_<sub>(self, mu, sigma)</sub>:
  self.mu = mu
  self.sigma = sigma
</p>

<p>
def sample(self, num<sub>samples</sub>: int) -&gt; float:
  return self.mu + random.norm() * self.sigma
</p>

<p>
  @limit<sub>to</sub><sub>domain</sub>
  def logpdf(self, num<sub>samples</sub>: int) -&gt; float:
    return stats.norm.logpdf(x, self.mu, self.sigma)
```
</p>

<p>
Things to note:
</p>

<ul class="org-ul">
<li>A distribution ascribes a measure on the random variable's domain. The measure
is given by `logpdf`.</li>
<li>We can generate values at random that follow this measure using `sample`</li>
<li>We reified the measure to a class that also holds the sampling function and
properties such as the domain of the random variable, and the constraints
on the parameters' values.</li>
</ul>

<p>
So that when I write `x ~ Normal(0, 1)` what I am really writing is "x is a
random variable and the probability of its values are given by a Normal
distribution parametrized by a zero mean and unit variance". So, really, the `~`
symbol means `has` and not `is`. What `x` is, is potentially any number on its
domain. It feels rather strange that we are ascribing the domain the the
probability density function, but hey. The distribution defines a measure that
takes a single variable as an input. That variable that have several dimensions,
e.g. multivariate normal.
</p>

<p>
Now let us compare this with a simple model, a linear regression:
</p>

<p>
```python
def linear<sub>regression</sub>(X):
  weights ~ Normal(0, 1)
  sigma ~ Normal(0, 1)
  y ~ Normal(x * weight, sigma)
  return y
```
</p>

<p>
We can define a generative model as such: `linear<sub>regression</sub>` is a
function that returns a random value of `y` everytime we pass a number `x`. I
now have a question:
</p>

<p>
But if `linear<sub>regression</sub>` is a function, what does it mean to write (and should
it be valid)?
</p>

<p>
```python
w ~ linear<sub>regression</sub>(rng<sub>key</sub>, 10)
```
</p>

<p>
We can intuitively think that sampling values makes straightforward sense:
</p>

<p>
```python
w = linear<sub>regression</sub>(10).sample(rng<sub>key</sub>, num<sub>samples</sub>=1<sub>000</sub>)
```
</p>

<p>
But there is a cognitive conflict between the intepretations `linear<sub>regression</sub>(10)` is the result of a function and `linear<sub>regression</sub>(10)` is a distribution.
</p>

<p>
So, are generative models distributions? There is a sense that they should, but
it is not clear to me.
</p>

<p>
Let us write the `logpdf` function:
</p>

<p>
```python
w = linear<sub>regression.logpdf</sub>(10, weights, sigma, y)
```
</p>

<p>
Defining the random variable through its forward sampling behaviour.
</p>

<p>
Can we define other distributions like this:
</p>

<p>
```python
def Exponential(lmbda):
    U ~ Uniform(0, 1)
    t = - np.log(U) / lmbda
    return t
```
</p>

<p>
The function <b>generates</b> numbers drawn from the exponential distribution. What
is the associated logpdf?
</p>

<p>
```
def Exponential<sub>logpdf</sub>(lmbda, t):
    U = np.exp(- lmbda * t)
    return U.logpdf(np.exp(- lmbda * t))
```
</p>

<p>
Now another slightly more complicated distribution, the multivariate normal:
</p>

<p>
```python
def MvNormal(mu, sigma):
  z ~ Normal(0, 1, size=2)
  a = Cholesky(sigma)
  x = mu + a * z
  return x
```
</p>

<p>
All these use independent draws from the same distribution. Models are a
generalization of this concept where random number can be generated from draws
from different distributions. Note that the logpdfs here are functions of
deterministic variables. This is what we are interested in.
</p>

<p>
```
<del>---</del>      <del>---</del>
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">z</td>
<td class="org-left">&#x2014;&gt;</td>
<td class="org-left">x</td>
</tr>
</tbody>
</table>
<p>
<del>---</del>      <del>---</del>
```
</p>

<p>
The logpdf can be a function of any random variable or function of a random
variable. How do we indicate this is what we care about: we retun it!
</p>

<p>
generative definition -&gt; Distribution (sample, logpdf)
</p>

<p>
A generative model generated as above <b>is not</b> a distribution but it implicitly
defines a multivariate distribution through its generative process. It is a
probability distribution augmented by a `forward`, or `call` function.
</p>

<p>
```python
def linear<sub>regression</sub>(X):
  weights ~ Normal(0, 1)
  sigma ~ Normal(0, 1)
  y ~ Normal(x * weight, sigma)
  return y
```
</p>

<p>
```python
def linear<sub>logpdf</sub>(X, weight, sigma, y):
  logpdf = 0
  logpdf += Normal(0 ,1).logpdf(weight)
  logpdf += Normal(0, 1).logpdf(sigma)
  logpdf += Normal(X * weight, sigma).logpdf(y)
  return logpdf
```
</p>

<p>
So the `LinearRegression` logpdf could be defined as:
</p>

<p>
```python
class LinearRegression(Distribution):
    def _<sub>call</sub>_<sub>(self, X)</sub>:
      weights, sigma, y = self.sample(X)
      return y
</p>

<p>
def sample(self, X):
  weights = Normal(0, 1).sample()
  sigma = Normal(0, 1).sample()
  y = Normal(self.X * weights, sigma).sample()
  return weights, sigma, y
</p>

<p>
    def logpdf(self, X, weight, sigma, y):
      logpdf = 0
      logpdf += Normal(0 ,1).logpdf(weight)
      logpdf += Normal(0, 1).logpdf(sigma)
      logpdf += Normal(X * weight, sigma).logpdf(y)
      return logpdf
```
</p>

<p>
So a slight modification of the distributions can make them equivalent:
</p>

<p>
```python
class Normal(distribution):
  def _<sub>call</sub>_<sub>(self, mu, sigma)</sub>:
      return self.sample(mu, sigma)
</p>

<p>
def sample(self, mu, sigma):
    return mu + random.norm() * sigma
</p>

<p>
  def logpdf(self, x, mu, sigma):
      return stats.logpdf(x, mu, sigma)
```
</p>

<p>
```python
def normal(mu, sigma):
  event<sub>shape</sub> = ()
  batch<sub>shape</sub> = lax.broadcast<sub>shape</sub>(mu, sigma)
  domain = constraints.real
</p>

<p>
def sample(rng<sub>key</sub>, sample<sub>shape</sub>):
  return mu + random.norm(rng<sub>key</sub>, sample<sub>shape</sub>) * sigma
</p>

<p>
  @limit<sub>to</sub>(domain)
  def logpdf(x):
    return stats.norm(x, mu, sigma)
```
</p>

<p>
```python
def linear<sub>model</sub>(X):
    def call(rng<sub>key</sub>, sample<sub>sample</sub>):
</p>

<p>
def sample(rng<sub>key</sub>, sample<sub>shape</sub>):
  w = normal(0, 1)[0](rng<sub>key</sub>, sample<sub>shape</sub>)
  s = normal(0, 1)[0](rng<sub>key</sub>, sample<sub>shape</sub>)
  y = normal(X*w)[0](rng<sub>key</sub>)
  return y
</p>

<p>
    def logpdf(x, w, s, y):
      logpdf = 0
      logpdf += normal(0, 1)[1](w)
      logpdf += normal(0, 1)[1](s)
      logpdf += normal(X*w)[1](y)
      return logpdf
```
</p>

<p>
So when I do
</p>

<p>
```
model = mcx.model(linear<sub>regression</sub>)
model(X) gives a random sample from distribution -&gt; "call" = forward sampling
model.sample(X, &#x2026;) gives an aribitrary number of samples from distribution
model.logpdf(x, w, s, y) gives the logpdf
```
</p>

<p>
## The "return" is really only there to indicate which variables are interesting
## during forward calls.
</p>

<p>
before the model has been learned:
</p>

<p>
```
def forward(X):
    w = normal(0, 1).sample()
    s = normal(0, 1).sample()
    return normal(x*w).sample()
```
</p>


<p>
Once the model has been learned:
</p>

<p>
```
def forward(X, trace):
    w = empirical(trace['x']).sample()
    s = empirical(trace['s']).sample()
    return Normal(x*w).sample() &lt;&#x2013; important to have the "return" !!!
```
</p>

<p>
```python
sampler, logpdf = model(linear<sub>regression</sub>)
```
</p>


<p>
```python
class model():
  def _<sub>init</sub>_<sub>(self, X)</sub>:
    self.X = x
    print(sample(X))
</p>

<p>
def _<sub>call</sub>_<sub>(X)</sub>:
  w, s, y = self.sample<sub>forward</sub>(X)
  return y
</p>

<p>
def sample<sub>forward</sub>():
</p>

<p>
def sample<sub>posterior</sub>():
</p>

<p>
  def logpdf():
```
</p>

<p>
```python
Normal(0, 1)
sample<sub>normal</sub>(0, 1)
```
</p>

<p>
So a generative model can be transformed into a sample generating process as
follows:
</p>

<p>
```python
def linear<sub>model</sub>(X):
    weights = Normal(0, 1)
    sigma = Normal(0, 1)
    y = Normal(weights*x, sigma)
    return y
```
</p>

<p>
Model as distribution?
</p>

<p>
```python
@as<sub>distribution</sub>
def HorseshoePrior(a):
  return Normal(0, 1)
</p>

<p>
def complicated<sub>model</sub>(X):
    a @ HorseshoePrior(0)
    x @ Normal(a)
    return x
```
</p>

<p>
`Model` inherits from distribution. When asking for model logpdf -&gt; instantiates
Model.
</p>

<p>
Ok but how do we define a model?
</p>

<p>
```
model = model(complicated<sub>model</sub>)  # telling it's a model
```
</p>
</div>
</body>
</html>