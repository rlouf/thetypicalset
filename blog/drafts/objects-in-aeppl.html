<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-21 Wed 13:31 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Oh the objects we manipulate</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Oh the objects we manipulate</h1>
<p>
Since some dude has realized that we could implement boolean logic using electrical circuits, the history of computing has consisted in piling up abstractions on top of another. From the electronic the machine code was born, on top of which compilers were born, on top of which modern languages were born. And has the history keeps unfolding, these abstractions are getting us closer to the abstractions that are manipulated by other fields. Doing so, we are getting closer to have computers do something that is useful <i>to us</i>, rather than about us doing something that is useful for computing itself.
</p>

<p>
Sometimes, it works the other way, and it is the outside abstractions that help you understand your code better. I have spent the past few weeks mulling over the <i>meaning</i> of the following code:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aeppl
<span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at

<span style="color: #DFAF8F;">x_rv</span> = at.random.normal(0, 1)
<span style="color: #DFAF8F;">y</span> = at.clip(x_rv, 0, 1)

logprob, (y_vv,) = aeppl.joint_logprob(y)
</pre>
</div>

<p>
There is a little context to unpack here. In Aesara, <code>x_rv</code> is an instance of a <code>RandomVariable</code>, and compiling the following code will generate samples from <code>normal(0, 1)</code>, and <code>at.clip</code> will be applied to the resulting samples:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aesara
<span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at

<span style="color: #DFAF8F;">srng</span> = at.random.RandomStream(0)
<span style="color: #DFAF8F;">x_rv</span> = srng.normal(0, 3)
<span style="color: #DFAF8F;">y</span> = at.clip(x_rv, 0, 1)

<span style="color: #DFAF8F;">sample_fn</span> = aesara.function((), y)
<span style="color: #F0DFAF; font-weight: bold;">print</span>(sample_fn())
<span style="color: #F0DFAF; font-weight: bold;">print</span>(sample_fn())
<span style="color: #F0DFAF; font-weight: bold;">print</span>(sample_fn())
<span style="color: #F0DFAF; font-weight: bold;">print</span>(sample_fn())
</pre>
</div>

<p>
No ambiguity here. The <code>RandomVariable</code> operator is a function that takes a <code>rng</code> state, some parameters and returns a sample:
</p>

<div class="org-src-container">
<pre class="src src-haskell">RandomVariable :: RNG, mu, sigma -&gt; Sample
</pre>
</div>

<p>
\(f \longrightarrow \mathbb{R}\). The <code>at.clip(_, 0, 1)</code> operator represents a function \(\mathbb{R} \longrightarrow \left[0, 1\right]\), and so operates naturally on the output of the random variable. So in this code snippet, there is no ambiguity as long as <code>RandomVariable</code> generates values in \(\mathbb{R}\) or any of its subsets.
</p>

<p>
But the first snippet above tells a different story. Let's take a simpler example, trying to compute the joint log-probability density of the distribution defined as:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aeppl
<span style="color: #F0DFAF; font-weight: bold;">import</span> aesara
<span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at

<span style="color: #DFAF8F;">x_rv</span> = at.random.normal(0, 1)
<span style="color: #DFAF8F;">z_rv</span> = at.random.normal(x_rv, 1)

logprob, (x_vv, y_vv,) = aeppl.joint_logprob(x_rv, y_rv)
<span style="color: #DFAF8F;">fn</span> = aesara.function((x_vv, y_vv), logprob)
<span style="color: #F0DFAF; font-weight: bold;">print</span>(fn(1., 1.))
</pre>
</div>

<p>
Have you noticed? The <i>meaning</i> of <code>x_rv</code> has suddenly changed! It suddenly becomes an object to which a density is attached, and <code>joint_logprob</code> uses this information to compute the joint log-density of the full model.
</p>

<p>
<code>RandomVariable</code>\s are not merely functions in \(\mathbb{R}\) anymore. There are something more: random variables. In probability as measure theory, random variables are merely measurable functions, of measures, here of type \(\mathbb{MR}\). The output of <code>at.random.normal(0, 1)</code> is no longer \(\mathbb{R}\) but \(\mathbb{MR}\). So the first code snippet is indeed confusing:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #DFAF8F;">x_rv</span> = at.random.normal(0, 1)
<span style="color: #DFAF8F;">y</span> = at.clip(x, 0, 1)
</pre>
</div>

<p>
because <code>at.clip</code> is defined for arguments of type \(\mathbb{R}\) and not \(\mathbb{MR}\)! If you code a little bit, you are probably used to the concept of <i>operator overloading</i>, which is to use the same name for operators that operate in different types. We could thus overload <code>at.clip</code> in this context for arguments in \(\mathbb{MR}\). But what is the type of the output in this case?
</p>

<p>
Well for this you need a little bit of measure theory! The measure \(x_{rv}\) can be denoted as, using the expectation notations used by de Finetti:
</p>


<p>
\[
x_{rv} = \lambda f. \int_\mathbb{R} \operatorname{dnormal}(0, 1)(x)\; f(x) dx
\]
</p>

<p>
where \(\operatorname{dnormal(0, 1)}\) is the probability density of the normal distribution. What happens when we take <code>at.clip</code> of this measure? Well we just have to write it down:
</p>

\begin{align*}
y &= \lambda f. \int_\mathbb{R} \operatorname{dnormal}(0, 1)(x)\; f(\min(0, \max(1, x))) dx\\
  &= \lambda f. \int_{-\infty}^0 \operatorname{dnormal}(0, 1)(x) \mathrm{d}x\; f(0)\\
  &+ \lambda f. \int_{0}^1 \operatorname{dnormal}(0, 1)(x)\; f(x) \mathrm{d}x\\
  &+ \lambda f. \int_{1}^\infty \operatorname{dnormal}(0, 1)(x) \mathrm{d}x\; f(1)
\end{align*}

<p>
So \(y\) is a measure, and we can even compute its density!
</p>
</div>
</body>
</html>