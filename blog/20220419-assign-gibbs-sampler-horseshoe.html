<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-20 Tue 21:10 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Unify to build samplers</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Unify to build samplers</h1>
<p>
In the <a href="20220414-identify-horsehoe.html">last post</a> we showed how we can identify a horseshoe prior in an aesara model graph. This exercise may appear very academic at first, but we will show that it has practical applications in this and the following posts. In particular, we will show how unification of models with subgraphs will allow us to automatically build samplers for any model. Here, a Gibbs sampler.
</p>

<p>
Let us consider a simpler version of the previous model that only contains the horseshoe part:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aesara.tensor <span style="color: #F0DFAF; font-weight: bold;">as</span> at

<span style="color: #DFAF8F;">srng</span> = at.random.RandomStream(0)

<span style="color: #DFAF8F;">tau_rv</span> = srng.halfcauchy(1, size=1)
<span style="color: #DFAF8F;">lmbda_rv</span> = srng.halfcauchy(1, size=10)
<span style="color: #DFAF8F;">beta_rv</span> = srng.normal(0, tau_rv * lmbda_rv)
</pre>
</div>

<p>
The following Gibbs sampler updates the values of \(\tau\) and \(\lambda\) knowing the value of \(\beta\):
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">from</span> typing <span style="color: #F0DFAF; font-weight: bold;">import</span> Callable, Tuple

<span style="color: #F0DFAF; font-weight: bold;">from</span> aesara.tensor.random <span style="color: #F0DFAF; font-weight: bold;">import</span> RandomStream
<span style="color: #F0DFAF; font-weight: bold;">from</span> aesara.tensor.var <span style="color: #F0DFAF; font-weight: bold;">import</span> TensorVariable

<span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">step_horseshoe</span>(srng: RandomStream) -&gt; Callable:
    <span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">step</span>(
        lmbda: TensorVariable,
        tau: TensorVariable,
        beta: TensorVariable,
    ) -&gt; Tuple[TensorVariable, TensorVariable]:
        <span style="color: #DFAF8F;">lmbda_inv</span> = 1.0 / lmbda
        <span style="color: #DFAF8F;">tau_inv</span> = 1.0 / tau

        <span style="color: #DFAF8F;">upsilon_inv</span> = srng.exponential(1 + lmbda_inv)
        <span style="color: #DFAF8F;">zeta_inv</span> = srng.exponential(1 + tau_inv)

        <span style="color: #DFAF8F;">beta2</span> = beta * beta
        <span style="color: #DFAF8F;">lmbda_inv_new</span> = srng.exponential(upsilon_inv + 0.5 * beta2 * tau_inv)
        <span style="color: #DFAF8F;">tau_inv_new</span> = srng.gamma(
            0.5 * (beta.shape[0] + 1),
            zeta_inv + 0.5 * (beta2 * lmbda_inv_new).<span style="color: #DCDCCC; font-weight: bold;">sum</span>(),
        )

        <span style="color: #F0DFAF; font-weight: bold;">return</span> 1.0 / lmbda_inv_new, 1.0 / tau_inv_new

    <span style="color: #F0DFAF; font-weight: bold;">return</span> step
</pre>
</div>

<p>
It is possible to provide <code>step_horseshoe</code> (and many other samplers) in a sampler library, and rely on the users to wire models and samplers together. However this has many shortcomings:
</p>

<ol class="org-ol">
<li>It is <i>error-prone</i>: it would be very simple to import a Gibbs sampler that does <b>not</b> sample from this model's posterior distribution and use it anyway;</li>
<li>It is <i>not efficient</i>: users probably don't know that such a Gibbs sampler exist, and it is also possible that they don't care. They may thus miss opportunities to work with more efficient samplers.</li>
<li>It makes the user code dependent on the underlying implementation of algorithms, instead of having a single interface with all samplers that does not change from release to release.</li>
</ol>

<p>
Instead, we would like to provide a unified interface&#x2013;a single function&#x2013;to which users can pass a model, (a set of) observations and that returns a function that generates samples from the model's posterior distribution. Something akin to <a target='_blank' rel='noopener noreferrer' class='external' href="https://twiecki.io/blog/2013/08/12/bayesian-glms-1/">PyMC's "inference button"</a>. In this particular case it is fairly simple, if we call <code>beta_vv</code> the observed value for <code>beta_rv</code>, we can build a sampler from a model with the following function:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">def</span> <span style="color: #93E0E3;">build_sampler</span>(srng, beta_rv, beta_vv, num_samples):
    <span style="color: #DFAF8F;">lmbda_rv</span>, <span style="color: #DFAF8F;">tau_rv</span> = unify_horseshoe(beta_rv)
    <span style="color: #DFAF8F;">step_fn</span> = step_horseshoe(srng)

    <span style="color: #DFAF8F;">outputs</span>, <span style="color: #DFAF8F;">updates</span> = aesara.scan(
        step_fn,
        outputs_info=(lmbda_rv, tau_rv),
        non_sequences=(beta_vv,),
        n_steps=num_samples,
    )

    <span style="color: #F0DFAF; font-weight: bold;">return</span> outputs, updates
</pre>
</div>

<p>
Indeed, we can then compile a sampling function:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> aesara

<span style="color: #DFAF8F;">beta_vv</span> = beta_rv.clone()
<span style="color: #DFAF8F;">num_samples</span> = at.iscalar(<span style="color: #CC9393;">"num_samples"</span>)
<span style="color: #DFAF8F;">outputs</span>, <span style="color: #DFAF8F;">updates</span> = build_sampler(srng, beta_rv, beta_vv, num_samples)

<span style="color: #DFAF8F;">sample_fn</span> = aesara.function((beta_vv, num_samples), outputs, updates=updates)
</pre>
</div>

<p>
And use it to generate samples from the posterior distribution of \(\lambda\) and \(\tau\):
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F0DFAF; font-weight: bold;">import</span> numpy <span style="color: #F0DFAF; font-weight: bold;">as</span> np
<span style="color: #F0DFAF; font-weight: bold;">from</span> IPython.lib.pretty <span style="color: #F0DFAF; font-weight: bold;">import</span> pprint

<span style="color: #DFAF8F;">beta</span> = np.random.normal(size=10)
pprint(sample_fn(beta, 3))
</pre>
</div>

<p>
This example is trivial. We are only considering a very specific model and a single sampler, but you should get a feel of where things are going. In a realistic setting, with a reasonably well developed library, our matching algorithms will succesfully unify several patterns with possibly overlapping subgraphs. In this example, both \(\lambda\) and \(\tau\) are continuous variables, so we can sample from their posterior distribution using e.g. the NUTS sampler. We could also the use random walk Rosenbluth-Metropolis-Hastings as a step function. We thus need a data structure that allows us to efficiently store these possibilities as we walk down the graph and efficiently retrieve them to build a sampler for the whole model. More than enough material to cover for the next post.
</p>
</div>
</body>
</html>