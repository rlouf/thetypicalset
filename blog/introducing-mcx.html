<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-08-23 Tue 22:46 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Probabilistic programming via source code rewriting (MCX)</title>
<meta name="author" content="Rémi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Probabilistic programming via source code rewriting (MCX)</h1>
<p>
The mathematical representation of graphical model is simple. Take the Beta-Binomial model, which is typically used to model the results of \(N\) coin flips in introductory classes:
</p>

\begin{align*}
p &\sim \operatorname{Beta}(1, 2)\\
Y &\sim \operatorname{Binomial}(p)
\end{align*}

<p>
With a little experience you can quickly comprehend what they represent, their consequences. For instance that they implicitly define a <i>predictive distribution</i> \(\operatorname{CoinFlip}(Y | N)\) and a <i>joint distribution</i> \(\operatorname{CoinFip}(Y, p, N)\)<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>. The equations are <b>declarative</b>: they describe a model, not a computation procedure. Which makes them very <b>user-friendly</b> in the sense that their representation of a model matches closely our mental representation. This is <a href="../20220111192254-an_api_is_good_when_you_do_not_think_about_it.html#ID-1eb7d036-0133-496e-97de-ae4b92793b89">what great APIs do</a>.
</p>

<p>
To build a useful Probabilistic Programming Library \(\lor\) Language (PPL) we need to build at least two functions from this object: one that draws samples form the prior joint distribution, and the log-probability density function. But the one-to-any relationship between the notation and the functions we need in practice is not something that is easy to express in most programming languages. Designing a user-friendly API for a Probabilistic Programming Language is thus cursed.
</p>

<p>
Of course we can implement these two representations separately. This is simple to do with "elementary" distributions such as the Beta, Dirichlet, Multivariate Normal, etc. distributions. As a matter of fact, they are often implemented as a single object that implements a <code>sample</code> and a <code>logpdf</code> method. So when you write:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">x</span> = Beta(1., 1.)
</pre>
</div>

<p>
You can draw realizations from this random variable with
</p>

<div class="org-src-container">
<pre class="src src-python">x.sample()
</pre>
</div>

<p>
or compute the log-probability of <code>x</code> having the value <code>a</code> as
</p>

<div class="org-src-container">
<pre class="src src-python">x.logprob(a)
</pre>
</div>

<p>
This implementation is close enough to the mathematical representation \(x \sim \operatorname{Beta}(1., 1.)\). So what is the issue exactly?
</p>

<p>
Representing the model as a class becomes more tedious with complex graphical models. You could implement the Beta-Binomial model above as a class, thus defining the \(\operatorname{BetaBinomial}\) distribution. In fact <a target='_blank' rel='noopener noreferrer' class='external' href="https://docs.pymc.io/en/latest/api/distributions/generated/pymc.BetaBinomial.html">some PPLs do</a> because the Beta-Binomial distribution is a useful abstraction, and <a href="../20220405092607-distributions_are_merely_convenient_abstractions.html#ID-2a325058-fa43-4b50-832b-c02757865643">distributions are merely convenient abstractions</a>. But if you do this for every model you encounter you are effectively building a model zoo; The goal of Probabilistic Programming is to be able combine elementary distributions to express arbitrarily complex models.
</p>

<p>
Existing PPLs have different ways to do this, I recommend Junpeng's <a target='_blank' rel='noopener noreferrer' class='external' href="https://www.youtube.com/watch?v=WHoS1ETYFrw">talk at PyData Córdoba</a> for an overview, but we will take a different approach here. When I design an API, I like to take a sufficiently (but not too) complex example and to <a href="../20220405092736-start_with_designing_the_api_you_want.html#ID-0d452ccf-03aa-47ba-b70e-06b464ed152e">start implementing the API I want</a>. The following linear regression model will do:
</p>

\begin{align*}
  Y &\sim \operatorname{Normal}(\theta, \sigma)\\
  \theta &= \alpha + X^{T} \beta\\
  \sigma &\sim \operatorname{HalfNormal}(1)\\
  \alpha &\sim \operatorname{Normal}(0, 10)\\
  \beta &\sim \operatorname{Normal}(0, 10)\\
\end{align*}

<p>
It has random variables, random variables that are used to define othe random variables, and a mathematical expression. Assuming that the design matrix \(X\) is given, it would make sense to implement the model as a python function of \(X\) that returns the \(Y\) values. <a href="../20220111213141-design_by_manipulating_mock_code.html#ID-fff18475-59cb-445e-b738-069f59918aec">In pseudo code</a>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">regression</span>(X):
  <span style="font-weight: bold; font-style: italic;">alpha</span> = Normal(0, 10)
  <span style="font-weight: bold; font-style: italic;">beta</span> = Normal(np.zeros(5), 10)
  <span style="font-weight: bold; font-style: italic;">sigma</span> = HalfNormal(1)

  <span style="font-weight: bold; font-style: italic;">y</span> = alpha + np.dot(beta, X)
  <span style="font-weight: bold; font-style: italic;">predictions</span> = Normal(y, sigma)

  <span style="font-weight: bold;">return</span> predictions
</pre>
</div>


<p>
Welcome MCX. Well, almost.
</p>

<div id="outline-container-orgd0269b2" class="outline-2">
<h2 id="orgd0269b2">Defining probabilistic models with MCX</h2>
<div class="outline-text-2" id="text-orgd0269b2">
<p>
Here is how you express the beta-binomial model in MCX:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> mcx

<span style="font-weight: bold; text-decoration: underline;">@mcx.model</span>
<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">beta_binomial</span>(beta=2):
  a &lt;~ Beta(1, beta)
  b &lt;~ Binomial(a)
  <span style="font-weight: bold;">return</span> b
</pre>
</div>

<p>
Yes, this is working code. The model is syntactically close to both the mathematical notation and that of a standard python function. Notice the <code>&lt;~</code> operator, which is not standard python notation. It stands for "random variable" assignment in MCX.
</p>

<p>
MCX models are defined as <b>generative functions</b>. Like any function, MCX model can be parametrized by passing arguments to the model. <code>beta_binomial</code> takes <code>beta</code> as an argument, which can later be used to parametrize the model. It can also take data as arguments, such as the values of predictive features in a regression model. The <code>return</code> statement defines the model's output, which is a measured variable.
</p>

<p>
When you write a MCX model such as the one above you implicitely define 2 distributions. First the joint probability distribution of the associated graphical model. You can sample from this distribution with the <code>mcx.sample_joint(beta_binomial)</code> function. You can also the log-probability for a set of values of is random variables with <code>mcx.log_prob(model)</code>.
</p>

<p>
When you call this function, MCX parses the content of your model into an intermediate representation, which is a mix between a graphical model (which is the mathematical object your model describes) and an abstract syntax tree (to maintain control flow). Random variables are identified as well as their distributions and the other variables they depend on.
</p>

<p>
Writing a model as a function is intuitive: most bayesian models are generative models. Given input values and parameters they return other values that can be
observed. While MCX models also represent a distribution, in the API we treat them first as generative functions.
</p>
</div>
</div>

<div id="outline-container-org619af87" class="outline-2">
<h2 id="org619af87">Interacting with MCX models</h2>
<div class="outline-text-2" id="text-org619af87">
<p>
Consider the following linear regression model:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; text-decoration: underline;">@mcx.model</span>
<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">linear_regression</span>(x, lmba=1.):
    scale &lt;~ Exponential(lmbda)
    coef &lt;~ Normal(np.zeros(x.shape[-1]), 1)
    <span style="font-weight: bold; font-style: italic;">y</span> = np.dot(x, coef)
    preds &lt;~ Normal(y, scale)
    <span style="font-weight: bold;">return</span> preds
</pre>
</div>

<p>
Calling the generative function should return a different value each time it is called with a different value of <code>rng_key</code>:
</p>

<div class="org-src-container">
<pre class="src src-python">linear_regression(rng_key, x)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2.3</span>
</pre>
</div>

<p>
Note the apparition of <code>rng_key</code> between the definition and the call here, necessary because of JAX's pseudo-random number generation system. It can be cumbersome to specify a different <code>rng_key</code> at each call so we can handle the splitting automatically using:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">fn</span> = mcx.seed(linear_regression, rng_key)
fn(10)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">34.5</span>
fn(10)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">52.1</span>
</pre>
</div>

<p>
<code>linear_regression</code> is a regular function so we can use JAX's vmap construct to obtain a fixed number of samples from the prior predictive distribution.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> jax

<span style="font-weight: bold; font-style: italic;">keys</span> = jax.random.split(rng_key, num_samples)
jax.vmap(linear_regression, in_axes=(0, <span style="font-weight: bold; text-decoration: underline;">None</span>))(keys, x_data)
</pre>
</div>

<p>
Again, for convenience, we provide a <code>sample_predictive</code> function, which draws samples from the function's predictive distribution.
</p>

<div class="org-src-container">
<pre class="src src-python">mcx.sample_predictive(linear_regression, (x_data,), num_samples=1000)
</pre>
</div>

<p>
The generative function implicitly defines a multivariate distribution over the model's random variables. We include utilities to sample from this distribution. To sample from the prior distribution:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">sampler</span> = mcx.sample_joint(rng_key, linear_regression, (x_data,))
</pre>
</div>

<p>
Since forward sampling can be an efficient way to debug a model, we also introduce a convenient <code>forward</code> method to the model:
</p>

<div class="org-src-container">
<pre class="src src-python">linear_regression.forward(rng_key, x_data)
</pre>
</div>

<p>
If you have seeded the model as shown before (recommended when debugging), then you can call
</p>

<div class="org-src-container">
<pre class="src src-python">linear_regression.forward(x_data)
</pre>
</div>

<p>
To sample from the posterior distribution we need to specify which variables we are conditioning the distribution on (the observed variables) and the kernel we use to sample from the posterior:
</p>


<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">sampler</span> = mcx.sampler(
    rng_key,
    linear_regression,
    (x_data,),
    {<span style="font-style: italic;">"preds"</span>: y_data},
    HMC(100),
)
sampler.run(1000)
</pre>
</div>

<p>
Once the model's posterior distribution has been sampled we can define a new generative function that is the original function evaluated at the samples from the posterior distribution.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">evaluated_model</span> = mcx.evaluate(linear_regression, trace)
</pre>
</div>

<p>
When sampling from the predictive distribution, instead of drawing a value for each variable from its prior distribution, we sample one position of the chains and compute the function's output. Apart from this we can draw samples from the generative distribution like we would the model:
</p>

<div class="org-src-container">
<pre class="src src-python">evaluated_model(rng_key, x_data)
<span style="font-weight: bold; font-style: italic;">seeded</span> = mcx.seed(evaluated_model, rng_key)
mcx.sample_predictive(rng_key, evaluate_model, (x_data,), num_samples=100)
</pre>
</div>

<p>
Unlike the original model, however, the evaluated program is not a distribution. It is a generative function for which only predictive distributions are defined.
</p>
</div>

<div id="outline-container-orge309814" class="outline-3">
<h3 id="orge309814">Go on your own</h3>
<div class="outline-text-3" id="text-orge309814">
<p>
MCX provides a convenient interface to interact with the model's predictive,
joint prior and posterior distributions, but should you want to build something
more sophisticated you can always access the underlying functions directly:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Get the function that computes the logprobability</span>
log_prob(model)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Get a function that draws one sample from the joint distributon</span>
joint_sampler(model)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Function that draws one sample from the predictive distribution</span>
predictive_sampler(model)
</pre>
</div>

<p>
Which should get you covered for most of your applications. <code>log_prob</code>, in
particular, allows to you to write your model with MCX and use another library
(e.g. BlackJAX) to sample from the posterior distribution.
</p>
</div>
</div>
</div>
<div id="outline-container-orge1045b2" class="outline-2">
<h2 id="orge1045b2">How does it work? MCX's internals</h2>
<div class="outline-text-2" id="text-orge1045b2">
</div>
<div id="outline-container-org9d82a91" class="outline-3">
<h3 id="org9d82a91">Representing models with graphs (as in probabilistic <i>graphical</i> model)</h3>
<div class="outline-text-3" id="text-org9d82a91">
<p>
The models' graph can be accessed interactively. It can be changed in place. It is possible to set the value of one node and see how it impacts the others, very useful to debug without re-writing the whole in scipy!
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">new_graph</span> = simplify_conjugacy(graph)
</pre>
</div>

<p>
Having a graph is wonderful: it means that you can symbolically manipulate your
model. You can detect conjugacies and using conjugate distibution to optimize
sampling, reparametrization is trivial to do, etc. Manipulating the graph is
pretty much akin to manipulating the mathematical object.
</p>

<pre class="example" id="org2be8985">

                                     +----&gt; logpdf
@mcx.model                           |
def my_model(X):   -----&gt;   Graph  -------&gt; ....
    .....                            |
    return y                         +----&gt; forward_sampler

</pre>

<p>
All this happens in <b>pure python</b>, there is no framework involved. We do use
NetworkX to build and manipulate graphs for convenience, but could do without.
</p>

<p>
Currently the graph we compile is a static graph. It only contains the random
variables and transformation. As such it can only handle a fixed number of
random variables. This, however, is a strong
</p>

<p>
The advantage of compiling pure python function is that it nicely decouples the
modeling language from inference. Any inference library that accepts python
functions (with jax constructs) could use the functions used by the DSL. So far
the entire code only relies on functions in JAX that are present in numpy/scipy.
So you could very well consider this as a numpy/scipy function. And if you were
introduce JAX-specific constructs such as control flow, you could still specify
a different compiler for each backend since the graph representation is
framework-agnostic. Hell, you could even write, without too much effort, an
edward2, pymc3 or pyro compiler!
</p>

<pre class="example" id="org4182a36">
Example with control flow and different
</pre>

<p>
Is it crazy to do AST manipulation? It might be harder to do it right than in
language with a full-fledged macro system such as, say, Julia or Lisp, but done
correctly it actually gives us nice benefits: a nice API with a powerful
intermediate representation. Corner cases can also be tested as it is possible
to output the code of the logpdfs from the model.
</p>

<pre class="example" id="org96a72ee">
model.source_logpdf
</pre>
</div>
</div>

<div id="outline-container-org36e5c33" class="outline-3">
<h3 id="org36e5c33">Inference</h3>
<div class="outline-text-3" id="text-org36e5c33">
<p>
<i>The modeling language and the inference module are completely separate.</i> The modeling language compiles to a logpdf, and that is all the samplers need.
</p>

<p>
Inference in MCX is also very modular. The idea is that inference in traditional PPLs can be broken down in three different levels:
</p>

<ol class="org-ol">
<li>The building blocks (or <i>routines</i>) of the algorithms: integrators, metrics, proposals, &#x2026; which do only one thing and do it well.</li>
<li><i>Programs</i> like the HMC algorithm are a particular assembly of these building blocks. They form a transition kernel.</li>
<li><i>Runtimes</i>, that tie the data, the model and the kernel together and then make the chains move forward following an execution plan.</li>
</ol>

<pre class="example" id="org3f3e836">
Runtime (Sampling loop)
-------------------------------------------------
Programs (HMC)
-------------------------------------------------
Routines (velocity Verlet, dynamic proposal, etc.)
</pre>

<p>
MCX comes with sane defaults (runtimes and pre-defined programs), but has many
trap doors that allow you to tinker with the lower level.
</p>

<p>
Most users will interact with the pre-defined programs (HMC or NUTS with the
Stan warmup) and runtimes. But it is also possible to create custom inference
schemes in MCX, it can be as simple as overriding HMC's warmup by subclassing
it, or as complex as implementing your own transition kernel using the available
blocks or blocks you have programmed.
</p>

<p>
<b>Update 2022-04-05:</b> Everything related to inference has moved to <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/blackjax-devs/blackjax">Blackjax</a>, a sampling library which is focused on speed and modularity.
</p>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
In fact I am not sure everyone who uses PPLs have formalized this, which makes this notation even more powerful.
</p></div></div>


</div>
</div></div>
</body>
</html>
