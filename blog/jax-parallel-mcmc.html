<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-08-23 Tue 22:42 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Massively parallel MCMC with JAX</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Massively parallel MCMC with JAX</h1>

<div id="outline-container-org35f3792" class="outline-2">
<h2 id="org35f3792">TL;DR</h2>
<div class="outline-text-2" id="text-org35f3792">
<p>
<i>Edit on 2020/10/01:</i> As pointed out by <a target='_blank' rel='noopener noreferrer' class='external' href="https://twitter.com/SingularMattrix">Matthew Johnson</a> and <a target='_blank' rel='noopener noreferrer' class='external' href="https://twitter.com/eigenhector">Hector Yee</a>, the
results reported in a previous version of this post were artificially biaised in
favor of JAX due to my code not "advancing" the random number generator. I
updated all curves and numbers, and added a little word of caution regarding the
use of JAX's pseudo-random number generator.
</p>

<blockquote>
<p>
<b><b>JAX blows everyone out of the water</b></b>, by up to a factor of 20 in extreme
cases (1,000 samples with 1,000,000 chains). Numpy wins in the small number of
samples, small number of chains regime due to JAX's JIT compilation overhead. I
report results for tensorflow probability (TFP), but keep in mind the comparison
is unfair since its implementation of random walk metroplis includes more bells
and whistles than ours.
</p>
</blockquote>


<p>
The code necessary to reproduce the results can be found
<a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/rlouf/blog-benchmark-rwmetropolis">here</a>. Tips to make the
code run faster are appreciated. I currently only care about getting the last
sample of each chain; the number of iterations will eventually be dynamic and I
want to avoid pre-allocating too much memory.
</p>
</div>
</div>

<div id="outline-container-orge899aee" class="outline-2">
<h2 id="orge899aee">Vectorized MCMC</h2>
<div class="outline-text-2" id="text-orge899aee">
<p>
Colin Carroll recently wrote an interesting <a target='_blank' rel='noopener noreferrer' class='external' href="https://colindcarroll.com/2019/08/18/very-parallel-mcmc-sampling/">blog post</a> that
uses Numpy and a vectorized version of the random walk metropolis algorithm
(RWMH) to generate a large number of samples. Running multiple chains at
once is used to performing posterior checks on the convergence of algorithms. It
was traditionally achieved by running one chain per thread on a multithreaded
machine, in Python using joblib or a custom backend. It is cumbersome, but it
does the job.
</p>

<p>
Colin's post got me very excited about the possibility to sample thousands,
millions of chains in parallel with little added cost. He details a couple of
possible applications in his post, but I have the gut feeling that there is
something bigger waiting around the corner. But this is or another post.
</p>

<p>
Around the same time I stumbled upon <a target='_blank' rel='noopener noreferrer' class='external' href="http://github.com/jax/jax">JAX</a>. JAX seems interesting in the context
of Probabilistic Programming Languages for several reasons:
</p>

<ul class="org-ul">
<li>It is in most cases a drop-in replacement for Numpy, and Numpy is known for
its simple, clean interface (in most cases, don't jump on me here);</li>
<li>Autodiff is plain simple (Hello, Hamiltonian Monte Carlo!);</li>
<li>Its forward differentiation mode allows to easily compute higher-order
derivatives easily;</li>
<li>JAX performs JIT compilation using <a target='_blank' rel='noopener noreferrer' class='external' href="https://www.tensorflow.org/xla">XLA</a> which
accelerates your code, even on CPU;</li>
<li>Using GPUs and TPUs is straightforward;</li>
<li>This is a matter of taste, but it favours a functional style of programming.</li>
</ul>

<p>
Before diving right in and implementing a framework with JAX, I wanted to do a
little benchmaking to get an idea of what I am signing up for. Here I will be
comparing:
</p>

<ul class="org-ul">
<li>Numpy</li>
<li>Jax</li>
<li>Tensorflow Probability (TFP)</li>
<li>Tensorflow Probability with XLA compilation</li>
</ul>
</div>
</div>


<div id="outline-container-org4d825fb" class="outline-2">
<h2 id="org4d825fb">Notes about benchmarking</h2>
<div class="outline-text-2" id="text-org4d825fb">
<p>
Before giving the results, a few words of caution:
</p>

<ol class="org-ol">
<li>The reported times are the average of 10 runs on my laptop, with nothing
other than the terminal open. For all but the post-compilation JAX runs,
the times were measured with the <code>hyperfine</code> command line tool.</li>
<li>My code is probably not optimal, especially for TFP. I would
appreciate tips to make the codes faster.</li>
<li>The experiments are performed on CPU. JAX and TFP can run on GPU/TPU so
expect additional acceleration there.</li>
<li>For Numpy and JAX the sampler is a generator and the samples are not kept in
memory. This is not the case for TFP thus the computer runs out of memory
during large experiments.</li>
<li>Number of samples per second is not the metric that matters in probabilistic
programming, but rather the number of effective samples you get per second.
The latter is more of a matter of the algorithm you are using; this
benchmark is still a good indication of the raw performance of the different
frameworks.</li>
</ol>
</div>
</div>

<div id="outline-container-org5c582f8" class="outline-2">
<h2 id="org5c582f8">Setup and results</h2>
<div class="outline-text-2" id="text-org5c582f8">
<p>
I am sampling an arbitrary Gaussian mixture with 4 components. Using Numpy:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np
<span style="font-weight: bold;">from</span> scipy.stats <span style="font-weight: bold;">import</span> norm
<span style="font-weight: bold;">from</span> scipy.special <span style="font-weight: bold;">import</span> logsumexp


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">mixture_logpdf</span>(x):
    <span style="font-weight: bold; font-style: italic;">loc</span> = np.array([[-2, 0, 3.2, 2.5]]).T
    <span style="font-weight: bold; font-style: italic;">scale</span> = np.array([[1.2, 1, 5, 2.8]]).T
    <span style="font-weight: bold; font-style: italic;">weights</span> = np.array([[0.2, 0.3, 0.1, 0.4]]).T

    <span style="font-weight: bold; font-style: italic;">log_probs</span> = norm(loc, scale).logpdf(x)

    <span style="font-weight: bold;">return</span> logsumexp(np.log(weights) + log_probs, axis=0)
</pre>
</div>
</div>

<div id="outline-container-org085ce9f" class="outline-3">
<h3 id="org085ce9f">Numpy</h3>
<div class="outline-text-3" id="text-org085ce9f">
<p>
Colin Carroll's <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/ColCarroll/minimc">MiniMC</a> has the simplest and most readable implementations of
Random Walk Metropolis and Hamiltonian Monte Carlo I have seen. My Numpy
implementation is an iteration upon his:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">rw_metropolis_sampler</span>(logpdf, initial_position):
    <span style="font-weight: bold; font-style: italic;">position</span> = initial_position
    <span style="font-weight: bold; font-style: italic;">log_prob</span> = logpdf(initial_position)
    <span style="font-weight: bold;">yield</span> position

    <span style="font-weight: bold;">while</span> <span style="font-weight: bold; text-decoration: underline;">True</span>:
        <span style="font-weight: bold; font-style: italic;">move_proposals</span> = np.random.normal(0, 0.1, size=initial_position.shape)
        <span style="font-weight: bold; font-style: italic;">proposal</span> = position + move_proposals
        <span style="font-weight: bold; font-style: italic;">proposal_log_prob</span> = logpdf(proposal)

        <span style="font-weight: bold; font-style: italic;">log_uniform</span> = np.log(
            np.random.rand(initial_position.shape[0], initial_position.shape[1])
        )
        <span style="font-weight: bold; font-style: italic;">do_accept</span> = log_uniform &lt; proposal_log_prob - log_prob

        <span style="font-weight: bold; font-style: italic;">position</span> = np.where(do_accept, proposal, position)
        <span style="font-weight: bold; font-style: italic;">log_prob</span> = np.where(do_accept, proposal_log_prob, log_prob)
        <span style="font-weight: bold;">yield</span> position
</pre>
</div>
</div>
</div>

<div id="outline-container-orgda66efa" class="outline-3">
<h3 id="orgda66efa">JAX</h3>
<div class="outline-text-3" id="text-orgda66efa">
<p>
Let us unpack the JAX implementation. The kernel is very similar to Numpy's:
</p>


<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">from</span> functools <span style="font-weight: bold;">import</span> partial

<span style="font-weight: bold;">import</span> jax
<span style="font-weight: bold;">import</span> jax.numpy <span style="font-weight: bold;">as</span> np

<span style="font-weight: bold; text-decoration: underline;">@partial</span>(jax.jit, static_argnums=(1,))
<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">rw_metropolis_kernel</span>(rng_key, logpdf, position, log_prob):
    <span style="font-weight: bold; font-style: italic;">key</span>, <span style="font-weight: bold; font-style: italic;">subkey</span> = jax.random.split(rng_key)
    <span style="font-style: italic;">"""Moves the chain by one step using the Random Walk Metropolis algorithm.</span>

<span style="font-style: italic;">    Attributes</span>
<span style="font-style: italic;">    ----------</span>
<span style="font-style: italic;">    rng_key: jax.random.PRNGKey</span>
<span style="font-style: italic;">      Key for the pseudo random number generator.</span>
<span style="font-style: italic;">    logpdf: function</span>
<span style="font-style: italic;">      Returns the log-probability of the model given a position.</span>
<span style="font-style: italic;">    position: np.ndarray, shape (n_dims,)</span>
<span style="font-style: italic;">      The starting position.</span>
<span style="font-style: italic;">    log_prob: float</span>
<span style="font-style: italic;">      The log probability at the starting position.</span>

<span style="font-style: italic;">    Returns</span>
<span style="font-style: italic;">    -------</span>
<span style="font-style: italic;">    Tuple</span>
<span style="font-style: italic;">        The next positions of the chains along with their log probability.</span>
<span style="font-style: italic;">    """</span>
    <span style="font-weight: bold; font-style: italic;">move_proposals</span> = jax.random.normal(key, shape=position.shape) * 0.1
    <span style="font-weight: bold; font-style: italic;">proposal</span> = position + move_proposals
    <span style="font-weight: bold; font-style: italic;">proposal_log_prob</span> = logpdf(proposal)

    <span style="font-weight: bold; font-style: italic;">log_uniform</span> = np.log(jax.random.uniform(subkey))
    <span style="font-weight: bold; font-style: italic;">do_accept</span> = log_uniform &lt; proposal_log_prob - log_prob

    <span style="font-weight: bold; font-style: italic;">position</span> = np.where(do_accept, proposal, position)
    <span style="font-weight: bold; font-style: italic;">log_prob</span> = np.where(do_accept, proposal_log_prob, log_prob)
    <span style="font-weight: bold;">return</span> position, log_prob
</pre>
</div>

<p>
There are a few things to note here:
</p>
<ol class="org-ol">
<li><code>jax.numpy</code> acts as a drop-in replacement to <code>numpy</code>. For functions that only
involve array operations, replacing <code>import numpy as np</code> by <code>import jax.numpy
   as np</code> should already give you performance benefits.</li>
<li>We need to help JAX's compiler a little bit by indicating which arguments are
not going to change when the function is run several times:
<code>@partial(jax.jit, argnums=(1,))</code>. This is compulsory if you pass a
function as an argument, and can enable further compile-time optimizations.</li>
<li>The kernel is written for a single chain. Keep reading for an explanation.</li>
</ol>

<p>
Finally, and most importantly, JAX handles pseudo-random number generator in a
very specific way and this can be tricky to grasp at first. Notice the line
<code>key, subkey = jax.random.split(rng_key)</code>. What this line does is return the
original key, and <code>subkey</code> which is original key "advanced" one step. **If you
do not use <code>split</code> you will get a constant value instead of pseudo-random
numbers.** Please do read the PRNG section in the [Gotcha man page](). I
completely missed this in the previous version of this benchmark, and the
performance that I reported where grossly optimistic.
</p>


<p>
We will use <code>vmap</code> below to
   vectorize the function. It allows a very neat conceptual separation: a
   transition kernel advances a chain; it should be the rest of the code's
   responsibility to make kernels run in parallel.
</p>

<p>
The first thing to notice is that the function is written as if there only was a
single chain.
</p>

<p>
If you are familiar with Numpy, the syntax should feel very familiar to you.
There are a few differences:
</p>

<ol class="org-ol">
<li>JAX handle random number generation differently from other Python packages,
for <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/google/jax/blob/master/design_notes/prng.md">very good reasons</a> (read it!). Every distribution takes a PRNG key as an input.</li>
<li>I extracted the kernel from the sampler because JAX cannot compile
generators (or can it?). So we extract and JIT the function that does all the
heavy lifting: <code>rw_metropolis_kernel</code>.</li>
</ol>
</div>
</div>


<div id="outline-container-org1cf3b36" class="outline-3">
<h3 id="org1cf3b36">Tensorflow Probability</h3>
<div class="outline-text-3" id="text-org1cf3b36">
<p>
For TFP we use the Random Walk Metropolis algorithm implemented in the library:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">from</span> functools <span style="font-weight: bold;">import</span> partial

<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np
<span style="font-weight: bold;">import</span> tensorflow <span style="font-weight: bold;">as</span> tf
<span style="font-weight: bold;">import</span> tensorflow_probability <span style="font-weight: bold;">as</span> tfp
<span style="font-weight: bold; font-style: italic;">tfd</span> = tfp.distributions

<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">run_raw_metropolis</span>(n_dims, n_samples, n_chains, target):
    <span style="font-weight: bold; font-style: italic;">samples</span>, <span style="font-weight: bold; font-style: italic;">_</span> = tfp.mcmc.sample_chain(
        num_results=n_samples,
        current_state=np.zeros((n_dims, n_chains), dtype=np.float32),
        kernel=tfp.mcmc.RandomWalkMetropolis(target.log_prob, seed=42),
        num_burnin_steps=0,
        parallel_iterations=8,
    )
    <span style="font-weight: bold;">return</span> samples

<span style="font-weight: bold; font-style: italic;">run_mcm</span> = partial(run_tfp_mcmc, n_dims, n_samples, n_chains, target)

<span style="font-weight: bold; font-style: italic;">## </span><span style="font-weight: bold; font-style: italic;">Without XLA</span>
run_mcm()

<span style="font-weight: bold; font-style: italic;">## </span><span style="font-weight: bold; font-style: italic;">With XLA compilation</span>
tf.xla.experimental.<span style="font-weight: bold;">compile</span>(run_mcm)

</pre>
</div>
</div>
</div>

<div id="outline-container-org4d82cb4" class="outline-3">
<h3 id="org4d82cb4">Results</h3>
<div class="outline-text-3" id="text-org4d82cb4">
<p>
We have two degrees of freedom: the number of samples and the number of
chains. While the first relies on raw number crunching power, the second also
relies on the way vectorization is implemented. I thus decided to benchmark
algorithm on both dimensions.
</p>

<p>
I consider the following cases:
</p>

<ol class="org-ol">
<li>The Numpy implementation;</li>
<li>The JAX implementation;</li>
<li>The JAX implementation to which I subtract the compilation time. This is a
hypothetical situation, just to show the improvement brought by compilation.</li>
<li>Tensorflow Probability;</li>
<li>Tensorflow Probability with the experimental XLA compilation.</li>
</ol>
</div>

<div id="outline-container-orgb45def5" class="outline-4">
<h4 id="orgb45def5">Draw an increasing number of samples with 1,000 chains</h4>
<div class="outline-text-4" id="text-orgb45def5">
<p>
We fix the number of chains and make the number of samples vary.
</p>


<div id="org7bba124" class="figure">
<p><img src="img/jax-parallel-mcmc-samples.png" alt="jax-parallel-mcmc-samples.png" width="100%" />
</p>
</div>

<p>
You will notice the missing point for TFP implementation. Since the TFP
algorithm stores all the samples, it runs out of memory for large numbers of
samples. This did not happen with the XLA-compiled version, probably because it
uses more memory-efficient data structures.
</p>

<p>
For less than 1,000 samples the vanilla TFP and Numpy implementation are faster
than their compiled counterparts. This is due to the compilation overhead: when
you subtract the compilation time for JAX (thus obtaining the green curve), it
becomes faster by a large margin. Only when the number of samples becomes large
and the total sampling time is dominated by the time it takes to draw samples do
you start to reap the benefits of compilation.
</p>

<p>
<b>There is no magic: JIT compilation implies a noticeable, but
constant, computation overhead.</b>
</p>

<p>
I would recommend to go with JAX in most cases. Sampling in .3 seconds instead of 3
seconds only matters when this difference is going to be compounded by executing
the same piece of code more than ten times. However, compilation is something that
only need to happen once; in this case the investment will be paid off before
you reach 10 iterations. For all practical purposes, JAX wins.
</p>
</div>
</div>


<div id="outline-container-org3b28e44" class="outline-4">
<h4 id="org3b28e44">Draw 1,000 samples with an increasing number of chains</h4>
<div class="outline-text-4" id="text-org3b28e44">
<p>
Here we fix the number of samples and make the number of chains vary.
</p>


<div id="orge087885" class="figure">
<p><img src="img/jax-parallel-mcmc-chains.png" alt="jax-parallel-mcmc-chains.png" width="100%" />
</p>
</div>

<p>
JAX is still a clear winner: it is faster than Numpy as soon as the number of
chains reaches 10,000. You will note that there is a bump on the JAX curve, that
is entirely due to compilation (the green curve does not have this bump). I do
not have an explanation why, so ping me if you have the answer!
</p>

<p>
Here's the mind-blowing highlight:
</p>

<blockquote>
<p>
JAX can generate a billion samples in 25s on CPU. 20 times faster than Numpy.
</p>
</blockquote>
</div>
</div>
</div>
</div>


<div id="outline-container-org92d1163" class="outline-2">
<h2 id="org92d1163">Conclusion</h2>
<div class="outline-text-2" id="text-org92d1163">
<p>
For something that allows us to write code in pure python, JAX's performance is
incredible. Numpy still is a decent contender, especially for the smaller
numbers where most of JAX's execution time is spent compiling.
</p>

<p>
However, Numpy is not suitable for a Probabilistic Programming Language. The
implementation of efficient sampling algorithms like Hamiltonian Monte Carlo
requires to compute the gradient of the probability density functions. JAX,
however, between its performance and autodiff capabilities, has all it takes. No
wonder Uber's team started working with JAX on <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/pyro-ppl/numpyro">Numpyro</a>.
</p>

<p>
Don't read too much in Tensorflow Probability's poor performance. When it comes
to sampling from a distribution, what matters is not raw speed, but the number
of effective samples per second. TFP's implementation includes more bells and
whistles, and I would expect it to be way more competitive in terms of effective
number of samples per second.
</p>

<p>
Finally, note that it is way easier to scale by multiplying the number of chains
than the number of samples. We don't know what to do with those chains yet, but
I have the gut feeling that once we do, probabilistic programming will have
another breakthrough.
</p>
</div>
</div>
</div>
</body>
</html>
