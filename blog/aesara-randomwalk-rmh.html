<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-10-10 Mon 08:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Random Walk Rosenbluth-Metropolis-Hastings in Aesara</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Random Walk Rosenbluth-Metropolis-Hastings in Aesara</h1>
<p>
Right before I started working on <a href="file:///home/runner/projects/thetypicalset/org/blog/introducing-mcx.html">MCX</a> I wrote a simple benchmarks for PyTorch, Tensorflow and JAX on a very simple problem: using the random walk Rosenbluth-Metropolis-Hastings algorithm to sample from a mixture distribution. MCX was discontinued a bit more than a year ago, when I started working with a PPL based on <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aesara">Aesara</a>. So let me revisit this simple example using Aeasara!
</p>

<p>
The full code was added to the <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/rlouf/blog-benchmark-rwmetropolis/blob/master/aesara_sampler.py">repository</a>. For this example we use the <i>C backend</i>, though Aesara also offers a Numba and a JAX backend.
</p>

<div id="outline-container-orgea9b609" class="outline-2">
<h2 id="orgea9b609">Mixture model</h2>
<div class="outline-text-2" id="text-orgea9b609">
<p>
<a href="jax-parallel-mcmc.html">In the original blog post</a> I set to sample from a mixture distribution with 4 components. I had to write the corresponding log-probability density function by hand, i.e. without using a PPL. Implementing a mixture model in <code>Aesara</code> is straightforward. No need for a <code>Mixture</code> distribution (like in e.g. PyMC), you just write it like it is:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> aesara.tensor <span style="font-weight: bold;">as</span> at
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np

<span style="font-weight: bold; font-style: italic;">srng</span> = at.random.RandomStream(0)

<span style="font-weight: bold; font-style: italic;">loc</span> = np.array([-2, 0, 3.2, 2.5])
<span style="font-weight: bold; font-style: italic;">scale</span> = np.array([1.2, 1, 5, 2.8])
<span style="font-weight: bold; font-style: italic;">weights</span> = np.array([0.2, 0.3, 0.1, 0.4])

<span style="font-weight: bold; font-style: italic;">N_rv</span> = srng.normal(loc, scale, name=<span style="font-style: italic;">"N"</span>)
<span style="font-weight: bold; font-style: italic;">I_rv</span> = srng.categorical(weights, name=<span style="font-style: italic;">"I"</span>)
<span style="font-weight: bold; font-style: italic;">Y_rv</span> = N_rv[I_rv]
</pre>
</div>

<p>
We can generate forward samples from this model by compiling the model graph choosing <code>Y_rv</code> as an output:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> aesara

<span style="font-weight: bold; font-style: italic;">sample_fn</span> = aesara.function((), Y_rv)
<span style="font-weight: bold; font-style: italic;">samples</span> = [sample_fn() <span style="font-weight: bold;">for</span> _ <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(10000)]

<span style="font-weight: bold;">print</span>(samples[:10])
</pre>
</div>

<pre class="example">
[array(2.51645571), array(0.16094803), array(4.16173818), array(-0.75365736), array(0.91897138), array(-1.96086176), array(2.60226408), array(2.28198192), array(-1.05260784), array(1.38469404)]
</pre>


<p>
If you are not familiar with Theano/Aesara, the <code>aesara.function</code> may surprise you. What does it do exactly? When you manipulate Aesara tensors, you are not manipulating numbers, but rather you are <i>describing the computation to perform on the inputs</i>. As a result, the result of an Aesara operation is a graph:
</p>

<div class="org-src-container">
<pre class="src src-python">aesara.dprint(Y_rv)
</pre>
</div>

<pre class="example" id="org2b6cf1d">
Subtensor{int64} [id A]
 |normal_rv{0, (0, 0), floatX, False}.1 [id B] 'N'
 | |RandomGeneratorSharedVariable(&lt;Generator(PCG64) at 0x7FBAB334F680&gt;) [id C]
 | |TensorConstant{[]} [id D]
 | |TensorConstant{11} [id E]
 | |TensorConstant{[-2.   0. .. 3.2  2.5]} [id F]
 | |TensorConstant{[1.2 1.  5.  2.8]} [id G]
 |ScalarFromTensor [id H]
   |categorical_rv{0, (1,), int64, False}.1 [id I] 'I'
     |RandomGeneratorSharedVariable(&lt;Generator(PCG64) at 0x7FBAB17AE7A0&gt;) [id J]
     |TensorConstant{[]} [id K]
     |TensorConstant{4} [id L]
     |TensorConstant{[0.2 0.3 0.1 0.4]} [id M]
</pre>

<p>
<code>aesara.function</code> is therefore used to <i>compile</i> the graph into a function that can be executed. For that, we need to specify the inputs and outputs of the function. In this case there are no outputs, and the value of <code>Y_rv</code> is the output.
</p>

<p>
To compute the log-probability density function we can use <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aeppl">AePPL</a>'s <code>joint_logprob</code> function. AePPL transforms the Aesara model graph to get the graph that computes the model's joint logprob (see, working with computation graphs is nice!). We pass a dictionary that tells which value to associate with the random variables <code>Y_rv</code> and <code>I_rv</code>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">from</span> aeppl <span style="font-weight: bold;">import</span> joint_logprob

<span style="font-weight: bold; font-style: italic;">y_vv</span> = Y_rv.clone()
<span style="font-weight: bold; font-style: italic;">i_vv</span> = I_rv.clone()
<span style="font-weight: bold; font-style: italic;">logprob</span> = joint_logprob({Y_rv: y_vv, I_rv: i_vv})

<span style="font-weight: bold;">print</span>(logprob.<span style="font-weight: bold;">eval</span>({y_vv: 10., i_vv: 3}))
</pre>
</div>

<pre class="example">
-6.452221131239579
</pre>


<p>
Here we do not really care about the values that <code>I_rv</code> takes, so we marginalize the log-probability density function over <code>I_rv</code>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">logprob</span> = []
<span style="font-weight: bold;">for</span> i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(4):
    <span style="font-weight: bold; font-style: italic;">i_vv</span> = at.as_tensor(i, dtype=<span style="font-style: italic;">"int64"</span>)
    logprob.append(joint_logprob({Y_rv: y_vv, I_rv: i_vv}))
<span style="font-weight: bold; font-style: italic;">logprob</span> = at.stack(logprob, axis=0)
<span style="font-weight: bold; font-style: italic;">total_logprob</span> = at.logsumexp(at.log(weights) + logprob)

<span style="font-weight: bold;">print</span>(total_logprob.<span style="font-weight: bold;">eval</span>({y_vv: 10.}))
</pre>
</div>

<pre class="example">
-6.961941398089025
</pre>
</div>
</div>

<div id="outline-container-org9e5dc00" class="outline-2">
<h2 id="org9e5dc00">Implement the algorithm</h2>
<div class="outline-text-2" id="text-org9e5dc00">
<p>
The random walk Rosenbluth-Metropolis-Hasting algorithm is also straightforward to implement:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">rw_metropolis_kernel</span>(srng, logprob_fn):
    <span style="font-style: italic;">"""Build the random walk Rosenbluth-Metropolis-Hastings (RNH) kernel."""</span>

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">one_step</span>(position, logprob):
        <span style="font-style: italic;">"""Generate one sample using the random walk RMH algorithm.</span>

<span style="font-style: italic;">        Attributes</span>
<span style="font-style: italic;">        ----------</span>
<span style="font-style: italic;">        position:</span>
<span style="font-style: italic;">            The initial position.</span>
<span style="font-style: italic;">        logprob:</span>
<span style="font-style: italic;">            The initial value of the logprobability.</span>

<span style="font-style: italic;">        Returns</span>
<span style="font-style: italic;">        ------</span>
<span style="font-style: italic;">        The next positions and values of the logprobability.</span>

<span style="font-style: italic;">        """</span>
        <span style="font-weight: bold; font-style: italic;">move_proposal</span> = 0.1 * srng.normal(0, 1)
        <span style="font-weight: bold; font-style: italic;">proposal</span> = position + move_proposal
        <span style="font-weight: bold; font-style: italic;">proposal_logprob</span> = logprob_fn(proposal)

        <span style="font-weight: bold; font-style: italic;">log_uniform</span> = at.log(srng.uniform())
        <span style="font-weight: bold; font-style: italic;">do_accept</span> = log_uniform &lt; proposal_logprob - logprob

        <span style="font-weight: bold; font-style: italic;">position</span> = at.where(do_accept, proposal, position)
        <span style="font-weight: bold; font-style: italic;">logprob</span> = at.where(do_accept, proposal_logprob, logprob)

        <span style="font-weight: bold;">return</span> position, logprob

    <span style="font-weight: bold;">return</span> one_step
</pre>
</div>

<p>
Syntactically, <code>aesara.tensor</code> looks like a drop-in replacement to <code>numpy</code>. Remember, however, that these functions do not act on numbers but add an operation to an existing graph of computation. In particular, <code>logprob_fn</code> is a function that takes a graph (possibly a single variable), and returns the graph that computes the value of the log-probability density function.
</p>
</div>
</div>

<div id="outline-container-org8c08ebf" class="outline-2">
<h2 id="org8c08ebf">So, does it work?</h2>
<div class="outline-text-2" id="text-org8c08ebf">
<p>
Let us sample 1000 chains concurrently for an increasing number of samples and compare the running time to NumPy's and JAX's:
</p>


<div id="org2ae88d7" class="figure">
<p><img src="img/rmh-aesara-comparison.png" alt="rmh-aesara-comparison.png" width="100%" />
</p>
</div>

<p>
For small number of samples, Aesara (C backend) and JAX spend most of their time compiling the kernel and NumPy is faster. Past \(10^4\) samples NumPy lags behind, with Aesara catching up with JAX around \(10^5\) samples.
</p>
</div>
</div>

<div id="outline-container-org37080c0" class="outline-2">
<h2 id="org37080c0">Perspectives</h2>
<div class="outline-text-2" id="text-org37080c0">
<p>
Aesara is still young and holds many promises for the future, <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aesara/issues">come help us</a>! Here is what you can expect to change with this example in the near future:
</p>

<p>
<b><b>Maginalize automatically.</b></b>  <code>AePPL</code> will soon allow to automatically marginalize over discrete random variable (see <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aeppl/issues/21">related issue</a>).
</p>

<p>
<b><b>Vectorize computation.</b></b> The implementation for the multiple chain sampler is currently close to NumPy's for performance reasons, but you should soon be able to write the kernel for a single chain, and use the equivalent of <code>np.vectorize</code> or <code>jax.vmap</code> to vectorize the computation (see <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aesara/issues/695">related issue</a>).
</p>

<p>
<b><b>Work with different backends.</b></b> You will soon be able to compile this example using Aesara's JAX backend and Numba backend (<a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aesara/tree/main/aesara/link">work in progress, you can already try it!</a>). This means you will be able to interact with different ecosystems and leverage the strengths of different compilers / hardware devices with the <i>same model expression</i> in python. This also means that your model code is more future-proof as you can make the backend move under it.
</p>

<p>
<b><b>Build samplers automatically.</b></b> <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aemcmc">AeMCMC</a> analyzes your model graph and builds an efficient sampler for it.
</p>

<p>
Still not sure what Aesara is about? Read <a target='_blank' rel='noopener noreferrer' class='external' href="https://github.com/aesara-devs/aesara/discussions/879#discussioncomment-2472927">Brandon Willard's explanation</a>.
</p>
</div>
</div>
</div>
</body>
</html>
