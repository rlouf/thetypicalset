<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-02 Fri 11:26 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Classification uncertainty and (Bayesian) Neural Networks</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Classification uncertainty and (Bayesian) Neural Networks</h1>
<p>
This answer is a little longer than I originally expected, but I hope it is clear.
</p>


<div id="outline-container-org3ce84b0" class="outline-2">
<h2 id="org3ce84b0">Bayesian NNs are NN ensembles</h2>
<div class="outline-text-2" id="text-org3ce84b0">
<p>
It is important to define exactly what it is we are trying to compute, so I am going to briefly go over the difference between bayesian neural nets and others. We train classifiers because we are interested in the probability that an item indexed by \(i\) belongs to a category \(c\) given a model and a dataset on which we have "trained" the model:
</p>

\begin{align*}
P\left(\hat{y}_i = c | \mathcal{D}\right) = \int P(\hat{y}_i=c|\theta) P(\theta|\mathcal{D})\; \mathrm{d}\theta
\end{align*}

<p>
Where \(\theta\) is a vector that contains the model's weights, \(\mathcal{D} = \left\{x_i, y_i \right\}\) the training data. We can interpret the <i>predictive distribution</i> as the expection of the likelihood for a single network \(P(\hat{y}_i=c|\theta)\) under the <i>posterior distribution</i> \(P(\theta|\mathcal{D})\). The predictive distribution can therefore be interpreted as an ensemble of neural networks.
</p>

<p>
In practice, inference for Bayesian Neural Networks consists in drawing samples from the posterior distribution \(P(\theta|\mathcal{D})\) to be able to approximate \(P\left(\hat{y}_i = c | \mathcal{D}\right)\). Therefore in finding many networks (values of the parameters) that can plausibly explain the observations in \(\mathcal{D}\). Inference for non-Bayesian neural networks consists in finding the network (with weights \(\theta^*\)) that minimizes a given loss function. Hence non-bayesian NNs are more prone to overfitting on the training data.
</p>
</div>
</div>

<div id="outline-container-orgb402b0d" class="outline-2">
<h2 id="orgb402b0d">How confident can by model be?</h2>
<div class="outline-text-2" id="text-orgb402b0d">
<p>
Now we can go back to what I understood as your question: how do we estimate how "confused" our model is?
</p>

<p>
I take it that you consider the value of \(p^* = \operatorname{max}_c P(\hat{y}_i=c)\) as an estimator. The closer to 1 the more confident the neural net in its predictions. Ok, but in which of the following situations would you estimate your net is more confident:
</p>

<p>
This one?
</p>

<div class="org-src-container">
<pre class="src src-ascii">  x
  x
  x
  x   x   x
  x   x   x   x
| 1 | 2 | 3 | 4 |
</pre>
</div>

<p>
Or this one?
</p>

<div class="org-src-container">
<pre class="src src-ascii">  x   x
  x   x
  x   x
  x   x
  x   x
| 1 | 2 | 3 | 4 |
</pre>
</div>

<p>
I think most people would say that the result is more certain in the first situation. If I were to work with non-bayesian nets, and without any other information about the downstream requirements, I would use the <b>entropy of the histogram</b> as a measure instead. I explained earlier how to compute this histogram with a bayesian net, and we could indeed apply the same entropy measure in the bayesian case. The difference being that this histogram is supposedly more representative of the "true" probability of belonging to either category.
</p>

<p>
This may sound disappointing so far. But you can do something <b>much better</b> with Bayesian nets. We often ask how confident the model is because we are worried about the <i>consequences</i> of misclassification. If mis-classification costed us nothing we would just go with the highest-proability class all the time and not think too much about it. To illustrate let's take the example a neural net that takes pictures of machine parts and marks them as defective or good to go. There are two possibilities for misclassification:
</p>
<ul class="org-ul">
<li>Classifying a part as defective when it isn't. We typically loose the equivalent of the fabrication cost of that piece;</li>
<li>Classifying a part as not defective when it is. The cost of such a mistake wildly varies depending on the application. It can get extremely high for commercial airplanes, for instance.</li>
</ul>

<p>
The bayesian way of handling this situation is to <i>average</i> each of the cost functions over the posterior distribution \(P(\theta|\mathcal{D})\). We can imagine many scenarios in which someone using a standard net would not raise an alarm when the bayesian does so. For instance in situation where \(P(\hat{y}_i = \text{defective})\) is concentrated around very small values but with a "long" tail; in scenarios where the cost of mistakes is prohibitive the tail is amplified by the cost function so that the piece ends up being rejected. A classical net would have likely chosen a single value for \(P(\hat{y}_i = \text{defective})\) that is close to 0, and you probably would have ended up keeping the piece.
</p>
</div>

<div id="outline-container-orgf2f9353" class="outline-4">
<h4 id="orgf2f9353">TL;DR</h4>
<div class="outline-text-4" id="text-orgf2f9353">
<ul class="org-ul">
<li>Bayesian nets are ensemble models; To compute a given probability we consider <i>all</i> the model that could have plausibly explained the data;</li>
<li>In theory, estimating uncertainty is probably better done with entropy. In practice, you don't care about uncertainty but about mistakes. Bayesian nets are better equiped for that, see bayesian decision theory.</li>
<li>If you don't care about mistakes, NNs are probably fine. If you do, a lot, BNNs are worth the trouble.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org3e71168" class="outline-2">
<h2 id="org3e71168">References</h2>
<div class="outline-text-2" id="text-org3e71168">
<ul class="org-ul">
<li><i>Why the softmax?</i> <a target='_blank' rel='noopener noreferrer' class='external' href="https://crazyoscarchang.github.io/2018/08/29/why-the-softmax-function/">https://crazyoscarchang.github.io/2018/08/29/why-the-softmax-function/</a></li>
<li>Dropout as a Bayesian approximation: representing uncertainty in Deep Learning (<a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/abs/1506.02142">ArXiV</a>)</li>
<li>Weight Uncertainty in Neural Networks (<a target='_blank' rel='noopener noreferrer' class='external' href="https://arxiv.org/abs/1505.05424">ArXiV</a>)</li>
<li>"Bayesian Neural Networks" (<a target='_blank' rel='noopener noreferrer' class='external' href="https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/">David Duvenaud</a>)</li>
</ul>
</div>
</div>

<div id="outline-container-org832ae5a" class="outline-2">
<h2 id="org832ae5a">Links to this note</h2>
</div>
</div>
</body>
</html>