<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-21 Wed 19:04 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Linear regression is a data smoother</title>
<meta name="author" content="RÃ©mi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Linear regression is a data smoother</h1>
<dl class="org-dl">
<dt>tags</dt><dd><a href="20220104221840-machine_learning.html#ID-45c083ce-32ab-4758-aa1c-32b0496594b1">Machine Learning</a></dd>
</dl>


<p>
Let us consider the simpler linear regression \(r(x) = \alpha + \beta\, x\). The ordinary least square solution gives
</p>

\begin{equation}
  \hat{\beta} =\frac{\sum_{i} y_{i}\,x_{i}}{\sum_{i}x_{i}}
\end{equation}

<p>
The estimated regression function is thus:
</p>

\begin{equation}
  \hat{r}(x) = \sum_{i} y_{i} \frac{x_{i}}{n\, s_{X}^{2}} x
\end{equation}

<p>
where \(s_{X}^2\) is the sample variance of X. The prediction is a weighted average of the observed values of the dependent variable
where the weights are proportional to how far the \(x_i\) is from the center, relative to the variance, and proportional to \(x\).
</p>

<p>
It is a special case of the <b>linear smoothers</b> which are estimates of the regression function as:
</p>

\begin{equation}
  \hat{r}(x) = \sum_{i} y_{i} \hat{\omega}(x_{i, x})
\end{equation}

<div id="outline-container-org97272be" class="outline-2">
<h2 id="org97272be">Reference</h2>
<div class="outline-text-2" id="text-org97272be">
<ul class="org-ul">
<li><a target='_blank' rel='noopener noreferrer' class='external' href="https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch01.pdf">Notes of Cosma Shalizi on regression</a></li>
</ul>
</div>
</div>

<div id="outline-container-org63b2370" class="outline-2">
<h2 id="org63b2370">Links to this note</h2>
</div>
</div>
</body>
</html>