<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-01 Thu 09:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>One pollster's explanation for why the polls got it wrong</title>
<meta name="author" content="Rémi Louf" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="style.css" /><script data-goatcounter="https://thetypicalset.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">One pollster's explanation for why the polls got it wrong</h1>
<p>
Interview of <a href="20220308081309-david_shorr.html#ID-9da40b6f-5255-4bda-96ce-6b803feb6871">David Shor</a>.
</p>

<p>
2020 election polls predicted that Biden would win but with huge misses, up to 11 points. In that sense polling was not better in 2020 than it was in 2016 and 2018, it's just that Biden's lead was robust enough.
</p>

<p>
David thinks that it is due to a sorting bias, i.e. the fact that people who answer polls even controlled for demographics are very unlike the typical voter. His view is consistent with .
</p>

<p>
Assumption of polling is that you can take the responses of poll takers, weight them according to race, education, gender, etc. and get the views of the general population. This use to be roughly right but changed in 2016.
</p>

<blockquote>
<p>
But it turns out that people who answer surveys are really weird. They're considerably more politically engaged than normal and they have much higher agreeableness. [&#x2026;] They also have higher levels of social trust.
</p>
</blockquote>


<blockquote>
<p>
This gets to something that’s really scary about polling, which is that polling is fundamentally built on this assumption that people who answer surveys are the same as people who don’t, once you condition on enough things. That can be true at any given time. But these things that we’re trying to measure are constantly changing. And so you can have a method that worked in past cycles suddenly break.
</p>
</blockquote>

<p>
The polling business has become more complicated now that simple statistical weighing with a few variables does not work anymore:
</p>

<blockquote>
<p>
There used to be a world where polling involved calling people, applying classical statistical adjustments, and putting most of the emphasis on interpretation. Now you need voter files and proprietary first-party data and teams of machine learning engineers. It’s become a much harder problem.
</p>
</blockquote>

<p>
Against the <i>shy Trump voter</i> narrative:
</p>

<blockquote>
<p>
People tell the truth when you ask them who they’re voting for. They really do, on average. The reason why the polls are wrong is because the people who were answering these surveys were the wrong people.
</p>
</blockquote>

<div id="outline-container-orgf8a6574" class="outline-2">
<h2 id="orgf8a6574">Links to this note</h2>
</div>
</div>
</body>
</html>